{"meta":{"title":"Cyylog","subtitle":"世间百态，三千繁华，怎能独守这掌心地大，怎能错过最美的风景。","description":"精通原创，擅长盗版","author":"Cyylog","url":"https://cyylog.github.io","root":"/"},"pages":[{"title":"404","date":"2018-09-30T09:25:30.000Z","updated":"2020-09-23T18:31:16.745Z","comments":true,"path":"404.html","permalink":"https://cyylog.github.io/404.html","excerpt":"","text":""},{"title":"categories","date":"2020-09-23T17:42:38.000Z","updated":"2020-09-23T18:40:35.137Z","comments":true,"path":"categories/index.html","permalink":"https://cyylog.github.io/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2020-09-23T18:41:20.000Z","updated":"2020-09-29T14:46:21.126Z","comments":true,"path":"about/index.html","permalink":"https://cyylog.github.io/about/index.html","excerpt":"","text":""},{"title":"contact","date":"2020-09-23T17:47:42.000Z","updated":"2020-09-23T17:47:58.636Z","comments":true,"path":"contact/index.html","permalink":"https://cyylog.github.io/contact/index.html","excerpt":"","text":""},{"title":"friends","date":"2020-09-23T17:49:45.000Z","updated":"2020-09-23T18:30:06.244Z","comments":true,"path":"friends/index.html","permalink":"https://cyylog.github.io/friends/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-09-23T17:46:12.000Z","updated":"2020-09-23T17:46:40.476Z","comments":true,"path":"tags/index.html","permalink":"https://cyylog.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"运维面试题-01","slug":"Interview/运维面试题-01","date":"9999-08-16T01:59:59.000Z","updated":"2020-10-20T22:13:46.893Z","comments":true,"path":"9999/08/16/interview/yun-wei-mian-shi-ti-01/","link":"","permalink":"https://cyylog.github.io/9999/08/16/interview/yun-wei-mian-shi-ti-01/","excerpt":"","text":"PS：以下内容，网友提供 欢迎投稿 cyylog@aliyun.com 四剑客awk 统计日志中某个时间范围的 IP 访问量，并进行排序 $ start_dt='10/May/2018:23:47:43 $ end_dt='10/May/2018:23:49:05' $ awk -v st=${start_dt} -v ent=${end_dt} -F'[][ ]' '$5 == st,$5 == ent {print $1}' app.log |sort |uniq -c |sort -nr |head -n 10 66 223.13.142.15 6 110.183.13.212 4 1.69.17.127 1 113.25.94.69 1 110.183.58.144 awk '$4>=\"[30/May/2020:01:08:25\" && $4=\"[29/May/2020:20:39:11\" && $4","categories":[{"name":"Interview","slug":"Interview","permalink":"https://cyylog.github.io/categories/Interview/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/tags/Linux/"}]},{"title":"系统调优","slug":"Interview/系统调优","date":"9998-08-16T01:59:59.000Z","updated":"2020-10-07T08:02:00.673Z","comments":true,"path":"9998/08/16/interview/xi-tong-diao-you/","link":"","permalink":"https://cyylog.github.io/9998/08/16/interview/xi-tong-diao-you/","excerpt":"","text":"PS：以下内容，网友提供 欢迎投稿 cyylog@aliyun.com 性能优化优化的目的 1. 提高资源的利用率 2. 找到性能瓶颈以及缓解这个瓶颈（CPU，内存，IO调度、网络、使用的应用程序） 3. 通过性能管理实现合理的资源分配，以及提升硬件的性价比 4. 通常做两种调优： response time: 响应时间 Web服务器，用户感受度好 throughput: 吞吐量 文件服务器，拷贝的速度调优需要掌握的技能： 系统当前状况如何是不是有瓶颈的 1. 必须了解硬件和软件 2. 能够把所有的性能、指标量化，用数字说话 3. 设置一个正常期待值，比如将响应速度调到1.5秒 （企业版操作系统在出厂时已经调优，适用于普遍的应用，再根据个人的环境进行微调） 4. 建议有一定的开发能力 5. 想要更好的调优，需要更多的经验的积累，从而有一定洞察力，调节时所给参数才最恰当性能调优分层及效率问题： - 业务级调优 例如：网站一定要使用Apache吗? 例如：将原有的调度器由LVS换成F5-BigIP？ redware 例如：将原有的调度器由Nginx换成Haproxy？ 例如：能不能购买CDN？ 内容发布网络 例如：通否增加服务器数量？ 例如：将原有的Memcache换成Redis? 例如：将原有的MySQL Proxy换成MyCat? - 应用级调优 NFS，Samba，Apache、Nginx、php-fpm、MySQL、Oracle、KVM、LVS、PHP本身调优 对于日志的处理rsyslog：可以调整记录的日志等级或延后日志写，从而避免大量的I/O操作 禁用一些不必要的服务如蓝牙、smart card，makewhatis, updatadb 从运维人员角度来说无非就是参数的修改和配置 - kernel级调优 操作系统系统层面，即kernel调优具有普遍性：I/O CPU Network Memory，通常在系统初始完成 vm.dirty_expire_centisecs = 2 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_synack_retries = 5 考虑的是怎么让应用程序在我们系统上运行的更合理规范,或者说在硬件不变动的情况下,对系统优化提高性能和效率 优化效果 1.当前配置是否适合当前的运行环境和用户需求(如果是memcache 那我们考虑更多的可能是内存的消耗是不是更大些,如果是apache的话我们考虑的话可能就是cpu的消耗,所以所有的优化手段都要根据具体的环境,如果是编译需要的内存和cpu) 2.硬件的提升效果会更好，自上往下，效果越来越不明显优化 ＝＝调整系统kernel/应用的性能参数 ＝＝合理资源分配PAM，Cgroup(Docker) ＝＝架构优化，Nginx负载集群、MySQL读写分离/Galera Cluster # man proc /drop_cache Kernel级别的优化:查看内核文档 # yum -y install kernel-doc # ls /usr/share/doc/kernel-doc-3.10.0/Documentation/sysctl/ 00-INDEX abi.txt fs.txt kernel.txt net.txt README sunrpc.txt vm.txt # grep &quot;_reuse&quot; -R /usr/share/doc/kernel-doc-3.10.0/Documentation/ /usr/share/doc/kernel-doc-3.10.0/Documentation/networking/ipvs-sysctl.txt:conn_reuse_mode - INTEGER /usr/share/doc/kernel-doc-3.10.0/Documentation/networking/ip-sysctl.txt:tcp_tw_reuse - BOOLEAN # # grep &quot;drop_caches&quot; -R /usr/share/doc/kernel-doc-3.10.0/Documentation/ /usr/share/doc/kernel-doc-3.10.0/Documentation/sysctl/vm.txt:drop_caches获取硬件信息硬件方面，主要要调优的对象 CPU Memory Storage Networking 基本知识 基本知识： 所有存储类的设备 离CPU越近会越快，离CPU越远将越慢 离CPU越近存储的容量越小，离CPU越远存储的容量越大 离CPU最近的是寄存器，寄存器的时钟周期和CPU是一样的 离CPU稍远的存储： L1，在CPU中，静态内存，工艺复杂 L2，使用的是动态高速内存 L3 L4 Memory Storage TB, PB # lscpu L1d cache 一级数据缓存 L1i cache 一级指令缓存 L2 二级缓存（L2是否共享?） Thread(s) per core: 线程数为1，不支持超线程 Core(s) per socket: CPU核数 Socket(s): 1 几路 NUMA node(s): 1 不支持NUMA # x86info -c 查看CPU型号 # getconf -a 显示所有系统配值变量值 根据CPU访问内存的方法： a. UMA：一致性内存访问，传统架构 MCH: 内存控制芯片，即北桥 ICH： I/O控制芯片，即南桥，连接外设，硬盘，USB，慢速PCI总线设备 b. NUMA：非一致性内存访问（内存分为多片，而不是UMA时的一片内存） IOH： I/O HUB，也可以叫北桥 NUMA 架构显著的特色是：内存划分成片，CPU访问本区的内存的时候，速度非常快 Memory的相关指标 # cat /proc/meminfo # free # top # vmstat Memory size: 内存效率重要指标： a. bandwidth: 带宽，使用的是DDR几代；例如PC2100，指2100M/s的吞吐量，即带宽 b. latency: 延迟，纳秒级ns c. ECC内存 存储的相关指标 主要的调优对像，对内存而言，特别慢 拿16G的内存存数据！ 拿16G的硬盘存数据！ 当前两种磁盘类型： 机械磁盘： 盘片 电子SSD： 固态磁盘 机械磁盘： 昂贵的寻道时间 Expensive seek time 主轴转速： 7200RPM 10000RPM(10k) 15000RPM(15k) 缺点：容易坏 优点：存储量非常高，价格低 Burst speed: 突发速率，最快读写速率，指的是顺序读写 Sequential access average speed: 平均读写速率，生产中不能保证是顺序读写 Random access 调优： 进行大量的优化合并 SSD： Electronic disk: 电子磁盘，没有机械部件 No start-up time，读和写延迟非常低 安静，发热量小，不怕震动，而且很轻，用电少 价格高：在同等容量下比较 寿命相对短 连接的方式： 内部存储： IDE（PATA），SATA，SAS，FC 外部存储： SCSI线连接, SAS线连接，SATA线连接，Fibre Channel, ISCSI，网络带宽，延迟，多路径都会影响到外部存储的速度 4k/sector support： 4k对齐，效率更高（默认扇区大小512B） 外圈快，内圈慢： 磁盘转一圈，外圈读的扇区数多，所以数据写读时外圈快，内圈慢 RHEL安装时，默认将/, /boot分区扔到了最快圈，交换区自动选择内圈（它认为swap不需要经常访问） Networking Profile 带宽 延迟 尽量把不同的网络隔离开，让其处于不同的广播域，划分VLAN 使用bonding或Team技术实现多网卡绑定，从而实现HA或LB（提高带宽） 高端网卡支持虚拟化SR-IOV 高端网卡有自己的处理芯片，网络上的数据到达时，不需要中断CPU 其它获得硬件性能指标工具 dmidecode 直接查BIOS信息 dmidecode -t 0 type 0主要是硬件信息 powertop 查看最耗电进程 lspci lsusb ethtool ================================================================================= //检测是否有该设备 [root@install ~]# lspci |grep -i eth 03:00.0 Ethernet controller: Broadcom Corporation NetXtreme II BCM5708 Gigabit Ethernet (rev 12) 05:00.0 Ethernet controller: Broadcom Corporation NetXtreme II BCM5708 Gigabit Ethernet (rev 12) [root@install ~]# lspci |grep -i fib 12:02.0 Fibre Channel: Emulex Corporation Thor LightPulse Fibre Channel Host Adapter (rev 01) 12:02.1 Fibre Channel: Emulex Corporation Thor LightPulse Fibre Channel Host Adapter (rev 01) # ethtool eth0 Settings for eth0: Supported ports: [ TP MII ] Supported link modes: 10baseT/Half 10baseT/Full 100baseT/Half 100baseT/Full 1000baseT/Half 1000baseT/Full Supported pause frame use: No Supports auto-negotiation: Yes Advertised link modes: 10baseT/Half 10baseT/Full 100baseT/Half 100baseT/Full 1000baseT/Half 1000baseT/Full Advertised pause frame use: Symmetric Receive-only Advertised auto-negotiation: Yes Link partner advertised link modes: 10baseT/Half 10baseT/Full 100baseT/Half 100baseT/Full 1000baseT/Half 1000baseT/Full Link partner advertised pause frame use: Symmetric Link partner advertised auto-negotiation: Yes Speed: 1000Mb/s Duplex: Full Port: MII PHYAD: 0 Transceiver: internal Auto-negotiation: on Supports Wake-on: pumbg Wake-on: g Current message level: 0x00000033 (51) drv probe ifdown ifup Link detected: yes # ethtool -i eth0 driver: r8169 version: 2.3LK-NAPI firmware-version: rtl8168e-3_0.0.4 03/27/12 bus-info: 0000:04:00.0 supports-statistics: yes supports-test: no supports-eeprom-access: no supports-register-dump: yes supports-priv-flags: no # mii-tool eth0 eth0: negotiated 100baseTx-FD flow-control, link ok //是否加载相应的驱动 [root@install ~]# dmesg |grep -i fib Emulex LightPulse Fibre Channel SCSI driver 8.3.39 scsi3 : Emulex LP1050 PCI-X Fibre Channel Adapter on PCI bus 12 device 10 irq 19 scsi4 : Emulex LP1050 PCI-X Fibre Channel Adapter on PCI bus 12 device 11 irq 18 查看FC HBA wwn号 [root@install ~]# cat /sys/class/fc_host/host3/port_name 0x10000000c9672e4a [root@install ~]# cat /sys/class/fc_host/host4/port_name 0x10000000c9672e49 至强CPU Intel E3，E5，E7代表了3个不同档次的至强CPU，至强“E系列”的这种命名方式有些类似桌面上的Core i3，i5，i7；比较通俗易懂的解释就是可以对应我们的豪华汽车生产商宝马3系，5系和7系。分别对应好，更好和最好。 其中： 至强E3处理器是第一款使用Haswell微架构的至强处理器芯片（目前，英特尔的多数芯片是以Ivy Bridge架构为基础），最多配置4个内核，最高支持32GB内存，耗电量只有13瓦，主要用于工作站和单路服务器； 至强E5处理器主要用于中档服务器，最高支持768GB内存，耗电量为60至130瓦，适用入门级双路服务器、高性能双路和四路服务器，也是目前使用最为广泛的主流处理器； 至强E7处理器是英特尔性能最高的服务器处理器，芯片包括30GB三级缓存，最高支持4TB内存，耗电量为130瓦。这种处理器可用于8路服务器。 至于具体应用可以根据处理器的型号来判定，以英特尔最新发布的E5-2600 V2为例，这里的“2”，也就是连字符后的第一个数字，它代表处理器最多支持的并行路数，有1、2、4、8四种规格，分别代表了单路、双路、四路和八路。我们现在举例的这款E5-2600 至强CPU，连字符后的第一个数字是&quot;2&quot;，就表示这款CPU为双路，只能用于对应的双路芯片组的主板。 紧接着，我们来看连字符后的第二个数字，它代表处理器封装接口形式，一共有2，4，6，8四种规格，分别是2对应Socket H2(LGA 1155)、4对应Socket B2(LGA 1356)、6对应Socket R(LGA 2011)、8对应Socket LS(LGA 1567)。我们举例的这款E5-2600，连字符后的第二个数字是&quot;6&quot;，对应Socket R(LGA 2011) 然后，连字符后第三和第四位代表编号序列，一般是数字越大产品性能越高，价格也更贵。 最后，看连字符后第四位数字后面的代表什么。紧跟第四位数字后的&quot;L&quot;代表是低功耗版，留空的话就代表是标准版。 连字符后面最后的数字代表修订版本，比如v2、v3、v4等等，这次新发布的E5-2600 V2就是第二次升级版。 注： LGA 1155：表示不同的封装技术CPUcpu的相关指标 # cat /proc/cpuinfo ​ 物理的(多少路)，核心的，超线程 多路服务器： 双路四核，是指服务器/工作站有两个CPU，每个CPU都是四核处理器。 多路服务器/工作站的关键部件是多路主板，可安装多个处理器，双路主板举一示例如图： 一个CPU，配一套内存，双路就是双CPU加双套内存 CPU调度 cpu调度： 3个进程 a b c 都是R cpu处理谁? 这样就要依靠调度程序? a----b-----c 批处理操作系统 同样3个进程 a b c 每个运行1秒 好像是cpu独占 分时操作系统 哪种系统操作完消耗的时间少?批处理操作系统 OS从cpu的调度来分: 分时 分时操作系统以&quot;分时&quot;原则,为每个用户服务.将CPU的时间划分成若干个片段，称为时间片。操作系统把每个时间片分，轮流地切换给各终端用户的程序使用。由于时间间隔很短，每个用户的感觉就像他独占计算机一样。分时操作系统的特点是可有效增加资源的使用率。例如UNIX系统就采用剥夺式动态优先的CPU调度，有力地支持分时操作。 批处理 批处理(batch processing )就是将作业按照它们的性质分组（或分批），然后再成组（或成批）地提交给计算机系统，由计算机自动完成后再输出结果，从而减少作业建立和结束过程中的时间浪费。 如:打印机就是采用这种操作系统 实时 实时操作系统(RealTimeOperatingSystem，RTOS)是指使计算机能及时响应外部事件的请求在规定的严格时间内完成对该事件的处理，并控制所有实时设备和实时任务协调一致地工作的操作系统。实时操作系统要追求的目标是：对外部请求在严格时间范围内做出反应，有高可靠性和完整性。其主要特点是资源的分配和调度首先要考虑实时性然后才是效率。此外，实时操作系统应有较强的容错能力。 区别: 批处理系统(batch processing system)中，一个作业可以长时间地占用cpu。而分时系统中，一个作业只能在一个时间片（Time Slice，一般取100ms）的时间内使用cpu。 批处理系统的目的：提高系统吞吐量和资源的利用率。 分时系统的目的： 对用户的请求及时响应，并在可能条件下尽量提高系统资源的利用率。 进程与调度： 进程： 程序是一个文件,而process是一个执行中的程序实例。 linux系统中创建新进程使用fork()系统调用。 利用分时技术，在linux操作系统上同时可以运行多个进程，当进程的时间片用完时，kernel就利用调度程序切换到另一个进程去运行。 内核中的调度程序： 用于选择系统中下一个要运行的进程。你可以将调度程序看做在所有处于运行状态的进程之间分配CPU运行时间的管理代码。为了让进程有效的使用系统资源，又能让进程有较快的响应时间，就需要对进程的切换调度采用一定的调度策略。时钟频率 CPU timer 固定频率给CPU发中断,CPU可停下来通过调度器处理下一个任务 #grep HZ /boot/config-x.x.x #grep HZ /boot/config-2.6.32-358.el6.x86_64 CONFIG_NO_HZ=y # CONFIG_HZ_100 is not set # CONFIG_HZ_250 is not set # CONFIG_HZ_300 is not set CONFIG_HZ_1000=y ---------------------------1秒1000次 CONFIG_HZ=1000 CONFIG_MACHZ_WDT=m 所有的linux操作系统都是基于中断驱动的： 当我们在键盘上按下一个按键时，键盘就会对CPU说，一个键已经被按下。在这种情况下，键盘的IRQ线路中的电压就会发生一次变化，而这种电压的变化就是来自设备的请求，就相当于说这个设备有一个请求需要处理。 [root@mail ~]#watch -n 1 cat /proc/interrupts //包含有关于哪些中断正在使用和每个处理器各被中断了多少次的信息。 CPU0 0: 174 IO-APIC-edge timer 1: 70 IO-APIC-edge i8042 3: 1 IO-APIC-edge 4: 1 IO-APIC-edge 7: 0 IO-APIC-edge parport0 8: 0 IO-APIC-edge rtc0 9: 0 IO-APIC-fasteoi acpi 12: 1397 IO-APIC-edge i8042 14: 0 IO-APIC-edge ata_piix 15: 4180 IO-APIC-edge ata_piix 16: 945 IO-APIC-fasteoi Ensoniq AudioPCI 17: 778014 IO-APIC-fasteoi ehci_hcd:usb1, ioc0 18: 0 IO-APIC-fasteoi uhci_hcd:usb2 19: 487 IO-APIC-fasteoi eth0 第一列表示IRQ号,IRQ号决定了需要被CPU处理的优先级。IRQ号越小意味着优先级越高。 第二、三、四列表示相应的CPU核心被中断的次数。IO-APIC-edge表示终端接口，timer表示中断名称（为系统时钟）。174表示CPU0被中断了174次。i8042表示控制键盘和鼠标的键盘控制器。对于像rtc（real time clock）这样的中断，CPU是不会被中断的。因为RTC存在于电子设备中，是用于追踪时间的。NMI和LOC是系统所使用的驱动，用户无法访问和配置。 例如，如果CPU同时接收了来自键盘和系统时钟的中断，那么CPU首先会服务于系统时钟，因为他的IRQ号是 0 。 IRQ0 ：系统时钟（不能改变） IRQ1 ：键盘控制器（不能改变） IRQ3 ：串口2的串口控制器（如有串口4，则其也使用这个中断） IRQ4 ：串口1的串口控制器（如有串口3，则其也使用这个中断） IRQ5 ：并口2和3 或 声卡 IRQ6 ：软盘控制器 IRQ7 : 并口1。它被用于打印机或若是没有打印机，可以用于任何的并口。 调整：每接收到一个时钟中断就要处理另一个任务.调度器----根据调度算法 频率高 响应速度高 吞吐量低 频率低 响应速度底 吞吐量高 桌面推荐高频率,服务器推荐低频率 对于应用程序的运行，最好的办法是纵向升级（提升CPU频率）而不是横向升级（增加CPU数量）。这取决于你的应用程序是否能使用到多个处理器。例如一个单线程应用程序的升级方式最好是更换成更快的CPU而不是增加为多个CPUPS命令 ps命令： ps -o user,pid,%cpu -p 3288 ps -eo user,pid,%cpu --sort %cpu //升序 -%cpu 降序 top命令 #top -d 1 top - 10:18:15 up 1:45, 4 users, load average: 2.44, 2.42, 1.82 Tasks: 225 total, 3 running, 222 sleeping, 0 stopped, 0 zombie Cpu(s): 79.2%us, 17.6%sy, 0.0%ni, 0.3%id, 0.0%wa, 0.0%hi, 2.8%si, 0.0%st Mem: 4019424k total, 2419520k used, 1599904k free, 248248k buffers Swap: 8191992k total, 0k used, 8191992k free, 525780k cached ---------------------------------------------------------- 快捷键： P 按cpu排序 M 按内存排序 f 添加删除字段，带*的是默认显示 大写的是当前显示 小写的是现在没有显示的 可以关掉指定的列，开启需要的列，按对应的字母就可以 1 可以展开cpu，也就是如果有多个cpu可以这样查看 ，没有展开之前是平均值 top后按1，然后按W，会将此次的显示配置保存到 ~/.toprc 文件中,下次再top的时候就会直接显示多CPU使用率。 可以在脚本中直接使用top的Batch模式 #top -bn1一台服务器是否稳定 up后面的时间可以作为判断标准之一 load average: 1 5 15 1分钟，5分钟，15分钟之前到现在的cpu平均负载(等待cpu处理的进程队列平均长度) 负载： 正在运行的进程 进入到IO等待位置的进程 我是cpu 你们是程序 没人跟我说话 我现在是没有负载的 有一个人问问题 我有负载吗？ 没有 如果再有一个人问问题，如果之前同学的问题处理完了或者他俩的问题很简单，我一次就解决完了，这时候是没有负载的 如果第一个问的问题很困难，我一时解决不完，这时候再有人问问题的时候我就产生负载了 1分钟之内举例： 0 10 20 30 40 50 60 CPU A 只用了10秒，这时候cpu的负载是1/6 B 用了20秒， 这时候负载是1/2 C 用30秒， 这时候负载是1 最好的单核CPU负载是1 加上cpu核数，最好的负载是跟核数相等， 核数的2倍以内没什么大问题，2倍以上说明负载过高 8核 负载 25 15 3 ，这样的负载是没什么问题的 但是一定要判断到25那个地方发生了什么事情，能不能把那个峰值化解掉，不然如果峰值越来越高，那么服务器可能承受不住 比如邮件服务器的收发高峰和邮件备份产生的负载高峰，我们可以把他们两个操作分开成两个小高峰，这样最好 ----------------------------------------------------------- running 两个含义 1.正在运行 2.可运行的 //也就是此程序必须在调用cpu之前要准备好运行 sleeping 可以叫睡眠，等待或者阻塞 stoped表示挂起(暂停) ------------------------------------------------------------ %us 用户态程序占用cpu百分比 %sy 内核态程序占用cpu百分比 %ni 调整过nice值的程序占用cpu百分比 %id cpu空闲百分比 %wa io等待消耗cpu的百分比（cpu空转） %hi 硬件中断占用cpu的百分比 %si 软中断 %st cpu被偷走的百分比 …………………………………………………… 内核态和用户态 当一个任务（进程）执行系统调用而陷入内核代码中执行时，我们就称进程处于内核运行态（或简称为内核态）。此时处理器处于特权级最高的（0级）内核代码中执行。当进程处于内核态时，执行的内核代码会使用当前进程的内核栈。每个进程都有自己的内核栈。当进程在执行用户自己的代码时，则称其处于用户运行态（用户态）。即此时处理器在特权级最低的（3级）用户代码中运行。当正在执行用户程序而突然被中断程序中断时，此时用户程序也可以象征性地称为处于进程的内核态。因为中断处理程序将使用当前进程的内核栈。这与处于内核态的进程的状态有些类似。 例： #cat /etc/passwd 在用户级别执行cat //这些开销就是用户的开销 要用cat查看文件，必须得去硬盘上找文件，那么需要有硬盘驱动，这样就会需要内核，所以就会用到系统调用，因为有了系统调用才能跟内核对话 //这些开销就是内核态的开销 在内存里就会有一个cat命令的进程 把cat命令所需要的Lib库准备好 常用的系统调用 open() read() write() close() us &gt; sy 用户的开销一般大于系统开销 us一般不会超过%70，如果大于%70，那看看是不是运行的程序是否过多或者一个类似死循环的程序 sy一般不会超过%30，如果大于%30，那看看是不是运行了大量的内核态程序，比如iptables和lvs都是内核态程序 …………………………………………………… ni nice值的调整可能两种情况:系统自动去调整或者人工调整 那么这里的百分比是指人工调整 id 空闲太高不好，浪费资源，那么在20-30之间最好 如果%idle 的值高但系统响应慢时，有可能是 CPU 等待分配内存，此时应加大内存容量 如果%idle 的值持续低于1，则系统的 CPU 处理能力相对较低，表明系统中最需要解决的资源是 CPU 。 wa a在运行过程当中要产生10G数据，现在先产生了2G，cpu说你先停，你先把这2G写到硬盘上，但是现在硬盘慢，cpu要等，所以他可以去找b去运行，现在b程序发生了和a一样的情况，也在往硬盘写数据，那么cpu现在干什么？在等，这时候的等待状态我们称他为空转状态，所以通过这个数值可以判断硬盘慢不慢 如果%iowait 的值过高，表示硬盘存在I/O瓶颈 hi 中断：对cpu当前操作的一个打断 硬中断，也就是硬件产生的中断（比如键盘，鼠标，网卡）在实际工作当作，网卡产生的中断最多， si 软件中断，比如说计划任务 硬中断优先级比软中断优先级高 st 这个数值是针对虚拟机的 他的值我们看不到 vm1 vm2 //虚拟机 vcpu vcpu //虚拟cpu cpu //真实cpu 假如现在vm1正在使用vcpu，那么他实际上用的是cpu，那么这时候如果vm2要使用cpu的话，不能用，因为现在cpu正被vm1偷走了 ------------------------------------------------------------- buffers cached cached这个是属于mem那一行的，说是这里放不下了所以才放到了下面 两者都是RAM中的数据。简单来说，buffer是即将要被写入磁盘的，cache是被从磁盘中读出来的。 buffer是由各种进程分配的，被用在如输入队列等方面，索引缓存，存inode信息。一个简单的例子如某个进程要求有多个字段读入，在所有字段被读入完整之前，进程把先前读入的字段放在buffer中保存。 cache经常被用在磁盘的I/O请求上，存block信息，如果有多个进程都要访问某个文件，于是该文件便被做成cache以方便下次被访问，这样可提高系统性能。 手动释放buffers和cached: #echo 3 &gt; /proc/sys/vm/drop_caches ======================= 优先级 作用： 1、优先级高会被优先调度。 2、时间片会不同。 范围： 早先优先级范围 nice范围 0 -20 20 0 39 19 0~39 -20~19 现在： 0-139 linux优先级的整个范围，其中0号为优先级最高，139为最低 0-99 RT实时进程(静态)优先级范围 ， 0号保留，设置时使用1-99 100-139 非实时进程(动态)的优先级别范围 (由nice值映射过来) 分类： 动态优先级和静态优先级 top里面显示的PR是动态优先级，数值越小有限级越高 SYSV又加了60个优先级 ，那60个优先级就是静态优先级 现在Linux在之前40个优先级的基础上加了100个静态优先级，RT 实时优先级肯定大于非实时优先级 实际优先级范围是99优先级最大，0最小，在往后，从RT的角度来看全是0，但是实际是0-39 0--------------------99 100-----------139 整个优先级范围 -99---FIFO RR---------0--------------0 从RT的角度看后面优先级都是0 0--------------39 后面部分实际优先级 查看实时进程优先级： # chrt -p 5752 //pid为5752的进程在top内显示PR为20,但是在从RT的角度来看是0 pid 5752&#39;s current scheduling policy: SCHED_OTHER pid 5752&#39;s current scheduling priority: 0 优先级一样的时候先执行谁？ 早期：单cpu,单用户，单任务 在运行一个应用程序的时候，使用内存，会有空闲内存，cpu空闲，硬盘空闲 但是当时的硬件配置相当低(比如4k的内存)，而且执行很快(汇编写的程序)，所以也不会出现资源浪费，那么现在随着硬件的性能提高就会出现资源浪费现象 现在：多cpu,多用户，多任务 假如有一个应用程序占用CPU时间太长，那么后面的程序会无法执行 所以内核会对同优先级的程序进程判断是cpu消耗型还是IO消耗型，IO消耗型的程序并不是不使用cpu，只是用的时间很短，所以我们让他们两个都可以运行，但是在io消耗型程序使用cpu的时候临时给他调整一下优先级 内核可以让cpu消耗型在+5范围内优先级进程偏移，io消耗型在-5范围内优先级进程偏移，所以这里的优先级是动态优先级， 程序分为几种:1.cpu消耗型（计算） 2.IO消耗型（通常如果不是恶意程序，基本上就是IO消耗型，比如QQ，只有在收发信息的一瞬间才会消耗cpu。听歌消耗的是硬盘和声卡也是IO消耗型） cpu消耗型的 pri值为80~85 85标准cpu消耗型，如：死循环 io消耗型的 pri值为75~80 75标准io消耗型，往往是和用户交互的程序，如：bash,vi，下载 本身占CPU并不多，但I/0消耗多 调整优先级： 可以通过renice命令调整nice值,pri值调整不了,系统自已评估. 我们设置优先级别，设置的是nice 值，实际系统看的是pri值。用户调整nice值，系统会干预pri值 系统要考虑在两种进程的nice值一样的情况下,i/o消耗型的进程应该优先级别更高一些(也就是pri值更小) ps -l pri ni 77 0 ps 75 0 bash bash 典型的I/0消耗型,pri值大概为75 pri是系统以nice值为基数的调整 pri 浮动范围 85 &lt;---- 80 -----&gt; 75 CPU I/O 进程队列中如果有实时进程除非它释放资源,才执行非实时进程.当然实时进程也有优先级别高的,先执行完高优先级的才执行一下个进程.调度策略 查看所有调度策略： #chrt -m SCHED_OTHER min/max priority : 0/0 SCHED_FIFO min/max priority : 1/99 SCHED_RR min/max priority : 1/99 SCHED_BATCH min/max priority : 0/0 SCHED_IDLE min/max priority : 0/0 1/99是实时进程使用的调度策略 0/0非实时进程使用的调度策略 FIFO 先进先出，谁先来，处理谁 RR 分时算法，程序被分配时间片，每个程序使用一会儿 OTHER 跟RR一样论询，如果没有特殊情况跟RR一样 BATCH 批处理，来5个人处理一次 IDLE FIFO 和 RR 属于实时进程（也叫静态优先级进程）的调度策略。优先级高于非实时进程（动态优先级进程） BATCH和OTHER ，如果一个实时进程准备运行，调度器总是试图先调度实时进程。 BATCH是2.6内核新加入的策略，这种类型额进程一般都是后台处理进程，总是倾向于跑完自己的时间片，没有交互性，对于这样的调度策略，调度器一般给的优先级比较低。如果有一个程序设置成BATCH，cpu会尽量给这个程序使用，但是一旦有OTHER，那么先给OTHER策略的程序 ，也就是OTHER优先级高 IDLE是只要有其他程序运行，我就不运行，也就是只有在cpu空闲的时候就把这个程序设置成IDLE OTHER&gt;BATCH&gt;IDLE IDLE的nice值比19还要低，rhel6新出的，rhel5没有 修改调度策略： #chrt 调度中实时进程的优先级可以让一个非实时进程以实时进程的方式运行，也相当与提高了这个进程的优先级 #cat 执行一个cat命令 #ps -e | grep cat 查看PID 6183 #chrt -p 6183 查看默认策略 #chrt -p 10 6183 优先级改成10，默认策略变成RR #chrt -f 20 cat 在开始运行cat命令的时候制定优先级，策略为FIFO top 中有RT字样的是实时进程： 例如： 2 root RT -5 0 0 0 S 0.0 0.0 0:00.00 migration/0 # ps -el //结果中NI值为-的是实时进程 只要实时进程处于r状态 那么非实时进程就没有时间片了 pid 10以内的进程为内核进程 是内核的一部分cpu亲和力cpu亲和力（也就是cpu绑定） 系统资源不够用，一台机子上跑了几个比较重要的服务，每天我们还要在上面进行个备份压缩等处理，网络长时间传输，这就很影响本就不够用的系统资源； 这个时候我们就可以把一些不太重要的比如copy/备份/同步等工作限定在一颗cpu上，或者是多核的cpu的一颗核心上进行处理，虽然这不一定是最有效的方法，但可以最大程度上利用了有效资源，降低那些不太重要的进程占用cpu资源； cpu绑定： 把一个程序绑定到一个cpu上去执行 好处： 1.多核cpu使用的时候，内核会优先使用0，1号cpu，假如有4核，2，3号可能会利用率比较低，那么我们可以把不同的程序绑定到2，3号cpu上以提高cpu使用率 2.可以提高缓存命中率 安装软件： util-linux-ng-2.17.2-12.9.el6.x86_64 查看cpu绑定: # taskset -p 1 pid 1&#39;s current affinity mask: 3 affinity //cpu亲和力(也就是cpu绑定) 假如4核cpu：3 2 1 0 cpu编号从0开始 2^3+2^2+2^1+2^0=和 再把和换算成16进制就是mask后面的值，可以利用这个算式和值找到我们正在使用哪个cpu 绑定cpu: #taskset -c 1，2 cat 把cat程序绑定到1号和2号cpu上执行 修改已经启动进程的cpu绑定： # taskset -pc 0 11358 让进程11358运行在0号cpu上 ======================================= 多并发： 多核情况下以线程方式运行效果更好一些,单核的话区别就不大了 进程和线程 有本质的区别 线程模式 创建和撤销开销小 资源竞争问题 进程模式 创建和撤销开销大 没有资源竞争问题 如apache 线程和进程模式 多核 每秒处理并发访问量 达到1000 ，这时线程表现较好 测试： #ab -n 1000 -c 1000 http://localhost 当2000时 apache 挂了 ab命令最多支持2个万测试 测试nginx 单进程和多进程 加大页面内容 cps 每秒的并发连接数(TCP) qps 每秒的并发请求数 GET/HEAD/DELETE/PUT -------------------------------------- 平衡cpu中断: 上下文切换: cpu读取数据的时候会从L1,L2,L3等一级二级三级缓存里面读，先去L1找，L1没有，去L2里找，L2没有，去L3找，L3没有找内存，内存没有找硬盘。 如果A被处理要把A放到L1里面，那么现在B需要处理，所以需要把B放到L1里，那么原来L1里的A清除掉，这就是上下文切换 当产生中断的时候就会产生一次上下文切换 a还没有运行完的时候，出现一个中断信号，cpu会处理这个中断，处理完成再回来处理a 那么打断多了好还是少了好?不一定，多有多的好处，少有少的好处 打断少了会增加吞吐量，但是响应就会变慢 所以在中断的时候要考虑是要吞吐量还是要响应能力 假如我是4核cpu： 我们发现大部分中断都集中在一个cpu上，所以我们可以平衡中断，把中断平分到其他cpu上去 #cat /proc/interrupts 显示系统内已经注册的中断 第1列注册了的中断编号 第2，3，4，5列 每个cpu处理了多少次中断 自动平衡，rhel6直接启动这个命令就可以 #/etc/init.d/irqbalance start rhel5里面没有自动平衡，只能手工平衡 rhel6如果想手工平衡的话，需要关闭上面的irqbalance服务 手工平衡实验： 确定想调整谁，就记住他的中断编号，然后去/proc/irq/里面找到相应的编号，比如23 /proc/irq/23 #cat /proc/irq/23/smp_affinity 这个文件里的值就是亲和力，算法一样跟之前一样 实验： #watch -n 1 cat /proc/interrupts 1秒钟监控一次后面的程序 然后拿另一台机器一直ping这台监控机器，我们发现网卡中断在cpu0上，那么现在要求把网卡中断交给cpu1处理 28: 25202611 1428501 PCI-MSI-edge eth0 #echo 2 &gt; /proc/irq/28/smp_affinity 发现中断会跑到第2块儿cpu上 cpu监控相关监测命令 系统负载 uptime 举例： 处理器 1 1分钟的负载值 &lt;=3 CPU使用率 TOP工具： 第三排信息值 Cpu(s): 消耗CPU处理时间的百分比 （iostat 看更全的单词） 95.8%us, 用户态 1.1%sy, 内核态 2.6%ni, 优先级切换 0.0%id, CPU空闲 ＊＊＊ 0.0%wa, 等待，IO输入输出等待 0.0%hi, 硬中断 什么叫中断呢？ 每个硬件都有会中断地址/proc/interrupts 0.5%si, 软中断 0.0%st CPU偷窃时间 ，与xen虚拟化有关系 [root@xen ~]# iostat Linux 2.6.18-194.el5xen (xen.pg.com) 2011年01月18日 avg-cpu: %user %nice %system %iowait %steal %idle 2.75 0.75 1.31 1.89 0.01 93.29 Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtn sda 7.46 152.21 82.99 1240948 676550 sda1 1.05 11.99 0.03 97781 256 sda2 0.00 0.00 0.00 6 0 sda5 0.12 5.22 0.00 42549 0 sda6 0.05 1.30 0.00 10621 16 sda7 0.01 0.22 0.00 1818 38 sda8 0.01 0.16 0.00 1315 0 sda9 6.22 133.26 82.95 1086402 676240 观测占用CPU时间 top (这个命令本身就挺消耗CPU时间的) 找出 R 状态的进程。 临时: 使用renice 调整进程的优先级。 治本： 要明确这个进程的功能了。 如果有问题的，结束，修改程序。 如果没有问题，是正常的进程。花钱买CPU。 举例： WEB服务器（PHP） CPU负载跟CPU使用率都很高，而且CPU不能扩充。 怎么办？ “集群“ 进程列表: ps 只对具体进程进行观测 #ps -eo &quot;pid,comm,rss,pcpu&quot; --sort pcpu 升序 #ps -eo &quot;pid,comm,rss,pcpu&quot; --sort -pcpu 降序 #ps -eo &quot;pid,comm,rss,pcpu,rtprio,ni,pri,stat&quot; --sort -pcpu 实时进程优先级 如果显示为空，说明不是实时进程 man ps AIX FORMAT DESCRIPTORS This ps supports AIX format descriptors, which work somewhat like the formatting codes of printf(1) and printf(3). For example, the normal default output can be produced with this: ps -eo &quot;%p %y %x %c&quot;. The NORMAL codes are described in the next section. CODE NORMAL HEADER %C pcpu %CPU %G group GROUP %P ppid PPID %U user USER %a args COMMAND %c comm COMMAND %g rgroup RGROUP %n nice NI %p pid PID %r pgid PGID %t etime ELAPSED %u ruser RUSER %x time TIME %y tty TTY %z vsz VSZ 多核心监测 mpstat工具： mpstat是Multiprocessor Statistics的缩写，是实时系统监控工具。其报告与CPU的一些统计信息，这些信息存放在/proc/stat文件中。在多CPUs系统里，其不 但能查看所有CPU的平均状况信息，而且能够查看特定CPU的信息。mpstat最大的特点是：可以查看多核心cpu中每个计算核心的统计数据；而类似工 具vmstat只能查看系统整体cpu情况 #mpstat -P ALL 1 1000 #mpstat [-P {|ALL}] [internal [count]] 参数 解释 -P {|ALL} 表示监控哪个CPU， cpu在[0,cpu个数-1]中取值 internal 相邻的两次采样的间隔时间、 count 采样的次数，count只能和delay一起使用 当没有参数时，mpstat则显示系统启动以后所有信息的平均值。有interval时，第一行的信息自系统启动以来的平均信息。从第二行开始，输出为前一个interval时间段的平均信息。 [root@xen ~]# mpstat Linux 2.6.18-194.el5xen (xen.pg.com) 2011年01月18日 12时16分20秒 CPU %user %nice %sys %iowait %irq %soft %steal %idle intr/s 12时16分20秒 all 2.70 0.62 1.19 1.64 0.00 0.00 0.03 93.82 212.95 [root@xen ~]# mpstat -P ALL Linux 2.6.18-194.el5xen (xen.pg.com) 2011年01月18日 12时16分29秒 CPU %user %nice %sys %iowait %irq %soft %steal %idle intr/s 12时16分29秒 all 2.70 0.62 1.19 1.64 0.00 0.00 0.03 93.82 213.14 12时16分29秒 0 3.10 0.63 1.39 2.71 0.00 0.00 0.04 92.14 121.29 12时16分29秒 1 2.31 0.61 0.98 0.58 0.00 0.00 0.02 95.50 91.85 [root@xen ~]# mpstat -P 0 1 10 每隔一秒取一次，取10次 高级系统检查 sar -u 使用率 sar -q 系统平均负载 立刻采集显示 &lt;interval&gt; &lt;count&gt; sar -q 1 10 sar -u 1 10 即时性的报告 获取CPU各个核心信息 sar -u -P ALL 1 [root@xen sa]# sar -I ALL 1 获取每个进程CPU使用率，注意默认这个数据不记录在数据库。 sar -x ALL 1 3 * 指定文件读取 sar -u -f /var/log/sa/sa28 输出SAR格式数据 sar -u 1 10 -o /tmp/ooo 根据时间过滤数据 sar -u -s 13:00:00 -e 13:05:01 [root@xen sa]# sar -u -f /var/log/sa/sa18 -s 09:50:01 -e 12:40:01 Linux 2.6.18-194.el5xen (xen.pg.com) 2019年01月18日 09时50分01秒 CPU %user %nice %system %iowait %steal %idle 10时00分01秒 all 2.08 0.00 0.32 0.15 0.01 97.44 10时10分01秒 all 1.74 0.00 0.36 0.21 0.01 97.68 10时20分01秒 all 3.04 0.00 0.33 0.02 0.01 96.60 10时30分01秒 all 6.22 0.00 1.16 0.15 0.01 92.45 10时40分01秒 all 2.79 0.17 0.75 0.16 0.01 96.13 10时50分01秒 all 5.55 9.29 9.93 15.51 0.03 59.68 11时00分01秒 all 4.32 0.50 1.28 0.49 0.01 93.40 11时10分01秒 all 0.03 0.00 0.02 0.00 0.01 99.94 11时20分01秒 all 0.03 0.00 0.03 0.06 0.01 99.87 11时30分01秒 all 0.04 0.00 0.01 0.03 0.01 99.91 11时40分01秒 all 0.04 0.08 0.04 0.04 0.01 99.79 11时50分01秒 all 3.06 0.00 0.67 0.03 0.01 96.23 12时00分01秒 all 2.72 0.00 0.70 1.17 0.23 95.17 12时10分01秒 all 0.64 0.00 0.28 0.04 0.05 98.99 12时20分01秒 all 3.52 0.00 0.66 0.04 0.07 95.70 12时30分01秒 all 4.52 0.00 1.05 0.04 0.08 94.31 12时40分01秒 all 4.31 0.08 1.22 0.05 0.08 94.27 Average: all 2.63 0.60 1.11 1.07 0.04 94.56 =============================================== 通过上面那些工具可以搜集一段时间的数据，通过数据可以分析系统状态 比如在公司写服务器运行报告的时候，怎么写，可以写这些工具搜集的信息 实际上搜集那些信息不用我们自己去做，如果安装了sysstat工具，那么就会有下面的计划任务 # ls /etc/cron.d/sysstat /etc/cron.d/sysstat 这个计划任务会给我生成下面的记录文件，这些记录文件记录了cpu，mem和net，io等等所有的信息 # cd /var/log/sa/ sa01 sa07 这些文件后面的数字是日志，比如01就是1号 # sar -f sa01 查看那些记录文件 [root@xen sa]# sar -f /var/log/sa/sa18 Linux 2.6.18-194.el5xen (xen.pg.com) 2019年01月18日 09时50分01秒 CPU %user %nice %system %iowait %steal %idle 10时00分01秒 all 2.08 0.00 0.32 0.15 0.01 97.44 10时10分01秒 all 1.74 0.00 0.36 0.21 0.01 97.68 10时20分01秒 all 3.04 0.00 0.33 0.02 0.01 96.60 10时30分01秒 all 6.22 0.00 1.16 0.15 0.01 92.45 10时40分01秒 all 2.79 0.17 0.75 0.16 0.01 96.13 10时50分01秒 all 5.55 9.29 9.93 15.51 0.03 59.68 11时00分01秒 all 4.32 0.50 1.28 0.49 0.01 93.40 11时10分01秒 all 0.03 0.00 0.02 0.00 0.01 99.94 11时20分01秒 all 0.03 0.00 0.03 0.06 0.01 99.87 11时30分01秒 all 0.04 0.00 0.01 0.03 0.01 99.91 11时40分01秒 all 0.04 0.08 0.04 0.04 0.01 99.79 11时50分01秒 all 3.06 0.00 0.67 0.03 0.01 96.23 12时00分01秒 all 2.72 0.00 0.70 1.17 0.23 95.17 12时10分01秒 all 0.64 0.00 0.28 0.04 0.05 98.99 12时20分01秒 all 3.52 0.00 0.66 0.04 0.07 95.70 12时30分01秒 all 4.52 0.00 1.05 0.04 0.08 94.31 12时40分01秒 all 4.31 0.08 1.22 0.05 0.08 94.27 Average: all 2.63 0.60 1.11 1.07 0.04 94.56 [root@xen sa]# sar -u -f /var/log/sa/sa18 Linux 2.6.18-194.el5xen (xen.pg.com) 2019年01月18日 09时50分01秒 CPU %user %nice %system %iowait %steal %idle 10时00分01秒 all 2.08 0.00 0.32 0.15 0.01 97.44 10时10分01秒 all 1.74 0.00 0.36 0.21 0.01 97.68 10时20分01秒 all 3.04 0.00 0.33 0.02 0.01 96.60 10时30分01秒 all 6.22 0.00 1.16 0.15 0.01 92.45 10时40分01秒 all 2.79 0.17 0.75 0.16 0.01 96.13 10时50分01秒 all 5.55 9.29 9.93 15.51 0.03 59.68 11时00分01秒 all 4.32 0.50 1.28 0.49 0.01 93.40 11时10分01秒 all 0.03 0.00 0.02 0.00 0.01 99.94 11时20分01秒 all 0.03 0.00 0.03 0.06 0.01 99.87 11时30分01秒 all 0.04 0.00 0.01 0.03 0.01 99.91 11时40分01秒 all 0.04 0.08 0.04 0.04 0.01 99.79 11时50分01秒 all 3.06 0.00 0.67 0.03 0.01 96.23 12时00分01秒 all 2.72 0.00 0.70 1.17 0.23 95.17 12时10分01秒 all 0.64 0.00 0.28 0.04 0.05 98.99 12时20分01秒 all 3.52 0.00 0.66 0.04 0.07 95.70 12时30分01秒 all 4.52 0.00 1.05 0.04 0.08 94.31 12时40分01秒 all 4.31 0.08 1.22 0.05 0.08 94.27 Average: all 2.63 0.60 1.11 1.07 0.04 94.56 [root@xen sa]# sar -q -f /var/log/sa/sa18 Linux 2.6.18-194.el5xen (xen.pg.com) 2019年01月18日 09时50分01秒 runq-sz plist-sz ldavg-1 ldavg-5 ldavg-15 10时00分01秒 0 220 0.03 0.12 0.17 10时10分01秒 0 220 0.25 0.10 0.10 10时20分01秒 0 220 0.02 0.07 0.08 10时30分01秒 0 220 0.16 0.19 0.12 10时40分01秒 0 225 0.32 0.22 0.13 10时50分01秒 4 227 1.65 1.28 0.69 11时00分01秒 0 220 0.00 0.24 0.40 11时10分01秒 0 220 0.00 0.02 0.19 11时20分01秒 0 221 0.00 0.00 0.08 11时30分01秒 0 221 0.00 0.00 0.02 11时40分01秒 0 221 0.08 0.02 0.01 11时50分01秒 2 221 0.42 0.22 0.08 12时00分01秒 0 229 0.17 0.13 0.08 12时10分01秒 0 232 0.00 0.01 0.03 12时20分01秒 2 228 0.30 0.16 0.09 12时30分01秒 1 228 0.33 0.18 0.11 12时40分01秒 1 228 0.31 0.20 0.11 Average: 1 224 0.24 0.19 0.15 =============================================== mem 内存的特点: 速度快,所存数据不会保存,内存的最大消耗来源于进程 测试内存速度： 安装软件:memtest86+-4.10-2.el6.x86_64.rpm 执行 memtest-setup命令 多出一个操作系统 内存存储的数据有那些? 程序代码,程序定义的变量(初始化和未初始化),继承父进程的环境变量,进程读取的文件,程序需要的库文件.还有程序本身动态申请的内存存放自己的数据 除了进程以外还有内核也要占用,还有buffer和cache,还有共享内存(如共享库) 我们使用管道符| 进程之间通讯也要使用到内存,socket文件 那我们可以优化哪部分程序? 内核内存不能省 buffer/cache不能省 进程通讯不省 系统支持的内存大小： 2的32次方 32位系统支持的内存 windows受到约束 linux只要换个pae(物理地址扩展)内核 可以支持2的36次方 2的64次方 查看系统内存信息 [root@localhost ~]# free -m total used free shared buffers cached Mem: 1010 981 29 0 145 649 -/+ buffers/cache: 186 824 Swap: 2047 0 2047 share 在6之前包括6，这个地方永远都是0，已经被废弃了，rhel7里面已经可以正常显示 # cat /proc/meminfo | grep -i shm share就是这个值,free和vmstat都是从/proc/meminfo文件里面搜集的信息 Shmem: 26620 kB used包括buffers和cached，是已使用的内存+buffers+cached ，剩下的是free -/+ buffers/cache: 186 824 186是减去buffers和cached之后的物理内存，824是总内存减去186之后的值 buffers 索引缓存 存inode信息 cached 页缓存 存block信息 catched实验: # watch -n 0.5 free -m 监控内存信息 在另外一个窗口dd一个1G的文件，观察buffer和cache # dd if=/dev/zero of=abc bs=1000M count=1 从/dev/zero里面读了1G数据到cache里面 dd之前: total used free shared buffers cached Mem: 3724 619 3105 0 59 242 dd之后: total used free shared buffers cached Mem: 3724 1652 2071 0 59 1246 buffers实验: #find / 把/下所有文件都列出来 会读到directory block，通过inode找到文件名，有多少个文件就会读多少次目录块，所以我们现在的查找实际上是块操作，所以使用的是buffer find之后: total used free shared buffers cached Mem: 3724 1713 2010 0 95 1246 [root@localhost ~]# vmstat procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------ r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 48584 118300 663564 0 0 12 23 60 240 3 2 95 0 0 procs: r 正在运行或可运行的进程数，如果长期大于cpu个数，说明cpu不足，需要增加cpu b block 但是这个是阻塞，表示在等待资源的进程数，比如正在等待I/O、或者内存交换等。 由于硬盘速度特别慢而导致内存同步的时候没成功，那么现在告诉程序，说你先不要产生数据，这就是阻塞 ！！！！！ ---&gt; b越大证明硬盘压力很大 memory swpd 切换到内存交换区的内存数量(k表示)。如果swpd的值不为0，或者比较大，比如超过了100m，只要si、so的值长期为0，系统性能还是正常 free 当前的空闲页面列表中内存数量(k表示) buff 作为buffer cache的内存数量 cache: 作为page cache的内存数量 swap si swapin 把swap分区的数据放到内存 so swapout 把内存数据放到磁盘 通过上面两个可以分析内存的使用情况，如果swap有数据是不是内存不够用了？不一定，因为系统会把一些用不到的进程放到swap里面，把腾出来的空间做缓存，如果发现si,so里面有数据，说明内存可能不够用了 IO bi blockin 这个是块进来 ，把块儿从硬盘搬进来，也就是说bi是读 bo blockout 把块儿从内存搬到硬盘，也就是说bo是写 实验: # vmstat -1 # dd if=/dev/zero of=/aa bs=1G count=1 //这条命令之后查看bo的数值，发现bo产生数据 记录了1+0 的读入 记录了1+0 的写出 1073741824字节(1.1 GB)已复制，4.90872 秒，219 MB/秒 #find / //这条命令之后查看bi，发现bi产生数据 如果 一直开着vmstat发现bo 5秒钟一个数，这就是因为脏数据5秒钟一次 如果要拿这个数据做图，bo的第一个数据一定要剔除到，这个数字是上一次重启到敲vmstat这条命令之间的平均值，所以这个数字没用 system 显示采集间隔内发生的中断数 in 列表示在某一时间间隔中观测到的每秒设备中断数。 cs列表示每秒产生的上下文切换次数 cpu： 剩下的就是cpu的各种使用百分比 以上解释都可以查看man手册:#man vmstat ======================================== buffer/cache 根据时间和数据大小同步 主要用于写缓存 内核里面的一套系统：伙伴系统，负责把内存里面的数据往硬盘上搬 rhel5： kswapd pdflush kswapd负责说什么时候搬数据 pdflush负责干活儿,他会一直开启着 rhel6: kswapd负责说什么时候搬数据，但是干活儿的不是pdflush了 有需要搬的数据的时候，才产生一个进程---&gt; flush 主设备号:从设备号 负责搬数据 已经同步到硬盘的数据就是干净数据 # cat /proc/sys/vm/dirty_ 查看的是脏数据（缓存内还没来得急同步到硬盘的数据） dirty_background_bytes dirty_expire_centisecs dirty_background_ratio dirty_ratio dirty_bytes dirty_writeback_centisecs [root@localhost ~]# cat /proc/sys/vm/dirty_expire_centisecs //想知道这里面是什么可以使用下面的man或者kernel-doc查看 2999 //单位百分之一秒，这里也就是30秒，30秒之后标记为脏数据，意味着用户写的数据在30秒之后才有可能被刷入磁盘，在这期间断电可能会丢数据 [root@localhost ~]# cat /proc/sys/vm/dirty_writeback_centisecs 499 // 5秒钟往硬盘同步一次数据5秒同步一次脏数据（在缓存中的） 假如我内存1G 1秒 100M 2秒 300M 3秒 400M 4秒 400M 还没到5秒，但是内存使用已经超过1G了，这时候怎么办？下面的文件来解决 [root@localhost ~]# cat /proc/sys/vm/dirty_ratio 40 //如果单个进程占用的buffer/cache达到内存总量的40%,立刻同步。 假如我内存1G，一个进程 1秒 1M 2秒 3M 3秒 4M 4秒 40M 那要是1000个进程呢？这时候怎么办？下面的文件来解决 [root@localhost ~]# cat /proc/sys/vm/dirty_background_ratio 10 //所有进程占用的buffer/cache使得剩余内存低于内存总量的10%，立刻同步 # cat /proc/sys/vm/dirty_background_bytes //上面的ratio文件用百分比，这个用字节限制，但是百分比存在的时候，字节不生效 0 如果服务器是一个数据服务器，比如NAS，dirty_writeback和dirty_ratio里面的数值可以适当改大一点,存储需要频繁读数据的时候，可以直接从内存里面读，而且在同步数据的时候会使用更大的连续的块儿。 ============================================= 释放buffer/cache [root@localhost ~]# cat /proc/sys/vm/drop_caches 0 1 释放buffer 2 释放cache 3 buffer/cache都释放 需要编译安装一个程序，在make的时候报错内存不足，这时候就可以释放一下缓存，一般情况下不要用 # watch -n 0.5 free -m # echo 3 &gt; /proc/sys/vm/drop_caches ============================= 内存如果真耗尽了，后果无法预测 OOM进程 OOM killer out 当内存耗尽的时候，系统会出现一个OOM killer进程在系统内随机杀进程 每个运行的程序都会有一个score(分)，这个是不良得分，所以谁分高，就杀谁 如果还不行的话，他会自杀，也就是杀kernel，就会出现内核恐慌(panic),所以会死机 实验： #cat # ps -el | grep cat 0 S 0 9566 2975 0 80 0 - 25232 n_tty_ pts/1 00:00:00 cat # cat /proc/9566/oom_score 1 # cat /proc/9566/oom_adj 0 可以用这个值干预上面oom得分 -17 15 -17免杀，15是先干掉 # echo 15 &gt; /proc/9566/oom_adj # echo f &gt; /proc/sysrq-trigger //启动OOM_KILLER 必杀一个 # cat //因为上面已经把9566的adj改成了15，所以这次启动杀死了cat进程 已杀死 swap 那么到底怎么解决内存耗尽的问题？swap 假如a，b，c已经把内存占满了，那么来了个d，内核先看看abc谁不忙，就把谁的数据先放到swap里面去，比如a不用，把a的数据放到swap里面去，释放出来的空间给d swap分区分多大？现在内存很大比如256G，那么就没必要2倍了。。。 什么样的数据才能往swap里面放？ # cat /proc/meminfo | grep -i active Active: 233836 kB Inactive: 1280348 kB Active(anon): 138780 kB Inactive(anon): 26740 kB Active(file): 95056 kB Inactive(file): 1253608 kB active活跃数据，inactive非活跃数据，又分为匿名数据和文件数据 匿名数据不能往swap里面放 文件形式的active不能往swap里放，只有文件的inactive才能往swap放 所以并不是有了swap，内存就解决了 什么时候放进去？根据swap_tendency（swap趋势） swap_tendency = mapped_ratio/2 + distress + vm_swappiness 这就是swap趋势，如果这个值到达100，就往交换分区里面放，如果小于100，尽量不往里面放，但是就算到100，也只能说内核倾向与要往swap里面放，但也不一定放 系统就只开放第三个给用户设置 # cat /proc/sys/vm/swappiness swap的喜好程度，范围0-100 60 ============================= 使用内存文件系统 #df -h tmpfs 1.9G 224K 1.9G 1% /dev/shm （共享内存） tmpfs 内存里面的临时文件系统 系统会承诺拿出50%（这里是2G）的空间来做SHM，只是承诺，实际用多少给多少，如果内存比较富裕的情况下，我们可以拿内存当硬盘使用 #mount -t tmpfs -o size=1000M tmpfs /mnt //挂内存 #dd if=/dev/zero of=/mnt/file1 bs=1M count=800 记录了800+0 的读入 记录了800+0 的写出 838860800字节(839 MB)已复制，0.310507 秒，2.7 GB/秒 //这里用的是内存的速度 # dd if=/dev/zero of=/tmp/file1 bs=1M count=800 oflag=direct 记录了800+0 的读入 记录了800+0 的写出 838860800字节(839 MB)已复制，8.77251 秒，95.6 MB/秒 //这里用的是硬盘的速度 如果临时对某一个目录有较高的io需求，可以使用上面的方法使用内存 ---------------------------------------------------------------------------------------------- # mount -t tmpfs -o size=20000M tmpfs /mnt //发现这样也可以，为什么，这只是承诺给20G，并没有实际给20G #dd if=/dev/zero of=/mnt/file1 //不指定多大，把swap关闭（如果不关会等半天），这样就会把内存耗尽， ======================== 虚拟内存和物理内存 查看： #top VIRT RES SHR 虚拟内存： 应用程序没办法直接使用物理内存，每个程序都有一个被内核分配给自己的虚拟内存 虚拟内存申请： 32位CPU,2^32也就是4G 64位cpu,2^64 每个程序都最多能申请4G的虚拟内存，但是现在这4G内存还和物理内存没关系呢，a说我先用100M，然后内核就会把100M映射给物理内存 VIRT就是程序运行的时候说申请的虚拟内存，RES就是映射的内存 为什么要有虚拟内存？ 跟开发有关系，内存是有地址空间的，开发者在调用内存的时候如果直接调用物理内存，开发者不知道哪块儿地址被占用了，所以在中间内核站出来给开发者分配，开发者只需要提出需要多大内存，由内核来解决你的内存就可以了 ------------ 程序1 程序2 4G 4G ----------- kernel ----------- 物理内存 ----------- 以上3层，第一层就是程序可以使用的虚拟内存，程序可以跟内核申请需要多少内存，内核就分配相应大小的物理内存给程序就可以了 ======================== 映射表： 概念： 内存是分页的，1个page是4k(默认值),在硬盘上分块，硬盘数据和内存数据是一一对应 问题： 条目非常多，查询特别慢 解决： 固有方法： 硬件TLB ，在cpu里面，用来解决查询映射表慢的问题，第一次查询过之后把结果缓存到TLB里面，以后再查的时候就可以直接从TLB里面提取 # yum install x86info # x86info -a 可以查询TLB信息 自定义方法： 如果page变大，条目就会变少，这样就会提高查询速度 大于4k的分页称为hugepage 巨页 ，但是这个需要程序支持 那我们现在的操作系统是否支持巨页 # cat /proc/meminfo | grep -i hugepage AnonHugePages: 26624 kB HugePages_Total: 0 我现在没有巨页 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB 说明现在我的系统支持2M的巨页 假如一个程序需要200M的巨页，那么就要把total改成100 #echo 100 &gt; /proc/sys/vm/nr_hugepages //修改巨页total数目 #mkdir dir1 #mount -t hugetlbfs none /dir1 那么现在程序使用/dir1就可以了 外翻： TLB(Translation Lookaside Buffer)传输后备缓冲器是一个内存管理单元用于改进虚拟地址到物理地址转换速度的缓存。TLB是一个小的，虚拟寻址的缓存，其中每一行都保存着一个由单个PTE组成的块。如果没有TLB，则每次取数据都需要两次访问内存，即查页表获得物理地址和取数据 ======================== 进程间通信(IPC) 种类： 进程间通信的方式有5种: 1.管道(pipe) 本地进程间通信，用来连接不同进程之间的数据流 2.socket 网络进程间通信，套接字(Socket)是由Berkeley在BSD系统中引入的一种基于连接的IPC，是对网络接口(硬件)和网络协议(软件)的抽象。它既解决了无名管道只能在相关进程间单向通信的问题，又解决了网络上不同主机之间无法通信的问题。 以上两种unix遗留下来的 3.消息队列(Message Queues) 消息队列保存在内核中，是一个由消息组成的链表。 4.共享内存段(Shared Memory) 共享内存允许两个或多个进程共享一定的存储区，因为不需要拷贝数据，所以这是最快的一种IPC。 5.信号量集(Semaphore Arrays) System V的信号量集表示的是一个或多个信号量的集合。 查看： ipc 进程间通信 #ipcs //这条命令可以看到后3种,前两种可以通过文件类型查看 含义： 管道 a | b 在内存打开一个缓冲区，a把结果存到缓冲区，b去缓冲区里面拿数据 管道通信的时候 独木桥：特点--&gt;只能一个人 单向 先进先出 3个人过桥，一个一个的过，那如果100个人过，速度会很慢，所以管道传输的数据有限 socket IE浏览器 访问网站 通过端口 端口在系统内实际不存在是个伪概念，只是一个标识， a会打开一个buffer b会打开一个buffer ，这两个buffer用来接受数据包，并且重组，交给apache的socket，apache就会去socket接受数据 消息队列 跟管道基本一样 也是独木桥，也是单向，先进先出，但是他会对消息进程排队，谁着急谁先走，那么过河的人多了之后，同样也是数据传输较慢 共享内存段 开辟一块内存，a把数据全都丢到共享内存里面，b去共享内存拿数据，而且b可以按需选择拿哪些数据 共享内存段在oracle里面肯定要使用 信号量 在a和b之间传递信号，a把一个文件锁住给b发一个信号，说这个文件我正在使用 信号所携带的数据量非常有限，只能指定信号是干什么用的 ============================================== 查看内存使用情况 [root@localhost ~]# sar -r 1 1 01时31分38秒 kbmemfree kbmemused %memused kbbuffers kbcached kbcommit %commit 01时31分39秒 6045368 1917916 24.08 67236 649020 2435764 17.73 kbcommit：保证当前系统所需要的内存,即为了确保不溢出而需要的内存(RAM+swap). %commit：这个值是kbcommit与内存总量(包括swap)的一个百分比. I/OI/O 这块是影响系统性能比较大的地方 查看速度： #hdparm -t /dev/sda 影响I/O性能的因素 1.mount 2.文件系统：日志文件系统和非日志文件系统 3.I/O调度算法 4.RAID，LVM（条带化） 网络附加存储（cpu的iowait转移到存储服务器）mount​ rw ro ​ atime noatime ​ Update inode access time for each access. This is the default.​ atime是这里影响性能的一个重要因素，可以用noatime关掉 ​ async、sync ​ [root@node1 ~]# mount /dev/sdb1 /mnt/​ [root@node1 ~]# mount -o remount,sync /dev/sdb1 /mnt/ ​ 数据传输3种:​ sync 应用程序产生数据也是写到缓存，但是会等到缓存内的脏数据同步到硬盘之后才会产生新的数据​ async 应用程序把数据写到缓存，操作系统通过伙伴程序kswapd把脏数据写到硬盘，而且会一直产生脏数据，不会等​ directIO直接IO 不会经过缓存，直接往硬盘上写 ​ 练习：​ 使用dd命令测试sync async directIO三种方式谁快谁慢 ​ 那么上面3种方式，对应用程序来说，async最快，其次是直接IO，最慢的是同步 问题： 虽然异步对应用程序快了，但是会产生问题，比如脏数据还没来得及全部同步到硬盘，突然断电了，这时候我们称硬盘上的不完整数据被损坏，或者数据不一致 解决： 断电我们阻止不了，但是我们可以重新写数据，但是现在他怎么知道哪个数据坏了？开机启动的时候使用fsck检测坏块儿问题： 但是fsck在数据量比较大的时候会非常慢，那怎么解决这个问题？ 解决： 通过Journal日志​ 在数据写到内存之前先记录一下日志，如果断电重启之后发现哪个写操作没有完成，就通过日志恢复哪个就可以了 ​ 有日志以后的数据存储过程:​ 写日志​ 写数据​ 删除日志 ext4可以关掉日志，但是只有在格式化的时候才能关 有日志的话，会变慢，那这个问题如何解决？ 创建2个分区，分别格式化为ext2、ext3，dd文件测试速度差异 记录日志的方法： #man mount Mount options for ext3 data={journal|ordered|writeback}​ journal最好，但是这种开销最大​ journal 把信息写到缓存，记录日志，记录inode和数据到日志，然后写到硬盘​ ordered 这是默认的模式把信息写到缓存，记录日志，只记录inode，然后写到硬盘​ writeback首先在缓存内把所有的写排好序,然后记录inode，最后写到硬盘​​ 性能上ordered和writeback差不多​​ 解决记录日志慢问题：​ 之所以日志慢：硬盘是一个一个的磁盘片，硬盘效率最高的时候是磁头在某一个磁道不换位置，如果数据不连续也就意味着磁头要来回在日志数据所在的磁道和数据所在的磁道跳换，如果能解决这个问题就OK了。我们可以采用日志分离的方式 ​ 日志分离:​ 两块硬盘，一块存日志，一块存数据，必须是两块磁盘，两个分区是不可以的​ #mke2fs -O journal_dev /dev/sdb5 //把这个设备格式化成专门记录日志的设备​ #mkfs.ext3 -J device=/dev/sdb5 /dev/sdb1 //在格式化sdb1的时候声明日志记录在sdb5上，因为是做实验，这里用的是分区​ #mount -o data=journal /dev/sdb1 /mnt/​ 在工作当中一般不用日志分离，因为一般我们的服务器上都是raid，所以速度不会慢到哪去，如果不用raid的设备就可能要用到日志分离 各种日志文件系统:ext3ext4jfs IBM的 恢复相当快xfs 处理大文件性能特别好reiserfs suse用的文件系统btresfs 处理小文件速度特别快 zfs z是26个英文字母的最后一个，寓意在他之后不会再有其他文件系统 rhel7已经放弃ext4了，使用xfs I/O调度算法查看调度算法 [root@localhost ~]# cat /sys/block/sda/queue/scheduler noop anticipatory deadline [cfq] //这里是所有的算法，被中括号扩起来的cfq就是默认的调度算法 CFQ (Completely Fair Queuing 完全公平的排队)(elevator=cfq)： 这是默认算法，对于通用服务器来说通常是最好的选择。它试图均匀地分布对I/O带宽的访问。在多媒体应用, 总能保证audio、video及时从磁盘读取数据。但对于其他各类应用表现也很好。每个进程一个queue，每个queue按照上述规则进行merge 和sort。进程之间round robin调度，每次执行一个进程的4个请求。 Deadline (elevator=deadline)： 这个算法试图把每次请求的延迟降至最低。该算法重排了请求的顺序来提高性能。 NOOP (elevator=noop): 这个算法实现了一个简单FIFO队列。他假定I/O请求由驱动程序或者设备做了优化或者重排了顺序(就像一个智能控制器完成的工作那样)。在有些SAN环境下，这个选择可能是最好选择。适用于随机存取设备, no seek cost，非机械可随机寻址的磁盘。 Anticipatory (elevator=as): 这个算法推迟I/O请求，希望能对它们进行排序，获得最高的效率。同deadline不同之处在于每次处理完读请求之后, 不是立即返回, 而是等待几个微妙在这段时间内, 任何来自临近区域的请求都被立即执行. 超时以后, 继续原来的处理.基于下面的假设: 几个微妙内, 程序有很大机会提交另一次请求.调度器跟踪每个进程的io读写统计信息, 以获得最佳预期. ------------------------------------- APP app发出请求到下层 buffer cache io调度器 硬盘 cfq 完全公平队列 a，b，c三个app发出请求， cfq会为每个app准备一个队列，cfg会在每个队列里面一次取4个请求，如果还有剩余请求，继续再取，总之是4个4个的取，他的好处就是每个app都会得到响应 每个app自己的请求可能都是连续的，但是app和app之间他们可能就不会在同一个磁道上，这样就会增加磁头的寻址 所以cfg适用于多媒体或者说桌面级系统 deadline deadline会把a,b,c的所有请求放到同一个队列，然后对他们进行排序，然后在把统一类型的请求比如读的请求放到fifo里面(fifo就是先进先出) 比如： a 11 12 13 47 b 31 32 38 39 c 27 28 41 42 deadline 11 12 13 27 28 31 32 38 39 41 42 47 先处理11，然后12，13，。。。。直到最后的47 但是deadline会使a进入饿死状态(比如47)，怎样解决 我们会给每一个请求规定一个时间（过了这个时间就饿死了），如果发现某一个请求到时间了，那么停止现在其他所有的操作去处理到达规定时间的那个请求 11 12 13 27 28 31 32 38 39 41 42 47 anticipatory跟deadline差不多,3.0的内核里面已经砍掉了，但是跟deadline不一样的地方是处理11，12，13完事儿之后先等一下，看还有没有14，15如果没有的话再继续 noop直接fifo，谁先来，先处理谁 ssd的硬盘会使用 san存储也会用到 -------------------------------------------------------------------- 对IO调度使用的建议 Deadline I/O scheduler 使用轮询的调度器,简洁小巧,提供了最小的读取延迟和尚佳的吞吐量,特别适合于读取较多的环境(比如数据库,Oracle 10G 之类). Anticipatory I/O scheduler 假设一个块设备只有一个物理查找磁头(例如一个单独的SATA硬盘),将多个随机的小写入流合并成一个大写入流,用写入延时换取最大的写入吞吐量.适用于大多数环境,特别是写入较多的环境(比如文件服务器)Web,App等应用我们可以采纳as调度. CFQ I/O scheduler使用QoS策略为所有任务分配等量的带宽,避免进程被饿死并实现了较低的延迟,可以认为是上述两种调度器的折中.适用于有大量进程的多用户系统 设置调度方法： linux启动时设置默认IO调度,让系统启动时就使用默认的IO方法,只需在grub.conf文件中加入类似如下行 kernel /vmlinuz-2.6.24 ro root=/dev/sda1 elevator=deadline查看I/O状态[root@localhost ~]# sar -d 1 100 Linux 2.6.32-358.el6.x86_64 (mail.robin.com) 2013年11月21日 _x86_64_ (1 CPU) 22时25分22秒 DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util 22时25分23秒 dev11-0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 22时25分23秒 dev8-0 844.58 0.00 10987.95 13.01 0.54 0.65 0.48 40.84 时间 设备主从号 每秒读写请求次数 每秒读多少个扇区 每秒写的扇区个数 平均请求大小 平均队列长度 等待时间 tps 每秒请求数 rd_sec/s 每秒读的扇区数 wr_sec/s 每秒写的扇区数 avgrq-sz 平均每个请求的大小(读写) avgqu-sz 平均队列长度 队列越长io性能越低 await io请求消耗的平均时间(毫秒) 所有时间(包括等待时间和服务时间) await-svctm=io的等待时间 差值越大 io越繁忙 等待时间：假如现在有块硬盘，处理速度每秒能处理10个请求，那么我现在给他发15个，处理不过来怎么办？排队 await就是从进队列一直到处理完一共用了多长时间 svctm：调度器处理这个排队的请求用了多长时间，比如你排队看医生，医生给你看病花了多长时间 %util io处理io请求所消耗的cpu百分比 [root@localhost ~]# iostat -k Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtn scd0 0.00 0.02 0.00 216 0 sda 653.44 28.39 4613.55 365863 59445489 [root@localhost ~]# iostat -x 1 Device: rrqm/s wrqm/s r/s w/s rsec/s wsec/s avgrq-sz avgqu-sz await svctm %util scd0 0.00 0.00 0.00 0.00 0.03 0.00 6.86 0.00 2.76 2.76 0.00 sda 2.46 500.26 0.82 651.74 56.41 9215.99 14.21 0.69 1.06 0.73 47.67 rrqm/s 每秒合并的读请求次数 wrqm/s 每秒合并的写请求次数 a对某一个块发出一个读请求，但是现在因为硬盘速度慢，还没来得及处理，结果a又对这个块发出一个读请求，这样我们就可以把这两个读请求合并到一起 a对某一个块发出一个写请求，比如把某一块数据改成123，但是没来得及处理，a又对这个块儿发出一个写请求要把数据改成789，那么这个最后的结果是789，所以我们可以把这两个写请求合并 filesystem（了解）需要一些编译工具，安装“开发工具” “开发库” [root@localhost ~]# yum groupinstall &quot;Development Libraries&quot; &quot;Development Tools&quot; 安装与现有内核版本相对应的src.rpm [root@localhost ~]# uname -r 2.6.18-308.el5xen [root@localhost ~]# rpm -ivh /tmp/kernel-2.6.18-308.el5.src.rpm rhel5会将src .rpm释放到/usr/src/redhat/中（rhel6会释放到当前用户家目录下的rpmbuild） [root@localhost ~]# cd /usr/src/redhat/ [root@localhost redhat]# ls BUILD RPMS SOURCES SPECS SRPMS BUILD 操作源码过程中产生的数据 RPMS 存放制作好的rpm SOURCES 存放程序的源代码和相关文件 SPECS 存放制作rpm所使用的spec脚本 SRPMS 存放src.rpm [root@localhost redhat]# cd SPECS/ [root@localhost SPECS]# ls -l 总计 820 -rw-r--r-- 1 root root 834836 2010-03-17 kernel-2.6.spec [root@localhost SPECS]# rpmbuild -bp --target=$(uname -m) kernel-2.6.spec [root@localhost ~]# cd /usr/src/redhat/BUILD/kernel-2.6.18/linux-2.6.18.i686 [root@localhost linux-2.6.18.i686]# cp /boot/config-2.6.18-308.el5xen .config 编辑Makefile，EXTRAVERSION = -308.el5xen 与当前内核版本相对应 [root@localhost linux-2.6.18.i686]# vim Makefile EXTRAVERSION = -308.el5xen #第四行 [root@localhost linux-2.6.18.i686]# make oldconfig 在编译模块的过程中需要 .tmp_versions临时目录 [root@localhost linux-2.6.18.i686]# mkdir .tmp_versions 在字符菜单中找到文件系统文件系统，在其中找到Reiserfs、JFS、XFS，使用M选中（ext4默认就已经有了） [root@localhost linux-2.6.18.i686]# make menuconfig File systems ---&gt; &lt;M&gt; Reiserfs support &lt;M&gt; JFS filesystem support &lt;M&gt; XFS filesystem support 单独编译3个文件系统模块 [root@localhost linux-2.6.18.i686]# make fs/xfs/xfs.ko [root@localhost linux-2.6.18.i686]# make fs/jfs/jfs.ko [root@localhost linux-2.6.18.i686]# make fs/reiserfs/reiserfs.ko 在系统内核模块目录中，创建3个文件系统的对应目录，并将编译好的模块拷贝到对应的目录中 [root@localhost linux-2.6.18.i686]# mkdir /lib/modules/2.6.18-308.el5xen/kernel/fs/jfs [root@localhost linux-2.6.18.i686]# mkdir /lib/modules/2.6.18-308.el5xen/kernel/fs/reiserfs [root@localhost linux-2.6.18.i686]# mkdir /lib/modules/2.6.18-308.el5xen/kernel/fs/xfs [root@localhost linux-2.6.18.i686]# cp fs/jfs/jfs.ko /lib/modules/2.6.18-308.el5xen/kernel/fs/jfs [root@localhost linux-2.6.18.i686]# cp fs/reiserfs/reiserfs.ko /lib/modules/2.6.18-308.el5xen/kernel/fs/reiserfs [root@localhost linux-2.6.18.i686]# cp fs/xfs/xfs.ko /lib/modules/2.6.18-308.el5xen/kernel/fs/xfs ext4的模块rhel5u8中默认就有，放在以下位置 [root@localhost linux-2.6.18.i686]# ls /lib/modules/2.6.18-194.el5/kernel/fs/ext4/ext4.ko /lib/modules/2.6.18-194.el5/kernel/fs/ext4/ext4.ko 检查模块依赖性并加载模块 [root@lcoalhost linux-2.6.18.i686]# depmod -a [root@lcoalhost linux-2.6.18.i686]# modprobe xfs [root@localhost linux-2.6.18.i686]# modprobe jfs [root@localhost linux-2.6.18.i686]# modprobe reiserfs [root@localhost linux-2.6.18.i686]# modprobe ext4 安装创建文件系统的工具 ext4： [root@localhost tmp]# yum install e4fsprogs JFS: [root@localhost tmp]# tar xf jfsutils-1.1.14.tar.gz ./configure &amp;&amp; make &amp;&amp; make install XFS: [root@localhost tmp]# tar xf xfsprogs_2.9.8-1.tar.bz2 ./configure &amp;&amp; make &amp;&amp; make install REISERFS: [root@localhost tmp]# tar xf reiserfsprogs-3.6.21.tar.bz2 ./configure &amp;&amp; make &amp;&amp; make install 创建4个分区每个2G，分别给4个分区创建4种文件系统，并挂载到对应名称的目录 [root@localhost tmp]# mkfs.xfs /dev/sda13 [root@localhost tmp]# mkfs.jfs /dev/sda12 [root@localhost tmp]# mkreiserfs /dev/sda11 [root@localhost tmp]# mkfs.ext4 /dev/sda10 [root@localhost tmp]# mkdir /xfs [root@localhost tmp]# mkdir /jfs [root@localhost tmp]# mkdir /reiserfs [root@localhost tmp]# mkdir /ext4 [root@localhost tmp]# mount -t xfs /dev/sda13 /xfs [root@localhost tmp]# mount -t reiserfs /dev/sda11 /reiserfs/ [root@localhost tmp]# mount -t jfs /dev/sda12 /jfs/ [root@localhost tmp]# mount -t ext4 /dev/sda10 /ext4 [root@apache xfsprogs-2.9.8]# df -T 文件系统 类型 1K-块 已用 可用 已用% 挂载点 /dev/sda13 xfs 1949656 4256 1945400 1% /xfs /dev/sda11 reiserfs 1959808 32840 1926968 2% /reiserfs /dev/sda12 jfs 1951440 372 1951068 1% /jfs /dev/sda10 ext4 1929068 35648 1795428 2% /ext4xfs（了解）XFS 最初是由 Silicon Graphics，Inc. 开发的。支持最大文件系统到16EB，最大文件8EB，那数以千万计的目录结构条目。XFS文件系统支持metadata 日志，更快的崩溃恢复，在线整理碎片，在线增长，使用实用工具（xfsdump和xfsrestore）备份与恢复。xfs在处理特大文件上有很好的表现，在多线程并行I/O的工作模式下处理小文件也有很好的表现 xfs也支持以下特征 基于Extent的分配方式 XFS文件系统中的文件用到的块由变长Extent管理，每一个Extent描述了一个或多个连续的块。相比将每个文件用到的所有的块存储为列表的文件系统，这种策略大幅缩短了列表的长度。有些文件系统用一个或多个面向块的栅格管理空间分配在XFS中这种结构被由一对B+树组成的、面向Extent的结构替代了；每个文件系统分配组(AG)包含这样的一个结构。其中，一个B+树用于索引未被使用的Extent的长度，另一个索引这些Extent的起始块。这种双索引策略使得文件系统在定位剩余空间中的Extent时十分高效。 条带化分配 在条带化RAID阵列上创建XFS文件系统时，可以指定一个“条带化数据单元”。这可以保证数据分配、inode分配、以及内部日志被对齐到该条带单元上，以此最大化吞吐量。 延迟分配 XFS在文件分配上使用了惰性计算技术。当一个文件被写入缓存时，XFS简单地在内存中对该文件保留合适数量的块，而不是立即对数据分配Extent。实际的块分配仅在这段数据被冲刷到磁盘时才发生。这一机制提高了将这一文件写入一组连续的块中的机会，减少碎片的同时提升了性能。 扩展属性 XFS通过实现扩展文件属性给文件提供了多个数据流，使文件可以被附加多个名/值对。文件名是一个最大长度为256字节的、以NULL字符结尾的可打印字符串，其它的关联值则可包含多达 64KB 的二进制数据。这些数据被进一步分入两个名字空间中，root和user。保存在root名字空间中的扩展属性只能被超级用户修改，user名字空间中的可以被任何对该文件拥有写权限的用户修改。扩展属性可以被添加到任意一种XFS inode上，包括符号链接、设备节点、目录，等等。可以使用 attr 这个命令行程序操作这些扩展属性。xfsdump 和 xfsrestore 工具在进行备份和恢复时会一同操作扩展属性，而其它的大多数备份系统则会忽略扩展属性。 XFS 支援 disk quota。 xfs局限性 1.XFS是一个单节点文件系统，如果需要多节点同时访问需要考虑使用GFS2文件系统 2.XFS支持16EB文件系统，而redhat仅支持100TB文件系统 3.XFS较少的适用在单线程元数据密集的工作负荷，在单线程创建删除巨大数量的小文件的工作负荷下，其他文件系统（ext4）表现的会更好一些 4.xfs文件在操作元数据时可能会使用2倍的ext4CPU资源，在CPU资源有限制的情况下可以研究使用不同文件系统 5.xfs更多适用的特大文件的系统快速存储，ext4在小文件的系统或系统存储带宽有限的情况下表现的更好 [root@node6 ~]# yum install xfsprogs -y [root@node6 ~]# mkfs.xfs /dev/vdb1 meta-data=/dev/vdb1 isize=256 agcount=4, agsize=6016 blks = sectsz=512 attr=2, projid32bit=0 data = bsize=4096 blocks=24064, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 log =internal log bsize=4096 blocks=1200, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 [root@node6 ~]# mkfs.xfs -l logdev=/dev/vdb2 /dev/vdb1 -f meta-data=/dev/vdb1 isize=256 agcount=4, agsize=6016 blks = sectsz=512 attr=2, projid32bit=0 data = bsize=4096 blocks=24064, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 log =/dev/vdb2 bsize=4096 blocks=24576, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 [root@node6 ~]# mount -o logdev=/dev/vdb2 /dev/vdb1 /xfs/ [root@node6 ~]# pvcreate /dev/vdb1 /dev/vdb2 [root@node6 ~]# vgcreate vgxfs /dev/vdb2 /dev/vdb1 [root@node6 ~]# lvcreate -l 25 -n lvxfs vgxfs Logical volume &quot;lvxfs&quot; created [root@node6 ~]# mkfs.xfs /dev/vgxfs/lvxfs meta-data=/dev/vgxfs/lvxfs isize=256 agcount=4, agsize=6400 blks = sectsz=512 attr=2, projid32bit=0 data = bsize=4096 blocks=25600, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 log =internal log bsize=4096 blocks=1200, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 [root@node6 ~]# mount /dev/vgxfs/lvxfs /xfs/ [root@node6 ~]# lvextend -l +100%FREE /dev/vgxfs/lvxfs [root@node6 ~]# xfs_growfs /xfs/ meta-data=/dev/mapper/vgxfs-lvxfs isize=256 agcount=4, agsize=6400 blks = sectsz=512 attr=2, projid32bit=0 data = bsize=4096 blocks=25600, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 log =internal bsize=4096 blocks=1200, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 25600 to 47104 [root@node6 ~]# umount /xfs [root@node6 ~]# xfs_repair /dev/vgxfs/lvxfs [root@node6 ~]# mkfs.xfs -l logdev=/dev/vdb2 /dev/vdb1 [root@node6 ~]# mount -o logdev=/dev/vdb2 /dev/vdb1 /xfs [root@node6 ~]# for FILE in file{0..3} ; do dd if=/dev/zero of=/xfs/${FILE} bs=4M count=100 &amp; done [root@node6 ~]# filefrag /xfs/file* [root@node6 ~]# xfs_fsr -v [root@node6 ~]# umount /xfs [root@node6 ~]# xfs_repair -n -l /dev/vdb2 /dev/vdb1 Phase 1 - find and verify superblock... Phase 2 - using external log on /dev/vdb2 - scan filesystem freespace and inode maps... - found root inode chunk Phase 3 - for each AG... - scan (but don&#39;t clear) agi unlinked lists... - process known inodes and perform inode discovery... - agno = 0 - agno = 1 - agno = 2 - agno = 3 - process newly discovered inodes... Phase 4 - check for duplicate blocks... - setting up duplicate extent list... - check for inodes claiming duplicate blocks... - agno = 0 - agno = 1 - agno = 2 - agno = 3 No modify flag set, skipping phase 5 Phase 6 - check inode connectivity... - traversing filesystem ... - traversal finished ... - moving disconnected inodes to lost+found ... Phase 7 - verify link counts... No modify flag set, skipping filesystem flush and exiting. [root@node6 ~]# [root@node6 ~]# xfs_repair -l /dev/vdb2 /dev/vdb1 Phase 1 - find and verify superblock... Phase 2 - using external log on /dev/vdb2 - zero log... - scan filesystem freespace and inode maps... - found root inode chunk Phase 3 - for each AG... - scan and clear agi unlinked lists... - process known inodes and perform inode discovery... - agno = 0 - agno = 1 - agno = 2 - agno = 3 - process newly discovered inodes... Phase 4 - check for duplicate blocks... - setting up duplicate extent list... - check for inodes claiming duplicate blocks... - agno = 0 - agno = 1 - agno = 2 - agno = 3 Phase 5 - rebuild AG headers and trees... - reset superblock... Phase 6 - check inode connectivity... - resetting contents of realtime bitmap and summary inodes - traversing filesystem ... - traversal finished ... - moving disconnected inodes to lost+found ... Phase 7 - verify and correct link counts... done [root@node6 ~]# mount -o logdev=/dev/vdb2 /dev/vdb1 /xfs [root@node6 ~]# yum install xfsdump [root@node6 ~]# xfsdump -L full -M dumpfile -l 0 - /xfs | xz &gt; /tmp/xfs.$(date +%Y%m%d).0.xz xfsdump: using file dump (drive_simple) strategy xfsdump: version 3.0.4 (dump format 3.0) - Running single-threaded xfsdump: level 0 dump of node6.uplooking.com:/xfs xfsdump: dump date: Sat Sep 14 17:39:47 2013 xfsdump: session id: 75f91e6b-c0bc-4ad1-978b-e2ee5deb01d4 xfsdump: session label: &quot;full&quot; xfsdump: ino map phase 1: constructing initial dump list xfsdump: ino map phase 2: skipping (no pruning necessary) xfsdump: ino map phase 3: skipping (only one dump stream) xfsdump: ino map construction complete xfsdump: estimated dump size: 1677743680 bytes xfsdump: /var/lib/xfsdump/inventory created xfsdump: creating dump session media file 0 (media 0, file 0) xfsdump: dumping ino map xfsdump: dumping directories xfsdump: dumping non-directory files xfsdump: ending media file xfsdump: media file size 1678152296 bytes xfsdump: dump size (non-dir files) : 1678101072 bytes xfsdump: dump complete: 152 seconds elapsed xfsdump: Dump Status: SUCCESS [root@node6 ~]# [root@node6 ~]# xfsdump -I file system 0: fs id: 467c218c-22b5-45bc-9b0e-cd5782be6e2e session 0: mount point: node6.uplooking.com:/xfs device: node6.uplooking.com:/dev/vdb1 time: Sat Sep 14 17:39:47 2013 session label: &quot;full&quot; session id: 75f91e6b-c0bc-4ad1-978b-e2ee5deb01d4 level: 0 resumed: NO subtree: NO streams: 1 stream 0: pathname: stdio start: ino 131 offset 0 end: ino 135 offset 0 interrupted: NO media files: 1 media file 0: mfile index: 0 mfile type: data mfile size: 1678152296 mfile start: ino 131 offset 0 mfile end: ino 135 offset 0 media label: &quot;dumpfile&quot; media id: de67b2b5-db72-4555-9804-a050829b2179 xfsdump: Dump Status: SUCCESS [root@node6 ~]# rm -rf /xfs/* [root@node6 ~]# xzcat /tmp/xfs.20130914.0.xz | xfsrestore - /xfs xfsrestore: using file dump (drive_simple) strategy xfsrestore: version 3.0.4 (dump format 3.0) - Running single-threaded xfsrestore: searching media for dump xfsrestore: examining media file 0 xfsrestore: dump description: xfsrestore: hostname: node6.uplooking.com xfsrestore: mount point: /xfs xfsrestore: volume: /dev/vdb1 xfsrestore: session time: Sat Sep 14 17:39:47 2013 xfsrestore: level: 0 xfsrestore: session label: &quot;full&quot; xfsrestore: media label: &quot;dumpfile&quot; xfsrestore: file system id: 467c218c-22b5-45bc-9b0e-cd5782be6e2e xfsrestore: session id: 75f91e6b-c0bc-4ad1-978b-e2ee5deb01d4 xfsrestore: media id: de67b2b5-db72-4555-9804-a050829b2179 xfsrestore: searching media for directory dump xfsrestore: reading directories xfsrestore: 1 directories and 4 entries processed xfsrestore: directory post-processing xfsrestore: restoring non-directory files xfsrestore: restore complete: 33 seconds elapsed xfsrestore: Restore Status: SUCCESS [root@node6 ~]# ls /xfs file0 file1 file2 file3net一台服务器CPU和内存资源额定有限的情况下，如何提高服务器的性能是作为系统运维的重要工作。要提高Linux系统下的负载能力，当网站发展起来之后，web连接数过多的问题就会日益明显。在节省成本的情况下，可以考虑修改Linux 的内核TCP/IP参数来部分实现； Linux系统下，TCP/IP连接断开后，会以TIME_WAIT状态保留一定的时间，然后才会释放端口。当并发请求过多的时候，就会产生大量的 TIME_WAIT状态的连接，无法及时断开的话，会占用大量的端口资源和服务器资源(因为关闭后进程才会退出)。这个时候我们可以考虑优化TCP/IP 的内核参数，来及时将TIME_WAIT状态的端口清理掉。 写在/etc/sysctl.conf里.开路由的也在这里. net.ipv4.tcp_syncookies = 1 表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭； net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭； net.ipv4.tcp_tw_recycle = 1 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭； net.ipv4.tcp_fin_timeout = 5 修改系统默认的 TIMEOUT 时间。 net.ipv4.tcp_timestamps = 1 以一种比重发超时更精确的方法（参阅 RFC 1323）来启用对 RTT 的计算；为了实现更好的性能应该启用这个选项 net.ipv4.tcp_keepalive_time = 1200 表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。 net.ipv4.ip_local_port_range = 10000 65000 表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为10000到65000。(注意：这里不要将最低值设的太低，否则可能会占用掉正常的端口！) net.ipv4.tcp_max_syn_backlog = 8192 表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。 net.ipv4.tcp_max_tw_buckets = 5000 表示系统同时保持TIME_WAIT的最大数量，如果超过这个数字，TIME_WAIT将立刻被清除并打印警告信息。默认为180000，改为5000。对于Apache、Nginx等服务器，上几行的参数可以很好地减少TIME_WAIT套接字数量。 网卡绑定网卡绑定 网卡绑定的分类 模式 7中 Ethernet Channel Bonding Linux双网卡绑定的实现是使用两块网卡虚拟成为一块网卡，这个聚合起来的设备看起来是一个单独的以太网接口设备，通俗点讲就是两块网卡具有相同的IP地址而并行链接聚合成一个逻辑链路工作。其实这项技术在Sun和Cisco中早已存在，被称为Trunking和Etherchannel技术，在Linux的2.4.x以上内核中也采用这这种技术，被称为bonding。bonding技术的最早应用是在集群，为了提高集群节点间的数据传输而设计的。 bonding的配置方式文档 [root@localhost ~]# rpm -q kernel-doc /usr/share/doc/kernel-doc-2.6.18/Documentation/networking/bonding.txt 系统是否支持bonding # modinfo bonding 如果没有返回消息，说明不支持需要重新编译内核 检查ifenslave #which ifenslave /sbin/ifenslave 分别修改2个网卡配置文件，声明自己为slave，master是bond0 [root@localhost ~]# vim /etc/sysconfig/network-scripts/ifcfg-eth0 －－－ DEVICE=eth0 USERCTL=no ONBOOT=yes BOOTPROTO=none MASTER=bond0 SLAVE=yes －－－ 生成master设备的配置文件 [root@localhost ~]# vim /etc/sysconfig/network-scripts/ifcfg-bond0 ----------- DEVICE=bond0 IPADDR=192.168.122.254 NETMASK=255.255.255.0 ONBOOT=yes BOOTPROTO=none USERCTL=no ----------- bond0是什么设备？实际我们做的网卡绑定，是通过bonding模块来实现的，所以要bonding模块设置一个别名，指向我们创建的bond0 [root@localhost ~]# vim /etc/modprobe.conf －－－ alias bond0 bonding options bonding miimon=100 mode=balance-rr options bond0 miimon=100 mode=1 primary=eth0 //模式为1的时候使用这行配置 RHEL6下不再有modprobe.conf这个文件 在/etc/modprobe.d/里建立bond0.conf # cat /etc/modprobe.d/bond0.conf alias bond0 bonding －－－ miimon是用来进行链路监测的。 比如:miimon=100，那么系统每100ms监测一次链路连接状态，如果有一条线路不通就转入另一条线路； mode的值表示工作模式，他共有0，1,2,3四种模式，常用的为0,1两种。 mode=0表示load balancing (round-robin)为负载均衡方式，两块网卡都工作。 mode=1表示fault-tolerance (active-backup)提供冗余功能，工作方式是主备的工作方式,也就是说默认情况下只有一块网卡工作,另一块做备份. 重启服务，如果不生效需要重启系统： [root@localhost ~]# service network restart 验证bond0是否成功： [root@localhost ~]# cat /proc/net/bonding/bond0 =================================== rhel6 DEVICE=bond0 IPADDR=192.168.0.253 NETMASK=255.255.255.0 ONBOOT=yes NM_CONTROLLED=no BOOTPROTO=none USERCTL=no BONDING_OPTS=&quot;miimon=50 mode=0&quot; 建立bonding.conf #cat /etc/modprobe.d/bond.conf alias bond0 bonding 在/etc/rc.local文件末尾加入如下内容 #vi /etc/rc.local ifenslave bond0 eth0 eth2资源控制ulimit系统性能一直是一个受关注的话题，如何通过最简单的设置来实现最有效的性能调优，如何在有限资源的条件下保证程序的运作，ulimit 是我们在处理这些问题时，经常使用的一种简单手段。ulimit 是一种 linux 系统的内键功能，它具有一套参数集，用于为由它生成的 shell 进程及其子进程的资源使用设置限制。 ulimit 功能简述 假设有这样一种情况，当一台 Linux 主机上同时登陆了 10 个人，在系统资源无限制的情况下，这 10 个用户同时打开了 500 个文档，而假设每个文档的大小有 10M，这时系统的内存资源就会受到巨大的挑战。 而实际应用的环境要比这种假设复杂的多，例如在一个嵌入式开发环境中,各方面的资源都是非常紧缺的，对于开启文件描述符的数量，分配堆栈的大小，CPU 时间，虚拟内存大小，等等，都有非常严格的要求。资源的合理限制和分配，不仅仅是保证系统可用性的必要条件，也与系统上软件运行的性能有着密不可分的联系。这时，ulimit 可以起到很大的作用，它是一种简单并且有效的实现资源限制的方式。 ulimit 用于限制 shell 启动进程所占用的资源，支持以下各种类型的限制：所创建的内核文件的大小、进程数据块的大小、Shell 进程创建文件的大小、内存锁住的大小、常驻内存集的大小、打开文件描述符的数量、分配堆栈的最大大小、CPU 时间、单个用户的最大线程数、Shell 进程所能使用的最大虚拟内存。同时，它支持硬资源和软资源的限制。 作为临时限制，ulimit 可以作用于通过使用其命令登录的 shell 会话，在会话终止时便结束限制，并不影响于其他 shell 会话。而对于长期的固定限制，ulimit 命令语句又可以被添加到由登录 shell 读取的文件中，作用于特定的 shell 用户。 [root@localhost Desktop]# ulimit -a core file size (blocks, -c) 0 #最大的 core 文件的大小， 以 blocks 为单位 core文件的简单介绍 在一个程序崩溃时，它一般会在指定目录下生成一个core文件。core文件仅仅是一个内存映象(同时加上调试信息)，主要是用来调试的。 data seg size (kbytes, -d) unlimited #进程最大的数据段的大小，以 Kbytes 为单位。 scheduling priority (-e) 0 file size (blocks, -f) unlimited #进程可以创建文件的最大值，以 blocks 为单位。 pending signals (-i) 59335 max locked memory (kbytes, -l) 64 #最大可加锁内存大小，以 Kbytes 为单位。 max memory size (kbytes, -m) unlimited #最大内存大小，以 Kbytes 为单位 open files (-n) 1024 #可以打开最大文件描述符的数量。 pipe size (512 bytes, -p) 8 #管道缓冲区的大小，以 512byte/个 为单位。 POSIX message queues (bytes, -q) 819200 real-time priority (-r) 0 stack size (kbytes, -s) 10240 #堆栈大小，以 Kbytes 为单位。 cpu time (seconds, -t) unlimited #最大的 CPU 占用时间，以秒为单位。 max user processes (-u) 59335 #用户最大可用的进程数。 virtual memory (kbytes, -v) unlimited #进程最大可用的虚拟内存，以 Kbytes 为单位。 file locks (-x) unlimited -a 显示当前所有的 limit 信息。 -H 设置硬资源限制，一旦设置不能增加。 ulimit Hs 64；限制硬资源，线程栈大小为 64K。 -S 设置软资源限制，设置后可以增加，但是不能超过硬资源设置。 ulimit Sn 32；限制软资源，32 个文件描述符。 限制使用CPU时间 [root@localhost ~]# vim /tmp/a.sh #!/bin/bash while true do : done [root@localhost ~]# ulimit -t 20 [root@localhost ~]# ulimit -a | grep &quot;cpu time&quot; cpu time (seconds, -t) 20 [root@localhost ~]# /tmp/a.sh Killed 限制用户使用的虚拟内存 [root@localhost ~]# ulimit -v 0 [root@localhost ~]# ls Killed [root@localhost ~]# df Killed [root@localhost ~]# ps Killed [root@localhost ~]# cd /etc [root@localhost etc]# echo 111 111 [root@localhost etc]# type cd cd is a shell builtin [root@localhost etc]# type echo echo is a shell builtin 限制创建文件大小 [root@node5 ~]# ulimit -f 1000 [root@node5 ~]# dd if=/dev/zero of=file1 bs=1024 count=1001 File size limit exceeded (core dumped) [root@node5 ~]# dd if=/dev/zero of=file1 bs=1024 count=900 900+0 records in 900+0 records out 921600 bytes (922 kB) copied, 0.0031932 s, 289 MB/s 系统性能一直是一个受关注的话题，如何通过最简单的设置来实现最有效的性能调优，如何在有限资源的条件下保证程序的运作，ulimit 是我们在处理这些问题时，经常使用的一种简单手段。ulimit 是一种 linux 系统的内键功能，它具有一套参数集，用于为由它生成的 shell 进程及其子进程的资源使用设置限制。 ulimit 功能简述 假设有这样一种情况，当一台 Linux 主机上同时登陆了 10 个人，在系统资源无限制的情况下，这 10 个用户同时打开了 500 个文档，而假设每个文档的大小有 10M，这时系统的内存资源就会受到巨大的挑战。 而实际应用的环境要比这种假设复杂的多，例如在一个嵌入式开发环境中,各方面的资源都是非常紧缺的，对于开启文件描述符的数量，分配堆栈的大小，CPU 时间，虚拟内存大小，等等，都有非常严格的要求。资源的合理限制和分配，不仅仅是保证系统可用性的必要条件，也与系统上软件运行的性能有着密不可分的联系。这时，ulimit 可以起到很大的作用，它是一种简单并且有效的实现资源限制的方式。 ulimit 用于限制 shell 启动进程所占用的资源，支持以下各种类型的限制：所创建的内核文件的大小、进程数据块的大小、Shell 进程创建文件的大小、内存锁住的大小、常驻内存集的大小、打开文件描述符的数量、分配堆栈的最大大小、CPU 时间、单个用户的最大线程数、Shell 进程所能使用的最大虚拟内存。同时，它支持硬资源和软资源的限制。 作为临时限制，ulimit 可以作用于通过使用其命令登录的 shell 会话，在会话终止时便结束限制，并不影响于其他 shell 会话。而对于长期的固定限制，ulimit 命令语句又可以被添加到由登录 shell 读取的文件中，作用于特定的 shell 用户。 [root@localhost Desktop]# ulimit -a core file size (blocks, -c) 0 #最大的 core 文件的大小， 以 blocks 为单位 core文件的简单介绍 在一个程序崩溃时，它一般会在指定目录下生成一个core文件。core文件仅仅是一个内存映象(同时加上调试信息)，主要是用来调试的。 data seg size (kbytes, -d) unlimited #进程最大的数据段的大小，以 Kbytes 为单位。 scheduling priority (-e) 0 file size (blocks, -f) unlimited #进程可以创建文件的最大值，以 blocks 为单位。 pending signals (-i) 59335 max locked memory (kbytes, -l) 64 #最大可加锁内存大小，以 Kbytes 为单位。 max memory size (kbytes, -m) unlimited #最大内存大小，以 Kbytes 为单位 open files (-n) 1024 #可以打开最大文件描述符的数量。 pipe size (512 bytes, -p) 8 #管道缓冲区的大小，以 512byte/个 为单位。 POSIX message queues (bytes, -q) 819200 real-time priority (-r) 0 stack size (kbytes, -s) 10240 #堆栈大小，以 Kbytes 为单位。 cpu time (seconds, -t) unlimited #最大的 CPU 占用时间，以秒为单位。 max user processes (-u) 59335 #用户最大可用的进程数。 virtual memory (kbytes, -v) unlimited #进程最大可用的虚拟内存，以 Kbytes 为单位。 file locks (-x) unlimited -a 显示当前所有的 limit 信息。 -H 设置硬资源限制，一旦设置不能增加。 ulimit Hs 64；限制硬资源，线程栈大小为 64K。 -S 设置软资源限制，设置后可以增加，但是不能超过硬资源设置。 ulimit Sn 32；限制软资源，32 个文件描述符。 限制使用CPU时间 [root@localhost ~]# vim /tmp/a.sh #!/bin/bash while true do : done [root@localhost ~]# ulimit -t 20 [root@localhost ~]# ulimit -a | grep &quot;cpu time&quot; cpu time (seconds, -t) 20 [root@localhost ~]# /tmp/a.sh Killed 限制用户使用的虚拟内存 [root@localhost ~]# ulimit -v 0 [root@localhost ~]# ls Killed [root@localhost ~]# df Killed [root@localhost ~]# ps Killed [root@localhost ~]# cd /etc [root@localhost etc]# echo 111 111 [root@localhost etc]# type cd cd is a shell builtin [root@localhost etc]# type echo echo is a shell builtin 限制创建文件大小 [root@node5 ~]# ulimit -f 1000 [root@node5 ~]# dd if=/dev/zero of=file1 bs=1024 count=1001 File size limit exceeded (core dumped) [root@node5 ~]# dd if=/dev/zero of=file1 bs=1024 count=900 900+0 records in 900+0 records out 921600 bytes (922 kB) copied, 0.0031932 s, 289 MB/s pam_limit.sopam_limits 资源限制模块（提供的管理组:session） pam 插入式验证模块 为使用此模块, 系统管理员必须首先建立一个 root只读 的文件(默认是 /etc/security/limits.conf). 这文件描述了superuser想强迫用户和用户组的资源限制. uid=0的帐号不会受限制. 以下参数可以用来改变此模块的行为: * debug - 往syslog(3)写入冗长的记录. * conf=/path/to/file.conf - 指定一个替换的limits设定档. 设定档的每一行描述了一个用户的限制,以下面的格式: &lt;domain&gt; &lt;type&gt; &lt;item&gt; &lt;value&gt; 上面列出的栏位可以填下面的值:... &lt;domain&gt; 可以是: * 一个用户名 * 一个组名,语法是@group * 通配符*, 定义默认条目 &lt;type&gt; 可以有一下两个值: * hard 为施行硬 资源限制. 这些限制由superuser设定,由Linux内核施行. 用户不能提升他对资源的需求到大于此值. * soft 为施行软 资源限制. 用户的限制能在软硬限制之间上下浮动. 这种限制在普通用法下可以看成是默认值. &lt;item&gt; 可以是以下之一: * core - 限制core文件的大小(KB) * data - 最大的资料大小 (KB) * fsize - 最大的文件尺寸 (KB) * memlock - 最大能锁定的内存空间(KB) * nofile - 最多能打开的文件 * rss - 最大的驻留程序大小(KB) * stack - 最大的堆栈尺寸(KB) * cpu - 最大的CPU 时间(分钟) * nproc - 最多的进程数 * as - 地址空间的限制 * maxlogins - 用户的最多登录数 * priority - 用户进程执行时的优先级 要完全不限制用户(或组), 可以用一个(-)(例如: ``bin -&#39;&#39;, ``@admin -&#39;&#39;). 注意,个体的限制比组限制的优先级高, 所以如果你设定admin组不受限制, 但是组中的某个成员被设定档中某行限制, 那么此用户就会依据这样被限制.还应该注意, 所有的限制设定只是每个登录的设定. 他们既不是全局的,也不是永久的 ; 之存在于会话期间. pam_limits 模块会通过syslog(3)报告它从设定档中找到的问题. 下面配置文件实例: # EXAMPLE /etc/security/limits.conf file: # ======================================= # &lt;domain&gt; &lt;type&gt; &lt;item&gt; &lt;value&gt; * soft core 0 * hard rss 10000 @student hard nproc 20 @faculty soft nproc 20 @faculty hard nproc 50 ftp hard nproc 0 注意, 对同一个资源的软限制和硬限制 - 这建立了用户可以从指定服务会话中得到的默认和最大允许的资源数. 限制用户登录次数 /etc/security/limits.conf [root@localhost ~]# vim /etc/pam.d/login session required pam_limits.so [root@localhost ~]# vim /etc/security/limits.conf hulk hard maxlogins 2 限制用户打开进程数 [root@localhost ~]# vim /etc/security/limits.conf hulk hard nproc 3 [root@localhost ~]# useradd hulk [root@localhost ~]# echo &quot;123456&quot; | passwd --stdin hulk [root@localhost ~]# su - hulk [hulk@localhost ~]$ sleep 3000 &amp; [1] 4650 [hulk@localhost ~]$ sleep 3000 &amp; [2] 4651 [hulk@localhost ~]$ sleep 3000 &amp; -bash: fork: retry: Resource temporarily unavailable 限制用户使用CPU时间 [root@localhost ~]# vim /etc/security/limits.conf hulk hard cpu 1 [hulk@localhost ~]$ ./a.sh 脚本执行1分钟后 Killed 限制用户创建文件大小 [root@localhost ~]# vim /etc/security/limits.conf hulk hard fsize 500 [root@localhost ~]# su - hulk [hulk@localhost ~]$ dd if=/dev/zero of=file1 bs=1M count=1 File size limit exceeded (core dumped) cgroupCgroups是什么？ Cgroups是control groups的缩写，是Linux内核提供的一种可以限制、记录、隔离进程组（process groups）所使用的物理资源（如：cpu,memory,IO等等）的机制。最初由google的工程师提出，后来被整合进Linux内核。Cgroups也是LXC为实现虚拟化所使用的资源管理手段，可以说没有cgroups就没有LXC。 概述LXC为LinuxContainer的简写。LinuxContainer容器是一种内核虚拟化技术，可以提供轻量级的虚拟化 Cgroups可以做什么？ Cgroups最初的目标是为资源管理提供的一个统一的框架，既整合现有的cpuset等子系统，也为未来开发新的子系统提供接口。现在的cgroups适用于多种应用场景，从单个进程的资源控制，到实现操作系统层次的虚拟化（OS Level Virtualization）。Cgroups提供了一下功能： 1.限制进程组可以使用的资源数量（Resource limiting ）。比如：memory子系统可以为进程组设定一个memory使用上限，一旦进程组使用的内存达到限额再申请内存，就会出发OOM（out of memory）。 2.进程组的优先级控制（Prioritization ）。比如：可以使用cpu子系统为某个进程组分配特定cpu share。 3.记录进程组使用的资源数量（Accounting ）。比如：可以使用cpuacct子系统记录某个进程组使用的cpu时间 4.进程组隔离（Isolation）。比如：使用ns子系统可以使不同的进程组使用不同的namespace，以达到隔离的目的，不同的进程组有各自的进程、网络、文件系统挂载空间。 5.进程组控制（Control）。比如：使用freezer子系统可以将进程组挂起和恢复。 Cgroups相关概念及其关系 相关概念 1.任务（task）。在cgroups中，任务就是系统的一个进程。 2.控制族群（control group）。控制族群就是一组按照某种标准划分的进程。Cgroups中的资源控制都是以控制族群为单位实现。一个进程可以加入到某个控制族群，也从一个进程组迁移到另一个控制族群。一个进程组的进程可以使用cgroups以控制族群为单位分配的资源，同时受到cgroups以控制族群为单位设定的限制。 3.层级（hierarchy）。控制族群可以组织成hierarchical的形式，既一颗控制族群树。控制族群树上的子节点控制族群是父节点控制族群的孩子，继承父控制族群的特定的属性。 4.子系统（subsytem）。一个子系统就是一个资源控制器，比如cpu子系统就是控制cpu时间分配的一个控制器。子系统必须附加（attach）到一个层级上才能起作用，一个子系统附加到某个层级以后，这个层级上的所有控制族群都受到这个子系统的控制。 相互关系 1.每次在系统中创建新层级时，该系统中的所有任务都是那个层级的默认 cgroup（我们称之为 root cgroup ，此cgroup在创建层级时自动创建，后面在该层级中创建的cgroup都是此cgroup的后代）的初始成员。 2.一个子系统最多只能附加到一个层级。 3.一个层级可以附加多个子系统 4.一个任务可以是多个cgroup的成员，但是这些cgroup必须在不同的层级。 5.系统中的进程（任务）创建子进程（任务）时，该子任务自动成为其父进程所在 cgroup 的成员。然后可根据需要将该子任务移动到不同的 cgroup 中，但开始时它总是继承其父任务的cgroup。 Cgroups子系统介绍 blkio 这个子系统为块设备设定输入/输出限制，比如物理设备（磁盘，固态硬盘，USB 等等）。 cpu 这个子系统使用调度程序提供对 CPU 的 cgroup 任务访问。 cpuacct 这个子系统自动生成 cgroup 中任务所使用的 CPU 报告。 cpuset 这个子系统为 cgroup 中的任务分配独立 CPU（在多核系统）和内存节点。 devices 这个子系统可允许或者拒绝 cgroup 中的任务访问设备。 freezer 这个子系统挂起或者恢复 cgroup 中的任务。 memory 这个子系统设定 cgroup 中任务使用的内存限制，并自动生成由那些任务使用的内存资源报告。 net_cls 这个子系统使用等级识别符（classid）标记网络数据包，可允许 Linux 流量控制程序（tc）识别从具体 cgroup 中生成的数据包。 ns 名称空间子系统。 安装kernel-doc查看帮助 [root@localhost ~]# ls /usr/share/doc/kernel-doc-2.6.32/Documentation/cgroups/ 00-INDEX cpuacct.txt freezer-subsystem.txt net_prio.txt blkio-controller.txt cpusets.txt memcg_test.txt resource_counter.txt cgroups.txt devices.txt memory.txt","categories":[{"name":"Interview","slug":"Interview","permalink":"https://cyylog.github.io/categories/Interview/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/tags/Linux/"}]},{"title":"Prometheus（1）","slug":"监控/Prometheus/Prometheus（1）","date":"2020-10-29T05:48:56.000Z","updated":"2020-10-30T15:05:39.548Z","comments":true,"path":"2020/10/29/jian-kong/prometheus/prometheus-1/","link":"","permalink":"https://cyylog.github.io/2020/10/29/jian-kong/prometheus/prometheus-1/","excerpt":"","text":"什么是 PrometheusPrometheus是由SoundCloud开发的开源监控报警系统和时序列数据库(TSDB)。Prometheus使用Go语言开发，是 Google BorgMon监控系统的开源版本 官网：https://prometheus.io/docs Prometheus 的特点 多维度数据模型。 灵活的查询语言。 不依赖分布式存储，单个服务器节点是自主的。 通过基于HTTP的pull方式采集时序数据。 可以通过中间网关进行时序列数据推送。 通过服务发现或者静态配置来发现目标服务对象。 支持多种多样的图表和界面展示，比如Grafana等。 基本原理Prometheus的基本原理是通过HTTP协议周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以 接入监控。不需要任何SDK或者其他的集成过程。输出被监控组件信息的HTTP接口被叫做exporter 。目前互联网公 司常用的组件大部分都有exporter可以直接使用，比如Varnish、Haproxy、Nginx、MySQL、Linux系统信息 服务过程 Prometheus Daemon负责定时去目标上抓取metrics(指标)数据，每个抓取目标需要暴露一个http服务的接口 给它定时抓取。Prometheus支持通过配置文件、文本文件、Zookeeper、Consul、DNS SRV Lookup等方式指 定抓取目标。Prometheus采用PULL的方式进行监控，即服务器可以直接通过目标PULL数据或者间接地通过中 间网关来Push数据。 Prometheus在本地存储抓取的所有数据，并通过一定规则进行清理和整理数据，并把得到的结果存储到新的时 间序列中。 Prometheus通过PromQL和其他API可视化地展示收集的数据。Prometheus支持很多方式的图表可视化，例如 Grafana、自带的Promdash以及自身提供的模版引擎等等。Prometheus还提供HTTP API的查询方式，自定义 所需要的输出。 PushGateway支持Client主动推送metrics到PushGateway，而Prometheus只是定时去Gateway上抓取数据。 Alertmanager是独立于Prometheus的一个组件，可以支持Prometheus的查询语句，提供十分灵活的报警方 式。 数据模型Prometheus从根本上存储的所有数据都是时间序列，具有时间戳的数据流只属于单个度量指标和该度量指标下的多 个标签维度。 时间序列数据由metric名称和它的标签labels键值对集合唯一确定。 labbels开启了Prometheus的多维数据模型：对于相同的度量名称，通过不同标签列表的结合, 会形成特定的度量维 度实例。 &lt;metric name&gt;{&lt;label name&gt;=&lt;label value&gt;, ...}例如： api_http_requests_total{method=&quot;POST&quot;, handler=&quot;/messages&quot;}类型： Counter计数器 表示单个单调递增计数器的累积度量，其值只能在重启时增加或重置为零。 Gauges测量器 表示一个既可以递增, 又可以递减的值。 Histogram柱状图 对观察结果进行采样或求和，并将其计入可配置存储桶中。 Summary概要 是采样点分位图统计，计算在一定时间窗口范围内metrics对象的总数以及所有metrics的总和 三大套件Server 主要负责数据采集和存储，提供PromQL查询语言的支持。 Alertmanager 警告管理器，用来进行报警。 Push Gateway 支持临时性Job主动推送指标的中间网关。 安装**Server**[root@master ~]# mkdir -p /data/promethues/{client,server} [root@master ~]# touch /data/promethues/server/rules.yml [root@master ~]# chmod 777 /data/promethues/server/rules.yml [root@master ~]# vi /data/promethues/server/promethues.yml [root@master ~]# cat /data/promethues/server/promethues.yml global: scrape_interval: 15s #默认抓取间隔,15秒向目标抓取一次数据 evaluation_interval: 15s external_labels: monitor: 'codelab-monitor' # 定义抓取对象 scrape_configs: - job_name: 'prometheus' #重写时间序例，每一条都会自动添加上{job_name:\"prometheus\"}的标签 - job_name: 'prometheus' scrape_interval: 5s # 重写全局抓取间隔时间，由15秒重写成5秒 static_configs: - targets: ['localhost:9090'] [root@master ~]# [root@master ~]# docker run --name=prometheus -d -p 9090:9090 -v /data/promethues/server/promethues.yml:/etc/prometheus/prometheus.yml -v /data/promethues/server/rules.yml:/etc/prometheus/rules.yml prom/prometheus -- config.file=/etc/prometheus/prometheus.yml --web.enable-lifecycle 667f9b60c7f0eedecd7685e6ae59dd034ac1e66e766f5b552986cd634eadfe32 启动时加上–web.enable-lifecycle启用远程热加载配置文件 调用指令是curl -X POST http://localhost:9090/-/reload 在浏览器中访问 访问http://192.168.42.150:9090/ http://192.168.42.150:9090/metrics *通过 node exporter 提供 metrics * [root@master ~]# docker run -d --name=node-exporter -p 9100:9100 prom/node-exporter f18b6aa81b086319808a383c070ac1fc0cf93db0b4818b727afa05371045280d [root@master ~]# vim /data/promethues/server/promethues.yml [root@master ~]# tail -5 /data/promethues/server/promethues.yml - job_name: 'nodes' static_configs: - targets: ['192.168.42.150:9100','192.168.42.151:9100','192.168.42.152:9100'] labels: group: 'client-node-exporter' [root@master ~]# curl -X POST http://localhost:9090/-/reload 在GUI中查看 Targets如下所示： 安装**pushgateway** pushgateway是为了允许临时作业和批处理作业向prometheus公开他们的指标。 由于这类作业的存在时间可能不够长, 无法抓取到, 因此它们可以将指标推送到推网关中。 Prometheus采集数据是用的pull也就是拉模型，这从上面设置的5秒参数就能看出来。但是有些数据并不适合采用这样的方式，对这样的数据可以使用Push Gateway服务。 它 就相当于一个缓存，当数据采集完成之后，就上传到这里，由Prometheus稍后再pull过来。 首先启动 Push Gateway [root@master ~]# docker run -d -p 9091:9091 --name pushgateway prom/pushgateway Unable to find image 'prom/pushgateway:latest' locally latest: Pulling from prom/pushgateway 76df9210b28c: Already exists 559be8e06c14: Already exists 8491d2b1da91: Pull complete 67e6436e486c: Pull complete Digest: sha256:c0d39b8d4cfebec5c86ed19e0af68bdfed493d0d8098f99b1e9acf91d10e781e Status: Downloaded newer image for prom/pushgateway:latest 712f422674083dd855c50b3205b8f5d9914daa22ffacae71da3506b4d5604b97 Pushgateway GUI 如下所示： 接下来就可以往pushgateway推送数据了，prometheus提供了多种语言的sdk，最简单的方式就是通过shell 推送一个指标如下操作： [root@node2 ~]# echo \"metric01 100\" | curl --data-binary @- http://192.168.42.150:9091/metrics/job/cyylog 效果如下： 推送多个指标如下所示： [root@node2 ~]# cat < END | curl --data-binary @- http://192.168.42.150:9091/metrics/job/cyy/instance/test > muscle_metric{label=\"TEST\"} 8800 > bench_press 100 > dead_lift 160 > deep_squal 160 > END 150 for: 1m labels: status: warning annotations: summary: \"{{$labels.instance}}:硬拉超标！lightweight baby!!!\" description: \"{{$labels.instance}}:硬拉超标！lightweight baby!!!\" [root@master server]# Prometheus 显示如下所示： Alertmanager展示如下： 邮件收到报警内容如下： PS: 告警设置请查看DL的公众号文章 https://mp.weixin.qq.com/s/30hht_j18-LfzPDBxxnLvQ","categories":[{"name":"监控","slug":"监控","permalink":"https://cyylog.github.io/categories/%E7%9B%91%E6%8E%A7/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://cyylog.github.io/tags/Prometheus/"}]},{"title":"influxdb+grafana监控网络情况","slug":"监控/influxdb+grafana监控网络情况","date":"2020-10-26T06:55:39.000Z","updated":"2020-10-26T13:06:09.338Z","comments":true,"path":"2020/10/26/jian-kong/influxdb-grafana-jian-kong-wang-luo-qing-kuang/","link":"","permalink":"https://cyylog.github.io/2020/10/26/jian-kong/influxdb-grafana-jian-kong-wang-luo-qing-kuang/","excerpt":"","text":"influxdb+grafana监控网络情况一、 部署InfluxDBdocker run -d -p 8086:8086 --name=influxdb influxdb 以上命令dcoker会自动从仓库下载最新版本的influxdb镜像，后台运行一个名为influxdb的容器并映射主机8086端口到容器8086端口。 若想将数据存储到宿主机而非容器内，可使用以下命令启动挂载本地目录到容器内。 # $pwd为当前工作目录，可替换为其它宿主机目录 [root@localhost influxdb]# pwd /opt/influxdb [root@localhost influxdb]# docker run -d -p 8086:8086 -v $PWD:/var/lib/influxdb --name=influxdb influxdb 启动InfluxDB容器后，通过http接口访问进行测试。 curl -G http://localhost:8086/query --data-urlencode &quot;q=show databases&quot;若influxdb运行正常，则会返回如下结果： # 链接查询参数为show databases 数据库会返回所有的数据库名，新安装的influxdb默认只有一个&quot;_internal&quot; # 数据库。 {&quot;results&quot;:[{&quot;statement_id&quot;:0,&quot;series&quot;:[{&quot;name&quot;:&quot;databases&quot;,&quot;columns&quot;:[&quot;name&quot;],&quot;values&quot;:[[&quot;telegraf&quot;],[&quot;_internal&quot;],[&quot;network&quot;]]}]}]}二、 启动influxdb，使用 PS：InfluxDB中文文档 ​ 官网地址 其他命令请自行查看文档 1. 进入容器[root@localhost influxdb]# docker exec -it influxdb /bin/bash PS： 因为实验，所以 我使用的是默认 influxdb 的配置文件，可以自行查看官网修改 2. 启动influxdb，并且使用 创建一个mydb数据库： > CREATE DATABASE network 现在数据库mydb已经创建好了，我们可以用SHOW DATABASES语句来看看已存在的数据库： &gt; SHOW DATABASES name: databases --------------- name _internal mydb 说明：_internal数据库是用来存储InfluxDB内部的实时监控数据的。 不像SHOW DATABASES，大部分InfluxQL需要作用在一个特定的数据库上。你当然可以在每一个查询语句上带上你想查的数据库的名字，但是CLI提供了一个更为方便的方式USE &lt;db-name&gt;，这会为你后面的所以的请求设置到这个数据库上。例如： &gt; USE mydb Using database mydb &gt;3. 使用HTTP接口创建数据库使用POST方式发送到URL的/query路径，参数q为CREATE DATABASE &lt;new_database_name&gt;，下面的例子发送一个请求到本地运行的InfluxDB创建数据库mydb: curl -i -XPOST http://localhost:8086/query --data-urlencode &quot;q=CREATE DATABASE mydb&quot;4. 使用HTTP接口查询数据HTTP接口是InfluxDB查询数据的主要方式。通过发送一个GET请求到/query路径，并设置URL的db参数为目标数据库，设置URL参数q为查询语句。下面的例子是查询在写数据里写入的数据点。 curl -G &#39;http://localhost:8086/query?pretty=true&#39; --data-urlencode &quot;db=mydb&quot; --data-urlencode &quot;q=SELECT \\&quot;value\\&quot; FROM \\&quot;cpu_load_short\\&quot; WHERE \\&quot;region\\&quot;=&#39;us-west&#39;&quot; InfluxDB返回一个json值，你查询的结果在result列表中，如果有错误发送，InfluxDB会在error这个key里解释错误发生的原因。 5. 用户管理&gt;shouw users (查看用户) &gt;create user &quot;username&quot; with password &#39;password&#39; (创建普通用户) &gt;create user &quot;username&quot; with password &#39;password&#39; with all privileges (创建管理员用户) &gt;drop user &quot;username&quot; (删除用户) &gt;auth (用户认证，设置密码后的登录认证)6. shell脚本插入数据 PS: 首先创建一个 数据库+用户+密码都为network （实验环境） 请自行搞定 脚本地址： My Github 为方便 grafana 的可观性，请自行修改dic 放到计划任务中，可以看到表中已经有数据写入 > use network Using database network > show MEASUREMENTS name: measurements name ---- SG_BRlaten SG_BRloss SG_VNlaten SG_VNloss > 三、 部署GrafanaGrafana同样采用官方docker镜像进行快速部署。 docker run -d -p 3000:3000 --name=grafana grafana/grafana 以上命令docker会拉取最新版grafana镜像，运行名为grafana的容器，并映射宿主机3000端口。 初次启动，grafana会创建数据库，时间稍长，稍后即可访问http://localhost:3000打开grafana登录页面。输入默认用户名密码登录（admin）。 按照主页向导完成初次配置。 1 添加数据源点击添加数据源，按照下图配置选择influxdb添加一个influxdb数据源。 url需配置成正确的宿主机ip和端口（防火墙需放行8086），若不想暴露数据库端口，可换成influxdb容器的ip地址（需自行进入容器查看，容器重启后可能会发生变化）避免数据库暴露至公网。 InfluxDB Details需填写数据名（默认telegraf）、用户名和密码（默认均为root）。 填写完成后，点击Save&amp;Test按钮，若访问正常，会出现Data source is working提示，否则请检查配置内容以及网络（防火墙）。 2 添加仪表板 返回主页，点击添加仪表板按钮添加新仪表板，点击Graph创建一个Graph Panel。 配置好数据源，然后添加面板展示数据 最后的结果如下 PS: 文章中使用的脚本地址： https://github.com/cyylog/Script/tree/master/Grafana/Ping","categories":[{"name":"监控","slug":"监控","permalink":"https://cyylog.github.io/categories/%E7%9B%91%E6%8E%A7/"}],"tags":[{"name":"influxdb","slug":"influxdb","permalink":"https://cyylog.github.io/tags/influxdb/"},{"name":"grafana","slug":"grafana","permalink":"https://cyylog.github.io/tags/grafana/"}]},{"title":"Zabbix-原理","slug":"监控/zabbix/Zabbix-原理","date":"2020-10-23T17:55:39.000Z","updated":"2020-10-30T03:13:25.754Z","comments":true,"path":"2020/10/24/jian-kong/zabbix/zabbix-yuan-li/","link":"","permalink":"https://cyylog.github.io/2020/10/24/jian-kong/zabbix/zabbix-yuan-li/","excerpt":"","text":"​ 想要用好zabbix进行监控，那么我们首要需要了解下zabbix这个软件的实现原理及它的架构。建议多阅读官方文档。 官网地址：https://www.zabbix.com/documentation/4.0/zh/manual 一、zabbix架构图 二、zabbix组件组件部分1、Zabbix Server 负责接收agent发送的报告信息的核心组件，所有配置，统计数据及操作数据均由其组织进行； 2、Database Storage 专用于存储所有配置信息，以及由zabbix收集的数据； 3、Web interface zabbix的GUI接口，通常与Server运行在同一台主机上； 4、Proxy 可选组件，常用于监控节点很多的分布式环境中，代理server收集部分数据转发到server，可以减轻server的压力； 5、Agent 部署在被监主机上，负责收集本地数据并发往Server端或Proxy端； 三、相关术语1、主机（host） 要监控的网络设备，可由IP或DNS名称指定； 2、主机组（host group） 主机的逻辑容器，可以包含主机和模板，但同一个组织内的主机和模板不能互相链接；主机组通常在给用户或用户组指派监控权限时使用； 3、监控项（item） 一个特定监控指标的相关的数据；这些数据来自于被监控对象；item是zabbix进行数据收集的核心，相对某个监控对象，每个item都由”key”标识； 4、触发器（trigger） 一个表达式，用于评估某监控对象的特定item内接收到的数据是否在合理范围内，也就是阈值；接收的数据量大于阈值时，触发器状态将从”OK”转变为”Problem”，当数据再次恢复到合理范围，又转变为”OK”； 5、事件（event） 触发一个值得关注的事情，比如触发器状态转变，新的agent或重新上线的agent的自动注册等； 6、动作（action） 指对于特定事件事先定义的处理方法，如发送通知，何时执行操作； 7、报警媒介类型（media） 发送通知的手段或者通道，如Email、Jabber或者SMS等； 8、模板（template） 用于快速定义被监控主机的预设条目集合，通常包含了item、trigger、graph、screen、application以及low-level discovery rule；模板可以直接链接至某个主机； 9、前端（frontend） Zabbix的web接口 四、监控流程监控系统运行流程 agentd需要安装到被监控的主机上，它负责定期收集各项数据，并发送到zabbix server端，zabbix server将数据存储到数据库中，zabbix web根据数据在前端进行展现和绘图。这里agentd收集数据分为主动和被动两种模式： 主动 agent请求server获取主动的监控项列表，并主动将监控项内需要检测的数据提交给server/proxy 【主动监测】通信过程如下： zabbix首先向ServerActive配置的IP请求获取active items，获取并提交active items数据值server或者proxy。很多人会提出疑问：zabbix多久获取一次active items？它会根据配置文件中的RefreshActiveChecks的频率进行，如果获取失败，那么将会在60秒之后重试。分两个部分 获取ACTIVE ITEMS列表 • Agent打开TCP连接• Agent请求items检测列表• Server返回items列表• Agent 处理响应• 关闭TCP连接• Agent开始收集数据 主动检测提交数据过程如下 • Agent建立TCP连接• Agent提交items列表收集的数据• Server处理数据，并返回响应状态• 关闭TCP连接 被动 server向agent请求获取监控项的数据，agent返回数据。 【被动监测】通信过程如下： • Server打开一个TCP连接• Server发送请求agent.ping\\n• Agent接收到请求并且响应• Server处理接收到的数据1• 关闭TCP连接 从以上过程我们可以看出来，被动模式每次都需要打开一个tcp连接，这样当监控项越来越多时，就会出现server端性能问题了。还有人会问，那实际监控中是用主动的还是被动的呢？这里主要涉及两个地方：1、新建监控项目时，选择的是zabbix代理还是zabbix端点代理程式（主动式），前者是被动模式，后者是主动模式。2、agentd配置文件中StartAgents参数的设置，如果为0，表示禁止被动模式，否则开启。一般建议不要设置为0，因为监控项目很多时，可以部分使用主动，部分使用被动模式。","categories":[{"name":"监控","slug":"监控","permalink":"https://cyylog.github.io/categories/%E7%9B%91%E6%8E%A7/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://cyylog.github.io/tags/zabbix/"}]},{"title":"Go-Redis","slug":"DevOPs/Golang/Go-Redis","date":"2020-10-23T11:55:39.000Z","updated":"2020-10-23T13:48:30.553Z","comments":true,"path":"2020/10/23/devops/golang/go-redis/","link":"","permalink":"https://cyylog.github.io/2020/10/23/devops/golang/go-redis/","excerpt":"","text":"Redis 容器docker run --name redis-d -v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime -p 6379:6379 redis:5-alpine redis-servver 使用的库https://github.com/go-redis/redis go get github.com/go-redis/redis/v8 Quickstartimport ( \"context\" \"github.com/go-redis/redis/v8\" ) var ctx = context.Background() func ExampleClient() { rdb := redis.NewClient(&amp;redis.Options{ Addr: \"localhost:6379\", Password: \"\", // no password set DB: 0, // use default DB }) err := rdb.Set(ctx, \"key\", \"value\", 0).Err() if err != nil { panic(err) } val, err := rdb.Get(ctx, \"key\").Result() if err != nil { panic(err) } fmt.Println(\"key\", val) val2, err := rdb.Get(ctx, \"key2\").Result() if err == redis.Nil { fmt.Println(\"key2 does not exist\") } else if err != nil { panic(err) } else { fmt.Println(\"key2\", val2) } // Output: key value // key2 does not exist } Look and feelSome corner cases: // SET key value EX 10 NX set, err := rdb.SetNX(ctx, \"key\", \"value\", 10*time.Second).Result() // SET key value keepttl NX set, err := rdb.SetNX(ctx, \"key\", \"value\", redis.KeepTTL).Result() // SORT list LIMIT 0 2 ASC vals, err := rdb.Sort(ctx, \"list\", &amp;redis.Sort{Offset: 0, Count: 2, Order: \"ASC\"}).Result() // ZRANGEBYSCORE zset -inf +inf WITHSCORES LIMIT 0 2 vals, err := rdb.ZRangeByScoreWithScores(ctx, \"zset\", &amp;redis.ZRangeBy{ Min: \"-inf\", Max: \"+inf\", Offset: 0, Count: 2, }).Result() // ZINTERSTORE out 2 zset1 zset2 WEIGHTS 2 3 AGGREGATE SUM vals, err := rdb.ZInterStore(ctx, \"out\", &amp;redis.ZStore{ Keys: []string{\"zset1\", \"zset2\"}, Weights: []int64{2, 3} }).Result() // EVAL \"return {KEYS[1],ARGV[1]}\" 1 \"key\" \"hello\" vals, err := rdb.Eval(ctx, \"return {KEYS[1],ARGV[1]}\", []string{\"key\"}, \"hello\").Result() // custom command res, err := rdb.Do(ctx, \"set\", \"key\", \"value\").Result() 常用类型 类型 封装类名 String （字符串） StringOperation Hash（哈希） HashOperation List （列表） ListOperation Set （集合） SetOperation Zset（有序集合) ZSetOperation","categories":[{"name":"Go","slug":"Go","permalink":"https://cyylog.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://cyylog.github.io/tags/Go/"},{"name":"Redis","slug":"Redis","permalink":"https://cyylog.github.io/tags/Redis/"}]},{"title":"Go-单例模式","slug":"DevOPs/Golang/Go-单例模式","date":"2020-10-22T13:55:39.000Z","updated":"2020-10-23T13:33:01.899Z","comments":true,"path":"2020/10/22/devops/golang/go-dan-li-mo-shi/","link":"","permalink":"https://cyylog.github.io/2020/10/22/devops/golang/go-dan-li-mo-shi/","excerpt":"","text":"单例模式package main import ( \"fmt\" \"sync\" ) //单例模式 type WebConfig struct { Port int } var cc *WebConfig var once sync.Once func GetConfig() *WebConfig { once.Do(func() { cc = &amp;WebConfig{Port: 8080} }) return cc } func main() { c := GetConfig() c2 := GetConfig() c.Port = 9090 fmt.Println(c2, c == c2) }","categories":[{"name":"Go","slug":"Go","permalink":"https://cyylog.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://cyylog.github.io/tags/Go/"}]},{"title":"Kubernetes-Pod-生命周期","slug":"容器/Kubernetes工作负载—Pod的生命周期","date":"2020-10-05T16:50:00.000Z","updated":"2020-10-30T04:13:07.156Z","comments":true,"path":"2020/10/06/rong-qi/kubernetes-gong-zuo-fu-zai-pod-de-sheng-ming-zhou-qi/","link":"","permalink":"https://cyylog.github.io/2020/10/06/rong-qi/kubernetes-gong-zuo-fu-zai-pod-de-sheng-ming-zhou-qi/","excerpt":"","text":"Pod 的生命周期本页面讲述 Pod 的生命周期。 Pod 遵循一个预定义的生命周期，起始于 Pending 阶段，如果至少 其中有一个主要容器正常启动，则进入 Running，之后取决于 Pod 中是否有容器以 失败状态结束而进入 Succeeded 或者 Failed 阶段。 在 Pod 运行期间，kubelet 能够重启容器以处理一些失效场景。 在 Pod 内部，Kubernetes 跟踪不同容器的状态 并处理可能出现的状况。 在 Kubernetes API 中，Pod 包含规约部分和实际状态部分。 Pod 对象的状态包含了一组 Pod 状况（Conditions）。 如果应用需要的话，你也可以向其中注入自定义的就绪性信息。 Pod 在其生命周期中只会被调度一次。 一旦 Pod 被调度（分派）到某个节点，Pod 会一直在该节点运行，直到 Pod 停止或者 被终止。 Pod 生命期和一个个独立的应用容器一样，Pod 也被认为是相对临时性（而不是长期存在）的实体。 Pod 会被创建、赋予一个唯一的 ID（UID）， 并被调度到节点，并在终止（根据重启策略）或删除之前一直运行在该节点。 如果一个节点死掉了，调度到该节点 的 Pod 也被计划在给定超时期限结束后删除。 Pod 自身不具有自愈能力。如果 Pod 被调度到某节点 而该节点之后失效，或者调度操作本身失效，Pod 会被删除；与此类似，Pod 无法在节点资源 耗尽或者节点维护期间继续存活。Kubernetes 使用一种高级抽象，称作 控制器，来管理这些相对而言 可随时丢弃的 Pod 实例。 任何给定的 Pod （由 UID 定义）从不会被“重新调度（rescheduled）”到不同的节点； 相反，这一 Pod 可以被一个新的、几乎完全相同的 Pod 替换掉。 如果需要，新 Pod 的名字可以不变，但是其 UID 会不同。 如果某物声称其生命期与某 Pod 相同，例如存储卷， 这就意味着该对象在此 Pod （UID 亦相同）存在期间也一直存在。 如果 Pod 因为任何原因被删除，甚至某完全相同的替代 Pod 被创建时， 这个相关的对象（例如这里的卷）也会被删除并重建。 Pod 结构图例 一个包含多个容器的 Pod 中包含一个用来拉取文件的程序和一个 Web 服务器， 均使用持久卷作为容器间共享的存储。 Pod 阶段Pod 的 status 字段是一个 PodStatus 对象，其中包含一个 phase 字段。 Pod 的阶段（Phase）是 Pod 在其生命周期中所处位置的简单宏观概述。 该阶段并不是对容器或 Pod 状态的综合汇总，也不是为了成为完整的状态机。 Pod 阶段的数量和含义是严格定义的。 除了本文档中列举的内容外，不应该再假定 Pod 有其他的 phase 值。 下面是 phase 可能的值： 取值 描述 Pending（悬决） Pod 已被 Kubernetes 系统接受，但有一个或者多个容器尚未创建亦未运行。此阶段包括等待 Pod 被调度的时间和通过网络下载镜像的时间， Running（运行中） Pod 已经绑定到了某个节点，Pod 中所有的容器都已被创建。至少有一个容器仍在运行，或者正处于启动或重启状态。 Succeeded（成功） Pod 中的所有容器都已成功终止，并且不会再重启。 Failed（失败） Pod 中的所有容器都已终止，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止。 Unknown（未知） 因为某些原因无法取得 Pod 的状态。这种情况通常是因为与 Pod 所在主机通信失败。 如果某节点死掉或者与集群中其他节点失联，Kubernetes 会实施一种策略，将失去的节点上运行的所有 Pod 的 phase 设置为 Failed。 容器状态Kubernetes 会跟踪 Pod 中每个容器的状态，就像它跟踪 Pod 总体上的阶段一样。 你可以使用容器生命周期回调 来在容器生命周期中的特定时间点触发事件。 一旦调度器将 Pod 分派给某个节点，kubelet 就通过 容器运行时 开始为 Pod 创建容器。 容器的状态有三种：Waiting（等待）、Running（运行中）和 Terminated（已终止）。 要检查 Pod 中容器的状态，你可以使用 kubectl describe pod &lt;pod 名称&gt;。 其输出中包含 Pod 中每个容器的状态。 每种状态都有特定的含义： Waiting （等待）如果容器并不处在 Running 或 Terminated 状态之一，它就处在 Waiting 状态。 处于 Waiting 状态的容器仍在运行它完成启动所需要的操作：例如，从某个容器镜像 仓库拉取容器镜像，或者向容器应用 Secret 数据等等。 当你使用 kubectl 来查询包含 Waiting 状态的容器的 Pod 时，你也会看到一个 Reason 字段，其中给出了容器处于等待状态的原因。 Running（运行中）Running 状态表明容器正在执行状态并且没有问题发生。 如果配置了 postStart 回调，那么该回调已经执行完成。 如果你使用 kubectl 来查询包含 Running 状态的容器的 Pod 时，你也会看到 关于容器进入 Running 状态的信息。 Terminated（已终止）处于 Terminated 状态的容器已经开始执行并且或者正常结束或者因为某些原因失败。 如果你使用 kubectl 来查询包含 Terminated 状态的容器的 Pod 时，你会看到 容器进入此状态的原因、退出代码以及容器执行期间的起止时间。 如果容器配置了 preStop 回调，则该回调会在容器进入 Terminated 状态之前执行。 容器重启策略Pod 的 spec 中包含一个 restartPolicy 字段，其可能取值包括 Always、OnFailure 和 Never。默认值是 Always。 restartPolicy 适用于 Pod 中的所有容器。restartPolicy 仅针对同一节点上 kubelet 的容器重启动作。当 Pod 中的容器退出时，kubelet 会按指数回退 方式计算重启的延迟（10s、20s、40s、…），其最长延迟为 5 分钟。 一旦某容器执行了 10 分钟并且没有出现问题，kubelet 对该容器的重启回退计时器执行 重置操作。 Pod 状况Pod 有一个 PodStatus 对象，其中包含一个 PodConditions 数组。Pod 可能通过也可能未通过其中的一些状况测试。 PodScheduled：Pod 已经被调度到某节点； ContainersReady：Pod 中所有容器都已就绪； Initialized：所有的 Init 容器 都已成功启动； Ready：Pod 可以为请求提供服务，并且应该被添加到对应服务的负载均衡池中。 字段名称 描述 type Pod 状况的名称 status 表明该状况是否适用，可能的取值有 “True“, “False“ 或 “Unknown“ lastProbeTime 上次探测 Pod 状况时的时间戳 lastTransitionTime Pod 上次从一种状态转换到另一种状态时的时间戳 reason 机器可读的、驼峰编码（UpperCamelCase）的文字，表述上次状况变化的原因 message 人类可读的消息，给出上次状态转换的详细信息 Pod 就绪态FEATURE STATE: Kubernetes v1.14 [stable] 你的应用可以向 PodStatus 中注入额外的反馈或者信号：Pod Readiness（Pod 就绪态）。 要使用这一特性，可以设置 Pod 规约中的 readinessGates 列表，为 kubelet 提供一组额外的状况供其评估 Pod 就绪态时使用。 就绪态门控基于 Pod 的 status.conditions 字段的当前值来做决定。 如果 Kubernetes 无法在 status.conditions 字段中找到某状况，则该状况的 状态值默认为 “False“。 这里是一个例子： kind: Pod ... spec: readinessGates: - conditionType: \"www.example.com/feature-1\" status: conditions: - type: Ready # 内置的 Pod 状况 status: \"False\" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z - type: \"www.example.com/feature-1\" # 额外的 Pod 状况 status: \"False\" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z containerStatuses: - containerID: docker://abcd... ready: true ... 你所添加的 Pod 状况名称必须满足 Kubernetes 标签键名格式。 Pod 就绪态的状态命令 kubectl patch 不支持修改对象的状态。 如果需要设置 Pod 的 status.conditions，应用或者 Operators 需要使用 PATCH 操作。 你可以使用 Kubernetes 客户端库 之一来编写代码，针对 Pod 就绪态设置定制的 Pod 状况。 对于使用定制状况的 Pod 而言，只有当下面的陈述都适用时，该 Pod 才会被评估为就绪： Pod 中所有容器都已就绪； readinessGates 中的所有状况都为 True 值。 当 Pod 的容器都已就绪，但至少一个定制状况没有取值或者取值为 False， kubelet 将 Pod 的状况设置为 ContainersReady。 容器探针探针 是由 kubelet 对容器执行的定期诊断。 要执行诊断，kubelet 调用由容器实现的 Handler （处理程序）。有三种类型的处理程序： ExecAction： 在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。 TCPSocketAction： 对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。 HTTPGetAction： 对容器的 IP 地址上指定端口和路径执行 HTTP Get 请求。如果响应的状态码大于等于 200 且小于 400，则诊断被认为是成功的。 每次探测都将获得以下三种结果之一： Success（成功）：容器通过了诊断。 Failure（失败）：容器未通过诊断。 Unknown（未知）：诊断失败，因此不会采取任何行动。 针对运行中的容器，kubelet 可以选择是否执行以下三种探针，以及如何针对探测结果作出反应： livenessProbe：指示容器是否正在运行。如果存活态探测失败，则 kubelet 会杀死容器， 并且容器将根据其重启策略决定未来。如果容器不提供存活探针， 则默认状态为 Success。 readinessProbe：指示容器是否准备好为请求提供服务。如果就绪态探测失败， 端点控制器将从与 Pod 匹配的所有服务的端点列表中删除该 Pod 的 IP 地址。 初始延迟之前的就绪态的状态值默认为 Failure。 如果容器不提供就绪态探针，则默认状态为 Success。 startupProbe: 指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被 禁用，直到此探针成功为止。如果启动探测失败，kubelet 将杀死容器，而容器依其 重启策略进行重启。 如果容器没有提供启动探测，则默认状态为 Success。 如欲了解如何设置存活态、就绪态和启动探针的进一步细节，可以参阅 配置存活态、就绪态和启动探针。 何时该使用存活态探针?FEATURE STATE: Kubernetes v1.0 [stable] 如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活态探针; kubelet 将根据 Pod 的restartPolicy 自动执行修复操作。 如果你希望容器在探测失败时被杀死并重新启动，那么请指定一个存活态探针， 并指定restartPolicy 为 “Always“ 或 “OnFailure“。 何时该使用就绪态探针?FEATURE STATE: Kubernetes v1.0 [stable] 如果要仅在探测成功时才开始向 Pod 发送请求流量，请指定就绪态探针。 在这种情况下，就绪态探针可能与存活态探针相同，但是规约中的就绪态探针的存在意味着 Pod 将在启动阶段不接收任何数据，并且只有在探针探测成功后才开始接收数据。 如果你的容器需要加载大规模的数据、配置文件或者在启动期间执行迁移操作，可以添加一个 就绪态探针。 如果你希望容器能够自行进入维护状态，也可以指定一个就绪态探针，检查某个特定于 就绪态的因此不同于存活态探测的端点。 说明： 请注意，如果你只是想在 Pod 被删除时能够排空请求，则不一定需要使用就绪态探针； 在删除 Pod 时，Pod 会自动将自身置于未就绪状态，无论就绪态探针是否存在。 等待 Pod 中的容器停止期间，Pod 会一直处于未就绪状态。 何时该使用启动探针？FEATURE STATE: Kubernetes v1.16 [alpha] 对于所包含的容器需要较长时间才能启动就绪的 Pod 而言，启动探针是有用的。 你不再需要配置一个较长的存活态探测时间间隔，只需要设置另一个独立的配置选定， 对启动期间的容器执行探测，从而允许使用远远超出存活态时间间隔所允许的时长。 如果你的容器启动时间通常超出 initialDelaySeconds + failureThreshold × periodSeconds 总值，你应该设置一个启动探测，对存活态探针所使用的同一端点执行检查。 periodSeconds 的默认值是 30 秒。你应该将其 failureThreshold 设置得足够高， 以便容器有充足的时间完成启动，并且避免更改存活态探针所使用的默认值。 这一设置有助于减少死锁状况的发生。 Pod 的终止由于 Pod 所代表的是在集群中节点上运行的进程，当不再需要这些进程时允许其体面地 终止是很重要的。一般不应武断地使用 KILL 信号终止它们，导致这些进程没有机会 完成清理操作。 设计的目标是令你能够请求删除进程，并且知道进程何时被终止，同时也能够确保删除 操作终将完成。当你请求删除某个 Pod 时，集群会记录并跟踪 Pod 的体面终止周期， 而不是直接强制地杀死 Pod。在存在强制关闭设施的前提下， kubelet 会尝试体面地终止 Pod。 通常情况下，容器运行时会发送一个 TERM 信号到每个容器中的主进程。 一旦超出了体面终止限期，容器运行时会向所有剩余进程发送 KILL 信号，之后 Pod 就会被从 API 服务器 上移除。如果 kubelet 或者容器运行时的管理服务在等待进程终止期间被重启， 集群会从头开始重试，赋予 Pod 完整的体面终止限期。 下面是一个例子： 你使用 kubectl 工具手动删除某个特定的 Pod，而该 Pod 的体面终止限期是默认值（30 秒）。 API 服务器中的 Pod 对象被更新，记录涵盖体面终止限期在内 Pod 的最终死期，超出所计算时间点则认为 Pod 已死（dead）。 如果你使用 kubectl describe 来查验你正在删除的 Pod，该 Pod 会显示为 “Terminating” （正在终止）。 在 Pod 运行所在的节点上：kubelet 一旦看到 Pod 被标记为正在终止（已经设置了体面终止限期），kubelet 即开始本地的 Pod 关闭过程。 如果 Pod 中的容器之一定义了 preStop 回调， kubelet 开始在容器内运行该回调逻辑。如果超出体面终止限期时，preStop 回调逻辑 仍在运行，kubelet 会请求给予该 Pod 的宽限期一次性增加 2 秒钟。 说明： 如果 preStop 回调所需要的时间长于默认的体面终止限期，你必须修改 terminationGracePeriodSeconds 属性值来使其正常工作。 kubelet 接下来触发容器运行时发送 TERM 信号给每个容器中的进程 1。 说明： Pod 中的容器会在不同时刻收到 TERM 信号，接收顺序也是不确定的。 如果关闭的顺序很重要，可以考虑使用 preStop 回调逻辑来协调。 与此同时，kubelet 启动体面关闭逻辑，控制面会将 Pod 从对应的端点列表（以及端点切片列表， 如果启用了的话）中移除，过滤条件是 Pod 被对应的 服务以某 选择算符选定。 ReplicaSets和其他工作负载资源 不再将关闭进程中的 Pod 视为合法的、能够提供服务的副本。关闭动作很慢的 Pod 也无法继续处理请求数据，因为负载均衡器（例如服务代理）已经在终止宽限期开始的时候 将其从端点列表中移除。 超出终止宽限期线时，kubelet 会触发强制关闭过程。容器运行时会向 Pod 中所有容器内 仍在运行的进程发送 SIGKILL 信号。 kubelet 也会清理隐藏的 pause 容器，如果容器运行时使用了这种容器的话。 kubelet 触发强制从 API 服务器上删除 Pod 对象的逻辑，并将体面终止限期设置为 0 （这意味着马上删除）。 API 服务器删除 Pod 的 API 对象，从任何客户端都无法再看到该对象。 强制终止 Pod 注意： 对于某些工作负载及其 Pod 而言，强制删除很可能会带来某种破坏。 默认情况下，所有的删除操作都会附有 30 秒钟的宽限期限。 kubectl delete 命令支持 --grace-period=&lt;seconds&gt; 选项，允许你重载默认值， 设定自己希望的期限值。 将宽限期限强制设置为 0 意味着立即从 API 服务器删除 Pod。 如果 Pod 仍然运行于某节点上，强制删除操作会触发 kubelet 立即执行清理操作。 说明： 你必须在设置 --grace-period=0 的同时额外设置 --force 参数才能发起强制删除请求。 执行强制删除操作时，API 服务器不再等待来自 kubelet 的、关于 Pod 已经在原来运行的节点上终止执行的确认消息。 API 服务器直接删除 Pod 对象，这样新的与之同名的 Pod 即可以被创建。 在节点侧，被设置为立即终止的 Pod 仍然会在被强行杀死之前获得一点点的宽限时间。 如果你需要强制删除 StatefulSet 的 Pod，请参阅 从 StatefulSet 中删除 Pod 的任务文档。 失效 Pod 的垃圾收集对于已失败的 Pod 而言，对应的 API 对象仍然会保留在集群的 API 服务器上，直到 用户或者控制器进程显式地 将其删除。 控制面组件会在 Pod 个数超出所配置的阈值 （根据 kube-controller-manager 的 terminated-pod-gc-threshold 设置）时 删除已终止的 Pod（阶段值为 Succeeded 或 Failed）。 这一行为会避免随着时间演进不断创建和终止 Pod 而引起的资源泄露问题。","categories":[{"name":"容器","slug":"容器","permalink":"https://cyylog.github.io/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://cyylog.github.io/tags/Kubernetes/"}]},{"title":"Kubernetes-Pod-Pods","slug":"容器/Kubernetes工作负载—Pods","date":"2020-10-05T16:10:00.000Z","updated":"2020-10-30T04:13:16.251Z","comments":true,"path":"2020/10/06/rong-qi/kubernetes-gong-zuo-fu-zai-pods/","link":"","permalink":"https://cyylog.github.io/2020/10/06/rong-qi/kubernetes-gong-zuo-fu-zai-pods/","excerpt":"","text":"PodsPod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。 Pod （就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个） 容器； 这些容器共享存储、网络、以及怎样运行这些容器的声明。 Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。 Pod 所建模的是特定于应用的“逻辑主机”，其中包含一个或多个应用容器， 这些容器是相对紧密的耦合在一起的。 在非云环境中，在相同的物理机或虚拟机上运行的应用类似于 在同一逻辑主机上运行的云应用。 除了应用容器，Pod 还可以包含在 Pod 启动期间运行的 Init 容器。 你也可以在集群中支持临时性容器 的情况外，为调试的目的注入临时性容器。 什么是 Pod？ 说明： 除了 Docker 之外，Kubernetes 支持 很多其他容器运行时， Docker 是最有名的运行时， 使用 Docker 的术语来描述 Pod 会很有帮助。 Pod 的共享上下文包括一组 Linux 名字空间、控制组（cgroup）和可能一些其他的隔离 方面，即用来隔离 Docker 容器的技术。 在 Pod 的上下文中，每个独立的应用可能会进一步实施隔离。 就 Docker 概念的术语而言，Pod 类似于共享名字空间和文件系统卷的一组 Docker 容器。 使用 Pod通常你不需要直接创建 Pod，甚至单实例 Pod。 相反，你会使用诸如 Deployment 或 Job 这类工作负载资源 来创建 Pod。如果 Pod 需要跟踪状态， 可以考虑 StatefulSet 资源。 Kubernetes 集群中的 Pod 主要有两种用法： 运行单个容器的 Pod。”每个 Pod 一个容器”模型是最常见的 Kubernetes 用例； 在这种情况下，可以将 Pod 看作单个容器的包装器，并且 Kubernetes 直接管理 Pod，而不是容器。 运行多个协同工作的容器的 Pod。 Pod 可能封装由多个紧密耦合且需要共享资源的共处容器组成的应用程序。 这些位于同一位置的容器可能形成单个内聚的服务单元 —— 一个容器将文件从共享卷提供给公众， 而另一个单独的“挂斗”（sidecar）容器则刷新或更新这些文件。 Pod 将这些容器和存储资源打包为一个可管理的实体。 说明： 将多个并置、同管的容器组织到一个 Pod 中是一种相对高级的使用场景。 只有在一些场景中，容器之间紧密关联时你才应该使用这种模式。 每个 Pod 都旨在运行给定应用程序的单个实例。如果希望横向扩展应用程序（例如，运行多个实例 以提供更多的资源），则应该使用多个 Pod，每个实例使用一个 Pod。 在 Kubernetes 中，这通常被称为 副本（Replication）。 通常使用一种工作负载资源及其控制器 来创建和管理一组 Pod 副本。 参见 Pod 和控制器以了解 Kubernetes 如何使用工作负载资源及其控制器以实现应用的扩缩和自动修复。 Pod 怎样管理多个容器Pod 被设计成支持形成内聚服务单元的多个协作过程（形式为容器）。 Pod 中的容器被自动安排到集群中的同一物理机或虚拟机上，并可以一起进行调度。 容器之间可以共享资源和依赖、彼此通信、协调何时以及何种方式终止自身。 例如，你可能有一个容器，为共享卷中的文件提供 Web 服务器支持，以及一个单独的 “sidecar（挂斗）”容器负责从远端更新这些文件，如下图所示： 有些 Pod 具有 Init 容器 和 应用容器。 Init 容器会在启动应用容器之前运行并完成。 Pod 天生地为其成员容器提供了两种共享资源：网络和 存储。 使用 Pod你很少在 Kubernetes 中直接创建一个个的 Pod，甚至是单实例（Singleton）的 Pod。 这是因为 Pod 被设计成了相对临时性的、用后即抛的一次性实体。 当 Pod 由你或者间接地由 控制器 创建时，它被调度在集群中的节点上运行。 Pod 会保持在该节点上运行，直到 Pod 结束执行、Pod 对象被删除、Pod 因资源不足而被 驱逐 或者节点失效为止。 说明： 重启 Pod 中的容器不应与重启 Pod 混淆。 Pod 不是进程，而是容器运行的环境。 在被删除之前，Pod 会一直存在。 当你为 Pod 对象创建清单时，要确保所指定的 Pod 名称是合法的 DNS 子域名。 Pod 和控制器你可以使用工作负载资源来创建和管理多个 Pod。 资源的控制器能够处理副本的管理、上线，并在 Pod 失效时提供自愈能力。 例如，如果一个节点失败，控制器注意到该节点上的 Pod 已经停止工作， 就可以创建替换性的 Pod。调度器会将替身 Pod 调度到一个健康的节点执行。 下面是一些管理一个或者多个 Pod 的工作负载资源的示例： Deployment StatefulSet DaemonSet Pod 模版负载资源的控制器通常使用 Pod 模板（Pod Template） 来替你创建 Pod 并管理它们。 Pod 模板是包含在工作负载对象中的规范，用来创建 Pod。这类负载资源包括 Deployment、 Job 和 DaemonSets等。 工作负载的控制器会使用负载对象中的 PodTemplate 来生成实际的 Pod。 PodTemplate 是你用来运行应用时指定的负载资源的目标状态的一部分。 下面的示例是一个简单的 Job 的清单，其中的 template 指示启动一个容器。 该 Pod 中的容器会打印一条消息之后暂停。 apiVersion: batch/v1 kind: Job metadata: name: hello spec: template: # 这里是 Pod 模版 spec: containers: - name: hello image: busybox command: ['sh', '-c', 'echo \"Hello, Kubernetes!\" &amp;&amp; sleep 3600'] restartPolicy: OnFailure # 以上为 Pod 模版 修改 Pod 模版或者切换到新的 Pod 模版都不会对已经存在的 Pod 起作用。 Pod 不会直接收到模版的更新。相反， 新的 Pod 会被创建出来，与更改后的 Pod 模版匹配。 例如，Deployment 控制器针对每个 Deployment 对象确保运行中的 Pod 与当前的 Pod 模版匹配。如果模版被更新，则 Deployment 必须删除现有的 Pod，基于更新后的模版 创建新的 Pod。每个工作负载资源都实现了自己的规则，用来处理对 Pod 模版的更新。 在节点上，kubelet并不直接监测 或管理与 Pod 模版相关的细节或模版的更新，这些细节都被抽象出来。 这种抽象和关注点分离简化了整个系统的语义，并且使得用户可以在不改变现有代码的 前提下就能扩展集群的行为。 资源共享和通信Pod 使它的成员容器间能够进行数据共享和通信。 Pod 中的存储一个 Pod 可以设置一组共享的存储卷。 Pod 中的所有容器都可以访问该共享卷，从而允许这些容器共享数据。 卷还允许 Pod 中的持久数据保留下来，即使其中的容器需要重新启动。 有关 Kubernetes 如何在 Pod 中实现共享存储并将其提供给 Pod 的更多信息， 请参考卷。 Pod 联网每个 Pod 都在每个地址族中获得一个唯一的 IP 地址。 Pod 中的每个容器共享网络名字空间，包括 IP 地址和网络端口。 Pod 内 的容器可以使用 localhost 互相通信。 当 Pod 中的容器与 Pod 之外 的实体通信时，它们必须协调如何使用共享的网络资源 （例如端口）。 在同一个 Pod 内，所有容器共享一个 IP 地址和端口空间，并且可以通过 localhost 发现对方。 他们也能通过如 SystemV 信号量或 POSIX 共享内存这类标准的进程间通信方式互相通信。 不同 Pod 中的容器的 IP 地址互不相同，没有 特殊配置 就不能使用 IPC 进行通信。 如果某容器希望与运行于其他 Pod 中的容器通信，可以通过 IP 联网的方式实现。 Pod 中的容器所看到的系统主机名与为 Pod 配置的 name 属性值相同。 网络部分提供了更多有关此内容的信息。 容器的特权模式Pod 中的任何容器都可以使用容器规约中的 安全性上下文中的 privileged 参数启用特权模式。 这对于想要使用使用操作系统管理权能（Capabilities，如操纵网络堆栈和访问设备） 的容器很有用。 容器内的进程几乎可以获得与容器外的进程相同的特权。 说明： 你的容器运行时必须支持 特权容器的概念才能使用这一配置。 静态 Pod静态 Pod（Static Pod） 直接由特定节点上的 kubelet 守护进程管理， 不需要API 服务器看到它们。 尽管大多数 Pod 都是通过控制面（例如，Deployment） 来管理的，对于静态 Pod 而言，kubelet 直接监控每个 Pod，并在其失效时重启之。 静态 Pod 通常绑定到某个节点上的 kubelet。 其主要用途是运行自托管的控制面。 在自托管场景中，使用 kubelet 来管理各个独立的 控制面组件。 kubelet 自动尝试为每个静态 Pod 在 Kubernetes API 服务器上创建一个 镜像 Pod。 这意味着在节点上运行的 Pod 在 API 服务器上是可见的，但不可以通过 API 服务器来控制。","categories":[{"name":"容器","slug":"容器","permalink":"https://cyylog.github.io/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://cyylog.github.io/tags/Kubernetes/"}]},{"title":"Redis-哨兵","slug":"SQL/Redis-哨兵","date":"2020-10-05T02:55:39.000Z","updated":"2020-10-30T04:13:37.549Z","comments":true,"path":"2020/10/05/sql/redis-shao-bing/","link":"","permalink":"https://cyylog.github.io/2020/10/05/sql/redis-shao-bing/","excerpt":"","text":"Redis宕机后如何实现快速恢复一、部署模式Redis在部署时，可以采用多种方式部署，每种部署方式对应不同的可用级别。 单节点部署 ：只有一个节点提供服务，读写均在此节点，此节点宕机则数据全部丢失，直接影响业务。 master-slave方式部署 ：两个节点组成master-slave模式，在master上写入，slave上读取，读写分离提高访问性能，master宕机后，需要手动把slave提升为master，业务影响程度取决于手动提升master的延迟。 master-slave+哨兵方式部署 ：master-slave与上述相同，不同的是增加一组哨兵节点，用于实时检查master的健康状态，在master宕机后自动提升slave为新的master，最大程度降低不可用的时间，对业务影响时间较短。 从上面几种部署模式可以看出，提高Redis可用性的关键是：多副本部署 + 自动故障恢复，而多副本正是依赖主从复制。 二、哨兵介绍哨兵是Redis高可用的解决方案，它是一个管理多个Redis实例的服务工具，可以实现对Redis实例的监控、通知、自动故障转移。 在部署哨兵时，我们只需要在配置文件中配置需要管理的master节点，哨兵节点就可以根据配置，对Redis节点进行管理，实现高可用。 一般我们需要部署多个哨兵节点，这是因为在分布式场景下，要想确定某个机器的某个节点上否发生故障，只用一台机器去检测可能是不准确的，很有可能这两台机器的网络发生了故障，而节点本身并没有问题。 所以对于节点健康检测的场景，一般都会采用多个节点同时去检测，且多个节点分布在不同机器上，节点数量为奇数个，避免因为网络分区导致哨兵决策错误。这样多个哨兵节点互相交换检测信息，最终决策才能确认某个节点上否真正发生了问题。 哨兵节点部署并配置完成后，哨兵就会自动地对配置的master-slave进行管理，在master发生故障时，及时地提升slave为新的master，保证可用性。 三、哨兵原理哨兵的工作流程主要分为以下几个阶段： 状态感知 心跳检测 选举哨兵领导者 选择新的master 故障恢复 客户端感知新master 1. 状态感知哨兵启动后只指定了master的地址，哨兵要想在master故障时进行故障恢复，就需要知道每个master对应的slave信息。每个master可能不止一个slave，因此哨兵需要知道整个集群中完整的的拓扑关系，如何拿到这些信息？ 哨兵每隔10秒会向每个master节点发送info命令，info命令返回的信息中，包含了主从拓扑关系，其中包括每个slave的地址和端口号。有了这些信息后，哨兵就会记住这些节点的拓扑信息，在后续发生故障时，选择合适的slave节点进行故障恢复。 哨兵除了向master发送info之外，还会向每个master节点特殊的pubsub中发送master当前的状态信息和哨兵自身的信息，其他哨兵节点通过订阅这个pubsub，就可以拿到每个哨兵发来的信息。 这么做的目的主要有2个： 哨兵节点可以发现其他哨兵的加入，进而方便多个哨兵节点通信，为后续共同协商提供基础 与其他哨兵节点交换master的状态信息，为后续判断master是否故障提供依据 2. 心跳检测在故障发生时，需要立即启动故障恢复机制，那么如何保证及时性呢？ 每个哨兵节点每隔1秒向master、slave、其他哨兵节点发送ping命令，如果对方能在指定时间内响应，说明节点健康存活。如果未在规定时间内（可配置）响应，那么该哨兵节点认为此节点主观下线。 3. 为什么叫做主观下线？因为当前哨兵节点探测对方没有得到响应，很有可能这两个机器之间的网络发生了故障，而master节点本身没有任何问题，此时就认为master故障是不正确的。 要想确认master节点是否真正发生故障，就需要多个哨兵节点共同确认才行。 每个哨兵节点通过向其他哨兵节点询问此master的状态，来共同确认此节点上否真正故障。 如果超过指定数量（可配置）的哨兵节点都认为此节点主观下线，那么才会把这个节点标记为客观下线。 4. 选举哨兵领导者确认这个节点真正故障后，就需要进入到故障恢复阶段。如何进行故障恢复，也需要经历一系列流程。 首先需要选举出一个哨兵领导者，由这个专门的哨兵领导者来进行故障恢复操作，不用多个哨兵都参与故障恢复。选举哨兵领导者的过程，需要多个哨兵节点共同协商来选出。 这个选举协商的过程，在分布式领域中叫做达成共识，协商的算法叫做共识算法。 共识算法主要为了解决在分布式场景下，多个节点如何针对某一个场景达成一致的结果。 共识算法包括很多种，例如Paxos、Raft、Gossip算法等，感兴趣的同学可以自行搜索相关资料，这里不再展开来讲。 哨兵选举领导者的过程类似于Raft算法，它的算法足够简单易理解。 简单来讲流程如下： 每个哨兵都设置一个随机超时时间，超时后向其他哨兵发送申请成为领导者的请求 其他哨兵只能对收到的第一个请求进行回复确认 首先达到多数确认选票的哨兵节点，成为领导者 如果在确认回复后，所有哨兵都无法达到多数选票的结果，那么进行重新选举，直到选出领导者为止选择出哨兵领导者后，之后的故障恢复操作都由这个哨兵领导者进行操作。 5. 选择新的master哨兵领导者针对发生故障的master节点，需要在它的slave节点中，选择一个节点来代替其工作。 这个选择新master过程也是有优先级的，在多个slave的场景下，优先级按照：slave-priority配置 &gt; 数据完整性 &gt; runid较小者进行选择。 也就是说优先选择slave-priority最小值的slave节点，如果所有slave此配置相同，那么选择数据最完整的slave节点，如果数据也一样，最后选择runid较小的slave节点。 6.提升新的master经过优先级选择，选出了备选的master节点后，下一步就是要进行真正的主从切换了。 哨兵领导者给备选的master节点发送slaveof no one命令，让该节点成为master。 之后，哨兵领导者会给故障节点的所有slave发送slaveof $newmaster命令，让这些slave成为新master的从节点，开始从新的master上同步数据。 最后哨兵领导者把故障节点降级为slave，并写入到自己的配置文件中，待这个故障节点恢复后，则自动成为新master节点的slave。 至此，整个故障切换完成。 7. 客户端感知新master最后，客户端如何拿到最新的master地址呢？ 哨兵在故障切换完成之后，会向自身节点的指定pubsub中写入一条信息，客户端可以订阅这个pubsub来感知master的变化通知。我们的客户端也可以通过在哨兵节点主动查询当前最新的master，来拿到最新的master地址。 另外，哨兵还提供了“钩子”机制，我们也可以在哨兵配置文件中配置一些脚本逻辑，在故障切换完成时，触发“钩子”逻辑，通知客户端发生了切换，让客户端重新在哨兵上获取最新的master地址。 一般来说，推荐采用第一种方式进行处理，很多客户端SDK中已经集成好了从哨兵节点获取最新master的方法，我们直接使用即可。 四、PS可见，为了保证Redis的高可用，哨兵节点要准确无误地判断故障的发生，并且快速的选出新的节点来代替其提供服务，这中间的流程还是比较复杂的。 中间涉及到了分布式共识、分布式协商等知识，目的都是为了保证故障切换的准确性。 我们有必要了解Redis高可用的工作原理，这样我们在使用Redis时能更准确地使用它。","categories":[{"name":"SQL","slug":"SQL","permalink":"https://cyylog.github.io/categories/SQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://cyylog.github.io/tags/Redis/"}]},{"title":"Kubernetes架构-控制器","slug":"容器/Kubernetes架构—控制器","date":"2020-10-03T12:57:00.000Z","updated":"2020-10-30T04:12:55.932Z","comments":true,"path":"2020/10/03/rong-qi/kubernetes-jia-gou-kong-zhi-qi/","link":"","permalink":"https://cyylog.github.io/2020/10/03/rong-qi/kubernetes-jia-gou-kong-zhi-qi/","excerpt":"","text":"控制器在机器人技术和自动化领域，控制回路（Control Loop）是一个非终止回路，用于调节系统状态。 这是一个控制环的例子：房间里的温度自动调节器。 当你设置了温度，告诉了温度自动调节器你的期望状态（Desired State）。 房间的实际温度是当前状态（Current State）。 通过对设备的开关控制，温度自动调节器让其当前状态接近期望状态。 控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。 控制器模式一个控制器至少追踪一种类型的 Kubernetes 资源。这些 对象 有一个代表期望状态的 spec 字段。 该资源的控制器负责确保其当前状态接近期望状态。 控制器可能会自行执行操作；在 Kubernetes 中更常见的是一个控制器会发送信息给 API 服务器，这会有副作用。 具体可参看后文的例子。 通过 API 服务器来控制Job 控制器是一个 Kubernetes 内置控制器的例子。 内置控制器通过和集群 API 服务器交互来管理状态。 Job 是一种 Kubernetes 资源，它运行一个或者多个 Pod， 来执行一个任务然后停止。 （一旦被调度了，对 kubelet 来说 Pod 对象就会变成了期望状态的一部分）。 在集群中，当 Job 控制器拿到新任务时，它会保证一组 Node 节点上的 kubelet 可以运行正确数量的 Pod 来完成工作。 Job 控制器不会自己运行任何的 Pod 或者容器。Job 控制器是通知 API 服务器来创建或者移除 Pod。 控制面中的其它组件 根据新的消息作出反应（调度并运行新 Pod）并且最终完成工作。 创建新 Job 后，所期望的状态就是完成这个 Job。Job 控制器会让 Job 的当前状态不断接近期望状态：创建为 Job 要完成工作所需要的 Pod，使 Job 的状态接近完成。 控制器也会更新配置对象。例如：一旦 Job 的工作完成了，Job 控制器会更新 Job 对象的状态为 Finished。 （这有点像温度自动调节器关闭了一个灯，以此来告诉你房间的温度现在到你设定的值了）。 直接控制相比 Job 控制器，有些控制器需要对集群外的一些东西进行修改。 例如，如果你使用一个控制环来保证集群中有足够的节点，那么控制就需要当前集群外的一些服务在需要时创建新节点。 和外部状态交互的控制器从 API 服务器获取到它想要的状态，然后直接和外部系统进行通信并使当前状态更接近期望状态。 （实际上有一个控制器可以水平地扩展集群中的节点。请参阅 集群自动扩缩容）。 期望状态与当前状态Kubernetes 采用了系统的云原生视图，并且可以处理持续的变化。 在任务执行时，集群随时都可能被修改，并且控制回路会自动修复故障。这意味着很可能集群永远不会达到稳定状态。 只要集群中控制器的在运行并且进行有效的修改，整体状态的稳定与否是无关紧要的。 设计作为设计原则之一，Kubernetes 使用了很多控制器，每个控制器管理集群状态的一个特定方面。 最常见的一个特定的控制器使用一种类型的资源作为它的期望状态， 控制器管理控制另外一种类型的资源向它的期望状态演化。 使用简单的控制器而不是一组相互连接的单体控制回路是很有用的。 控制器会失败，所以 Kubernetes 的设计正是考虑到了这一点。 说明： 可以有多个控制器来创建或者更新相同类型的对象。 在后台，Kubernetes 控制器确保它们只关心与其控制资源相关联的资源。 例如，你可以创建 Deployment 和 Job；它们都可以创建 Pod。 Job 控制器不会删除 Deployment 所创建的 Pod，因为有信息 （标签）让控制器可以区分这些 Pod。 运行控制器的方式Kubernetes 内置一组控制器，运行在 kube-controller-manager 内。 这些内置的控制器提供了重要的核心功能。 Deployment 控制器和 Job 控制器是 Kubernetes 内置控制器的典型例子。 Kubernetes 允许你运行一个稳定的控制平面，这样即使某些内置控制器失败了， 控制平面的其他部分会接替它们的工作。 你会遇到某些控制器运行在控制面之外，用以扩展 Kubernetes。 或者，如果你愿意，你也可以自己编写新控制器。 你可以以一组 Pod 来运行你的控制器，或者运行在 Kubernetes 之外。 最合适的方案取决于控制器所要执行的功能是什么。","categories":[{"name":"容器","slug":"容器","permalink":"https://cyylog.github.io/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://cyylog.github.io/tags/Kubernetes/"}]},{"title":"Kubernetes架构-节点","slug":"容器/Kubernetes架构—节点","date":"2020-10-03T12:57:00.000Z","updated":"2020-10-30T04:12:44.758Z","comments":true,"path":"2020/10/03/rong-qi/kubernetes-jia-gou-jie-dian/","link":"","permalink":"https://cyylog.github.io/2020/10/03/rong-qi/kubernetes-jia-gou-jie-dian/","excerpt":"","text":"节点Kubernetes 通过将容器放入在节点（Node）上运行的 Pod 中来执行你的工作负载。 节点可以是一个虚拟机或者物理机器，取决于所在的集群配置。每个节点都包含用于运行 Pod 所需要的服务，这些服务由 控制面负责管理。 通常集群中会有若干个节点；而在一个学习用或者资源受限的环境中，你的集群中也可能 只有一个节点。 节点上的组件包括 kubelet、 容器运行时以及 kube-proxy。 管理向 API 服务器添加节点的方式主要有两种： 节点上的 kubelet 向控制面执行自注册； 你，或者别的什么人，手动添加一个 Node 对象。 在你创建了 Node 对象或者节点上的 kubelet 执行了自注册操作之后， 控制面会检查新的 Node 对象是否合法。例如，如果你使用下面的 JSON 对象来创建 Node 对象： { \"kind\": \"Node\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"10.240.79.157\", \"labels\": { \"name\": \"my-first-k8s-node\" } } } Kubernetes 会在内部创建一个 Node 对象作为节点的表示。Kubernetes 检查 kubelet 向 API 服务器注册节点时使用的 metadata.name 字段是否匹配。 如果节点是健康的（即所有必要的服务都在运行中），则该节点可以用来运行 Pod。 否则，直到该节点变为健康之前，所有的集群活动都会忽略该节点。 说明： Kubernetes 会一直保存着非法节点对应的对象，并持续检查该节点是否已经 变得健康。 你，或者某个控制器必需显式地 删除该 Node 对象以停止健康检查操作。 Node 对象的名称必须是合法的 DNS 子域名。 节点自注册当 kubelet 标志 --register-node 为 true（默认）时，它会尝试向 API 服务注册自己。 这是首选模式，被绝大多数发行版选用。 对于自注册模式，kubelet 使用下列参数启动： --kubeconfig - 用于向 API 服务器表明身份的凭据路径。 --cloud-provider - 与某云驱动 进行通信以读取与自身相关的元数据的方式。 --register-node - 自动向 API 服务注册。 --register-with-taints - 使用所给的污点列表（逗号分隔的 &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;）注册节点。 当 register-node 为 false 时无效。 --node-ip - 节点 IP 地址。 --node-labels - 在集群中注册节点时要添加的 标签。 （参见 NodeRestriction 准入控制插件所实施的标签限制）。 --node-status-update-frequency - 指定 kubelet 向控制面发送状态的频率。 启用节点授权模式和 NodeRestriction 准入插件 时，仅授权 kubelet 创建或修改其自己的节点资源。 手动节点管理你可以使用 kubectl 来创建和修改 Node 对象。 如果你希望手动创建节点对象时，请设置 kubelet 标志 --register-node=false。 你可以修改 Node 对象（忽略 --register-node 设置）。 例如，修改节点上的标签或标记其为不可调度。 你可以结合使用节点上的标签和 Pod 上的选择算符来控制调度。 例如，你可以限制某 Pod 只能在符合要求的节点子集上运行。 如果标记节点为不可调度（unschedulable），将阻止新 Pod 调度到该节点之上，但不会 影响任何已经在其上的 Pod。 这是重启节点或者执行其他维护操作之前的一个有用的准备步骤。 要标记一个节点为不可调度，执行以下命令： kubectl cordon $NODENAME 说明： 被 DaemonSet 控制器创建的 Pod 能够容忍节点的不可调度属性。 DaemonSet 通常提供节点本地的服务，即使节点上的负载应用已经被腾空，这些服务也仍需 运行在节点之上。 节点状态一个节点的状态包含以下信息: 地址 状况 容量与可分配 信息 你可以使用 kubectl 来查看节点状态和其他细节信息： kubectl describe node 下面对每个部分进行详细描述。 地址这些字段的用法取决于你的云服务商或者物理机配置。 HostName：由节点的内核设置。可以通过 kubelet 的 --hostname-override 参数覆盖。 ExternalIP：通常是节点的可外部路由（从集群外可访问）的 IP 地址。 InternalIP：通常是节点的仅可在集群内部路由的 IP 地址。 状况conditions 字段描述了所有 Running 节点的状态。状况的示例包括： 说明： 如果使用命令行工具来打印已保护（Cordoned）节点的细节，其中的 Condition 字段可能 包括 SchedulingDisabled。SchedulingDisabled 不是 Kubernetes API 中定义的 Condition，被保护起来的节点在其规约中被标记为不可调度（Unschedulable）。 节点条件使用 JSON 对象表示。例如，下面的响应描述了一个健康的节点。 \"conditions\": [ { \"type\": \"Ready\", \"status\": \"True\", \"reason\": \"KubeletReady\", \"message\": \"kubelet is posting ready status\", \"lastHeartbeatTime\": \"2019-06-05T18:38:35Z\", \"lastTransitionTime\": \"2019-06-05T11:41:27Z\" } ] 如果 Ready 条件处于 Unknown 或者 False 状态的时间超过了 pod-eviction-timeout 值， （一个传递给 kube-controller-manager 的参数）， 节点上的所有 Pod 都会被节点控制器计划删除。默认的逐出超时时长为 5 分钟。 某些情况下，当节点不可达时，API 服务器不能和其上的 kubelet 通信。 删除 Pod 的决定不能传达给 kubelet，直到它重新建立和 API 服务器的连接为止。 与此同时，被计划删除的 Pod 可能会继续在游离的节点上运行。 节点控制器在确认 Pod 在集群中已经停止运行前，不会强制删除它们。 你可以看到这些可能在无法访问的节点上运行的 Pod 处于 Terminating 或者 Unknown 状态。 如果 kubernetes 不能基于下层基础设施推断出某节点是否已经永久离开了集群， 集群管理员可能需要手动删除该节点对象。 从 Kubernetes 删除节点对象将导致 API 服务器删除节点上所有运行的 Pod 对象并释放它们的名字。 节点生命周期控制器会自动创建代表状况的 污点。 当调度器将 Pod 指派给某节点时，会考虑节点上的污点。 Pod 则可以通过容忍度（Toleration）表达所能容忍的污点。 capacity 块中的字段标示节点拥有的资源总量。 allocatable 块指示节点上可供普通 Pod 消耗的资源量。 可以在学习如何在节点上预留计算资源 的时候了解有关容量和可分配资源的更多信息。 第二个是保持节点控制器内的节点列表与云服务商所提供的可用机器列表同步。 如果在云环境下运行，只要某节点不健康，节点控制器就会询问云服务是否节点的虚拟机仍可用。 如果不可用，节点控制器会将该节点从它的节点列表删除。 第三个是监控节点的健康情况。节点控制器负责在节点不可达 （即，节点控制器因为某些原因没有收到心跳，例如节点宕机）时， 将节点状态的 NodeReady 状况更新为 “Unknown“。 如果节点接下来持续处于不可达状态，节点控制器将逐出节点上的所有 Pod（使用体面终止）。 默认情况下 40 秒后开始报告 “Unknown“，在那之后 5 分钟开始逐出 Pod。 节点控制器每隔 --node-monitor-period 秒检查每个节点的状态。 kubelet 负责创建和更新 NodeStatus 和 Lease 对象。 当一个可用区域（Availability Zone）中的节点变为不健康时，节点的驱逐行为将发生改变。 节点控制器会同时检查可用区域中不健康（NodeReady 状况为 Unknown 或 False） 的节点的百分比。如果不健康节点的比例超过 --unhealthy-zone-threshold （默认为 0.55）， 驱逐速率将会降低：如果集群较小（意即小于等于 --large-cluster-size-threshold 个节点 - 默认为 50），驱逐操作将会停止，否则驱逐速率将降为每秒 --secondary-node-eviction-rate 个（默认为 0.01）。 在单个可用区域实施这些策略的原因是当一个可用区域可能从控制面脱离时其它可用区域 可能仍然保持连接。 如果你的集群没有跨越云服务商的多个可用区域，那（整个集群）就只有一个可用区域。 跨多个可用区域部署你的节点的一个关键原因是当某个可用区域整体出现故障时， 工作负载可以转移到健康的可用区域。 因此，如果一个可用区域中的所有节点都不健康时，节点控制器会以正常的速率 --node-eviction-rate 进行驱逐操作。 在所有的可用区域都不健康（也即集群中没有健康节点）的极端情况下， 节点控制器将假设控制面节点的连接出了某些问题， 它将停止所有驱逐动作直到一些连接恢复。 节点控制器还负责驱逐运行在拥有 NoExecute 污点的节点上的 Pod， 除非这些 Pod 能够容忍此污点。 节点控制器还负责根据节点故障（例如节点不可访问或没有就绪）为其添加 污点。 这意味着调度器不会将 Pod 调度到不健康的节点上。 注意： kubectl cordon 会将节点标记为“不可调度（Unschedulable）”。 此操作的副作用是，服务控制器会将该节点从负载均衡器中之前的目标节点列表中移除， 从而使得来自负载均衡器的网络请求不会到达被保护起来的节点。 Kubernetes 调度器保证节点上 有足够的资源供其上的所有 Pod 使用。它会检查节点上所有容器的请求的总和不会超过节点的容量。 总的请求包括由 kubelet 启动的所有容器，但不包括由容器运行时直接启动的容器， 也不包括不受 kubelet 控制的其他进程。 说明： 如果要为非 Pod 进程显式保留资源。请参考 为系统守护进程预留资源。 节点拓扑FEATURE STATE: Kubernetes v1.16 [alpha] 如果启用了 TopologyManager 特性门控， kubelet 可以在作出资源分配决策时使用拓扑提示。 参考控制节点上拓扑管理策略 了解详细信息。","categories":[{"name":"容器","slug":"容器","permalink":"https://cyylog.github.io/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://cyylog.github.io/tags/Kubernetes/"}]},{"title":"Kubernetes架构—组件","slug":"容器/Kubernetes架构—组件","date":"2020-10-03T11:57:00.000Z","updated":"2020-10-03T14:07:22.651Z","comments":true,"path":"2020/10/03/rong-qi/kubernetes-jia-gou-zu-jian/","link":"","permalink":"https://cyylog.github.io/2020/10/03/rong-qi/kubernetes-jia-gou-zu-jian/","excerpt":"","text":"Kubernetes 组件当你部署完 Kubernetes, 即拥有了一个完整的集群。 一个 Kubernetes 集群包含 集群由一组被称作节点的机器组成。这些节点上运行 Kubernetes 所管理的容器化应用。集群具有至少一个工作节点和至少一个主节点。 工作节点托管作为应用程序组件的 Pod 。主节点管理集群中的工作节点和 Pod 。多个主节点用于为集群提供故障转移和高可用性。 本文档概述了交付正常运行的 Kubernetes 集群所需的各种组件。 这张图表展示了包含所有相互关联组件的 Kubernetes 集群。 控制平面组件（Control Plane Components）控制平面的组件对集群做出全局决策(比如调度)，以及检测和响应集群事件（例如，当不满足部署的 replicas 字段时，启动新的 pod）。 控制平面组件可以在集群中的任何节点上运行。 然而，为了简单起见，设置脚本通常会在同一个计算机上启动所有控制平面组件，并且不会在此计算机上运行用户容器。 请参阅构建高可用性集群 中对于多主机 VM 的设置示例。 kube-apiserver主节点上负责提供 Kubernetes API 服务的组件；它是 Kubernetes 控制面的前端。 kube-apiserver 在设计上考虑了水平扩缩的需要。 换言之，通过部署多个实例可以实现扩缩。 参见构造高可用集群。 etcdetcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。 您的 Kubernetes 集群的 etcd 数据库通常需要有个备份计划。要了解 etcd 更深层次的信息，请参考 etcd 文档。 kube-scheduler主节点上的组件，该组件监视那些新创建的未指定运行节点的 Pod，并选择节点让 Pod 在上面运行。 调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。 kube-controller-manager在主节点上运行控制器的组件。 从逻辑上讲，每个控制器都是一个单独的进程，但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。 这些控制器包括: 节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应。 副本控制器（Replication Controller）: 负责为系统中的每个副本控制器对象维护正确数量的 Pod。 端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod)。 服务帐户和令牌控制器（Service Account &amp; Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌. cloud-controller-manager云控制器管理器是 1.8 的 alpha 特性。在未来发布的版本中，这是将 Kubernetes 与任何其他云集成的最佳方式。 cloud-controller-manager 进运行特定于云平台的控制回路。 如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境， 所部属的环境中不需要云控制器管理器。 与 kube-controller-manager 类似，cloud-controller-manager 将若干逻辑上独立的 控制回路组合到同一个可执行文件中，供你以同一进程的方式运行。 你可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。 下面的控制器都包含对云平台驱动的依赖： 节点控制器（Node Controller）: 用于在节点终止响应后检查云提供商以确定节点是否已被删除 路由控制器（Route Controller）: 用于在底层云基础架构中设置路由 服务控制器（Service Controller）: 用于创建、更新和删除云提供商负载均衡器 Node 组件节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。 kubelet一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。 kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。kubelet 不会管理不是由 Kubernetes 创建的容器。 kube-proxykube-proxy 是集群中每个节点上运行的网络代理,实现 Kubernetes Service 概念的一部分。 kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。 如果操作系统提供了数据包过滤层并可用的话，kube-proxy会通过它来实现网络规则。否则，kube-proxy 仅转发流量本身。 容器运行时（Container Runtime）容器运行环境是负责运行容器的软件。 Kubernetes 支持多个容器运行环境: Docker、 containerd、cri-o、 rktlet 以及任何实现 Kubernetes CRI (容器运行环境接口)。 插件（Addons）插件使用 Kubernetes 资源（DaemonSet、 Deployment等）实现集群功能。 因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 kube-system 命名空间。 下面描述众多插件中的几种。有关可用插件的完整列表，请参见 插件（Addons）。 DNS尽管其他插件都并非严格意义上的必需组件，但几乎所有 Kubernetes 集群都应该 有集群 DNS， 因为很多示例都需要 DNS 服务。 集群 DNS 是一个 DNS 服务器，和环境中的其他 DNS 服务器一起工作，它为 Kubernetes 服务提供 DNS 记录。 Kubernetes 启动的容器自动将此 DNS 服务器包含在其 DNS 搜索列表中。 Web 界面（仪表盘）Dashboard 是K ubernetes 集群的通用的、基于 Web 的用户界面。 它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除。 容器资源监控容器资源监控 将关于容器的一些常见的时间序列度量值保存到一个集中的数据库中，并提供用于浏览这些数据的界面。 集群层面日志集群层面日志 机制负责将容器的日志数据 保存到一个集中的日志存储中，该存储能够提供搜索和浏览接口。","categories":[{"name":"容器","slug":"容器","permalink":"https://cyylog.github.io/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://cyylog.github.io/tags/Kubernetes/"}]},{"title":"Mycat","slug":"SQL/Mycat","date":"2020-09-28T15:06:46.000Z","updated":"2020-09-28T15:17:13.849Z","comments":true,"path":"2020/09/28/sql/mycat/","link":"","permalink":"https://cyylog.github.io/2020/09/28/sql/mycat/","excerpt":"","text":"MySQL-ReplicationMySQL Replication主从复制（也称 AB 复制）允许将来自一个MySQL数据库服务器（主服务器）的数据复制到一个或多个MySQL数据库服务器（从服务器）。 复制是异步的 从站不需要永久连接以接收来自主站的更新。 根据配置，您可以复制数据库中的所有数据库，所选数据库甚至选定的表。 MySQL中复制的优点包括： 横向扩展解决方案 - 在多个从站之间分配负载以提高性能。在此环境中，所有写入和更新都必须在主服务器上进行。但是，读取可以在一个或多个从设备上进行。该模型可以提高写入性能（因为主设备专用于更新），同时显着提高了越来越多的从设备的读取速度。 数据安全性 - 因为数据被复制到从站，并且从站可以暂停复制过程，所以可以在从站上运行备份服务而不会破坏相应的主数据。 分析 - 可以在主服务器上创建实时数据，而信息分析可以在从服务器上进行，而不会影响主服务器的性能。 远程数据分发 - 您可以使用复制为远程站点创建数据的本地副本，而无需永久访问主服务器。 Replication的原理 前提是作为主服务器角色的数据库服务器必须开启二进制日志 主服务器上面的任何修改都会通过自己的 I/O tread(I/O 线程)保存在二进制日志 Binary log 里面。 从服务器上面也启动一个 I/O thread，通过配置好的用户名和密码, 连接到主服务器上面请求读取二进制日志，然后把读取到的二进制日志写到本地的一个Realy log（中继日志）里面。 从服务器上面同时开启一个 SQL thread 定时检查 Realy log(这个文件也是二进制的)，如果发现有更新立即把更新的内容在本机的数据库上面执行一遍。 每个从服务器都会收到主服务器二进制日志的全部内容的副本。 从服务器设备负责决定应该执行二进制日志中的哪些语句。 除非另行指定，否则主从二进制日志中的所有事件都在从站上执行。 如果需要，您可以将从服务器配置为仅处理一些特定数据库或表的事件。 重要: 您无法将主服务器配置为仅记录特定事件。 每个从站(从服务器)都会记录二进制日志坐标： 文件名 文件中它已经从主站读取和处理的位置。 由于每个从服务器都分别记录了自己当前处理二进制日志中的位置，因此可以断开从服务器的连接，重新连接然后恢复继续处理。 一主多从如果一主多从的话，这时主库既要负责写又要负责为几个从库提供二进制日志。此时可以稍做调整，将二进制日志只给某一从，这一从再开启二进制日志并将自己的二进制日志再发给其它从。或者是干脆这个从不记录只负责将二进制日志转发给其它从，这样架构起来性能可能要好得多，而且数据之间的延时应该也稍微要好一些。工作原理图如下： 练习题: 使用三台机器搭建一主多从的mysql架构并能够实现数据同步, 部署形式不限 关于二进制日志mysqld将数字扩展名附加到二进制日志基本名称以生成二进制日志文件名。每次服务器创建新日志文件时，该数字都会增加，从而创建一系列有序的文件。每次启动或刷新日志时，服务器都会在系列中创建一个新文件。服务器还会在当前日志大小达到max_binlog_size参数设置的大小后自动创建新的二进制日志文件 。二进制日志文件可能会比max_binlog_size使用大型事务时更大， 因为事务是以一个部分写入文件，而不是在文件之间分割。 为了跟踪已使用的二进制日志文件， mysqld还创建了一个二进制日志索引文件，其中包含所有使用的二进制日志文件的名称。默认情况下，它具有与二进制日志文件相同的基本名称，并带有扩展名&#39;.index&#39;。在mysqld运行时，您不应手动编辑此文件。 术语二进制日志文件通常表示包含数据库事件的单个编号文件。 术语 二进制日志 表示含编号的二进制日志文件集加上索引文件。 SUPER 权限的用户可以使用SET sql_log_bin=0语句禁用其当前环境下自己的语句的二进制日志记录 配置Replication配置步骤： 在主服务器上，您必须启用二进制日志记录并配置唯一的服务器ID。需要重启服务器。 编辑主服务器的配置文件 my.cnf，添加如下内容 [mysqld] log-bin=/var/log/mysql/mysql-bin server-id=1创建日志目录并赋予权限 [root@mysql ~]# mkdir /var/log/mysql [root@mysql ~]# chown mysql.mysql /var/log/mysql重启服务 [root@mysql ~]# systemctl restart mysqld注意：： 如果省略server-id（或将其显式设置为默认值0），则主服务器拒绝来自从服务器的任何连接。 为了在使用带事务的InnoDB进行复制设置时尽可能提高持久性和一致性， 您应该在master my.cnf文件中使用以下配置项： innodb_flush_log_at_trx_commit = 1 sync_binlog = 1 确保未在复制主服务器上启用skip-networking选项。 如果已禁用网络，则从站无法与主站通信，并且复制失败。2.应该创建一个专门用于复制数据的用户 每个从服务器需要使用MySQL 主服务器上的用户名和密码连接到主站。 例如，计划使用用户 repl 可以从任何主机上连接到 master 上进行复制操作, 并且用户 repl 仅可以使用复制的权限。 在 主服务器 上执行如下操作 mysql&gt; CREATE USER &#39;repl&#39;@&#39;%&#39; mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &#39;repl&#39;@&#39;%&#39; identified by &#39;123&#39;; mysql&gt; 3.在从服务器上使用刚才的用户进行测试连接 [root@mysql ~]# mysql -urepl -p&#39;123&#39; -hmysql-master1下面的操作根据如下情况继续 主服务器中有数据 如果在启动复制之前有现有数据需要与从属设备同步，请保持客户端正常运行，以便锁定保持不变。这可以防止进行任何进一步的更改，以便复制到从站的数据与主站同步。 在主服务器中导出先有的数据 如果主数据库包含现有数据，则必须将此数据复制到每个从站。有多种方法可以实现: 使用mysqldump工具创建要复制的所有数据库的转储。这是推荐的方法，尤其是在使用时 InnoDB。 [root@mysql ~]# mysqldump -u用户名 -p密码 --all-databases --master-data=1 &gt; dbdump.db 这里的用户是主服务器的用户如果不使用 --master-data 参数，则需要手动锁定单独会话中的所有表。 从主服务器中使用 scp 或 rsync 等工具，把备份出来的数据传输到从服务器中。 在主服务中执行如下命令 [root@mysql ~]# scp dbdump.db root@mysql-slave1:/root/ 这里的 mysql-slave1 需要能被主服务器解析出 IP 地址，或者说可以在主服务器中 ping 通。 配置从服务器，并重启在从服务器 上编辑其配置文件 my.cnf 并添加如下内容 // my.cnf 文件 [mysqld] server-id=2 导入数据到从服务器，并配置连接到主服务器的相关信息 登录到从服务器上，执行如下操作 /*导入数据*/ mysql&gt; source /root/fulldb.dump在从服务器配置连接到主服务器的相关信息 mysql&gt; CHANGE MASTER TO MASTER_HOST=&#39;mysql-master1&#39;, -- 主服务器的主机名(也可以是 IP) MASTER_USER=&#39;repl&#39;, -- 连接到主服务器的用户 MASTER_PASSWORD=&#39;123&#39;; -- 到主服务器的密码 启动从服务器的复制线程 mysql&gt; start slave; Query OK, 0 rows affected (0.09 sec) 检查是否成功 在从服务上执行如下操作，加长从服务器端 IO线程和 SQL 线程是否是 OK mysql&gt; show slave status\\G输出结果中应该看到 I/O 线程和 SQL 线程都是 YES, 就表示成功。 执行此过程后，在主服务上操作的修改数据的操作都会在从服务器中执行一遍，这样就保证了数据的一致性。 主服务器中无数据主服务器中设置 my.cnf配置文件 [mysqld] log-bin=/var/log/mysql/mysql-bin server-id=1创建日志目录并赋予权限 [root@mysql ~]# mkdir /var/log/mysql [root@mysql ~]# chown mysql.mysql /var/log/mysql重启服务 从服务器设置 my.cnf配置文件 [mysqld] server-id=3重启服务 查看主服务器的二进制日志的名称 通过使用命令行客户端连接到主服务器来启动主服务器上的会话，并通过执行以下 FLUSH TABLES WITH READ LOCK 语句来刷新所有表和阻止写语句： mysql&gt; FLUSH TABLES WITH READ LOCK; mysql&gt; show master status \\G ****************** 1. row **************** File: mysql-bin.000001 Position: 0 Binlog_Do_DB: Binlog_Ignore_DB: Executed_Gtid_Set: 1 row in set (0.00 sec) 在从服务器的 mysql 中执行如下语句 mysql&gt; CHANGE MASTER TO MASTER_HOST=&#39;mysql-master1&#39;, MASTER_USER=&#39;repl&#39;, MASTER_PASSWORD=&#39;123&#39;, MASTER_LOG_FILE=&#39;mysql-bin.000001&#39;, MASTER_LOG_POS=0; mysql&gt; start slave; 查看 在master上执行show binlog events命令，可以看到第一个binlog文件的内容。 mysql&gt; show binlog events\\G *************************** 1. row *************************** Log_name: mysql-bin.000001 Pos: 4 Event_type: Format_desc Server_id: 1 End_log_pos: 107 Info: Server ver: 5.5.28-0ubuntu0.12.10.2-log, Binlog ver: 4 *************************** 2. row *************************** Log_name: mysql-bin.000001 Pos: 107 Event_type: Query Server_id: 1 End_log_pos: 181 Info: create user rep *************************** 3. row *************************** Log_name: mysql-bin.000001 Pos: 181 Event_type: Query Server_id: 1 End_log_pos: 316 Info: grant replication slave on *.* to rep identified by &#39;123456&#39; 3 rows in set (0.00 sec) Log_name 是二进制日志文件的名称，一个事件不能横跨两个文件 Pos 这是该事件在文件中的开始位置 Event_type 事件的类型，事件类型是给slave传递信息的基本方法，每个新的binlog都以Format_desc类型开始，以Rotate类型结束 Server_id 创建该事件的服务器id End_log_pos 该事件的结束位置，也是下一个事件的开始位置，因此事件范围为Pos~End_log_pos - 1 Info 事件信息的可读文本，不同的事件有不同的信息 在从站上暂停复制您可以使用STOP SLAVE和 START SLAVE语句停止并启动从站上的复制 。 要停止从主服务器处理二进制日志，请使用 STOP SLAVE： mysql&gt; STOP SLAVE;当复制停止时，从I / O线程停止从主二进制日志读取事件并将它们写入中继日志，并且SQL线程停止从中继日志读取事件并执行它们。您可以通过指定线程类型单独暂停I / O或SQL线程： mysql&gt; STOP SLAVE IO_THREAD; mysql&gt; STOP SLAVE SQL_THREAD;要再次开始执行，请使用以下START SLAVE语句： mysql&gt; START SLAVE;要启动特定线程，请指定线程类型： mysql&gt; START SLAVE IO_THREAD; mysql&gt; START SLAVE SQL_THREAD;复制原理实现细节（了解） MySQL复制功能使用三个线程实现，一个在主服务器上，两个在从服务器上： Binlog转储线程 主设备创建一个线程，以便在从设备连接时将二进制日志内容发送到从设备。可以SHOW PROCESSLIST在主服务器的输出中将此线程标识为Binlog Dump线程。 二进制日志转储线程获取主机二进制日志上的锁，用于读取要发送到从机的每个事件。一旦读取了事件，即使在事件发送到从站之前，锁也会被释放。 从属 I/O线程 在从属服务器上发出 START SLAVE 语句时，从属服务器会创建一个 I/O 线程，该线程连接到主服务器并要求主服务器发送其在二进制日志中的更新记录。 从属 I/O线程读取主Binlog Dump线程发送的更新 （请参阅上一项）并将它们复制到包含从属中继日志的本地文件。 此线程的状态显示为 Slave_IO_running输出 SHOW SLAVE STATUS或 Slave_running输出中的状态SHOW STATUS。 从属SQL线程 从属设备创建一个SQL线程来读取由从属 I/O 线程写入的中继日志，并执行其中包含的事件。 当从属服务器从放的事件，追干上主服务器的事件后，从属服务器的 I/O 线程将会处于休眠状态，直到主服务器的事件有更新时，被主服务器发送的信号唤醒。 在前面的描述中，每个主/从连接有三个线程。具有多个从站的主站为每个当前连接的从站创建一个二进制日志转储线程，每个从站都有自己的I / O和SQL线程。 从站使用两个线程将读取更新与主站分开并将它们执行到独立任务中。因此，如果语句执行缓慢，则不会减慢读取语句的任务。例如，如果从服务器尚未运行一段时间，则当从服务器启动时，其I / O线程可以快速从主服务器获取所有二进制日志内容，即使SQL线程远远落后。如果从服务器在SQL线程执行了所有获取的语句之前停止，则I / O线程至少已获取所有内容，以便语句的安全副本本地存储在从属的中继日志中，准备在下次执行时执行奴隶开始。 配置Replication(gtid方式)基于事务的Replication，就是利用GTID来实现的复制 GTID（全局事务标示符）最初由google实现，在MySQL 5.6中引入.GTID在事务提交时生成，由UUID和事务ID组成.uuid会在第一次启动MySQL时生成，保存在数据目录下的auto .cnf文件里，事务ID则从1开始自增使用GTID的好处主要有两点： 不再需要指定传统复制中的master_log_files和master_log_pos，使主从复制更简单可靠 可以实现基于库的多线程复制，减小主从复制的延迟 实验环境要求： 5.7.6 以上版本 主库配置[mysqld] log-bin=/var/log/mysql/mysql-bin server-id=1 gtid_mode=ON enforce_gtid_consistency=1 # 强制执行GTID一致性。重启服务 其他和之前的一样 创建专属用户并授权 假如有数据导出数据 mysql&gt; CREATE USER &#39;repl&#39;@&#39;%&#39; IDENTIFIED BY &#39;123&#39;; mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &#39;repl&#39;@&#39;%&#39;; mysql&gt; 从库配置测试用户有效性 mysql -urepl -p&#39;123&#39; -hmysql-master1[mysqld] server-id=2 gtid_mode=ON enforce_gtid_consistency=1 # 可选项, 把连接到 master 的信息存到数据库中的表中 master-info-repository=TABLE relay-log-info-repository=TABLE重启服务 假如有数据，先导入数据 mysql&gt; source dump.dbMysql 终端执行连接信息 mysql&gt; CHANGE MASTER TO MASTER_HOST=&#39;172.16.153.10&#39;, MASTER_USER=&#39;repl&#39;, MASTER_PASSWORD=&#39;123&#39;, MASTER_AUTO_POSITION=1; &gt; start slave;Replication故障排除开启 GTID 后的导出导入数据的注意点 Warning: A partial dump from a server that has GTIDs will by default include the GTIDs of all transactions, even those that changed suppressed parts of the database. If you don’t want to restore GTIDs, pass –set-gtid-purged=OFF. To make a complete dump, pass –all-databases –triggers –routines –events 意思是： 当前数据库实例中开启了 GTID 功能, 在开启有 GTID 功能的数据库实例中, 导出其中任何一个库, 如果没有显示地指定–set-gtid-purged参数, 都会提示这一行信息. 意思是默认情况下, 导出的库中含有 GTID 信息, 如果不想导出包含有 GTID 信息的数据库, 需要显示地添加–set-gtid-purged=OFF参数. mysqldump -uroot -p --set-gtid-purged=OFF --all-databases &gt; alldb.db导入数据是就可以相往常一样导入了。 UUID一致，导致主从复制I/O线程不是yes Fatal error: The slave I/O thread stops because master and slave have equal MySQL server UUIDs; these UUIDs must be different for replication to work 致命错误：由于master和slave具有相同的mysql服务器uuid，从I/O线程将停止；这些uuid必须不同才能使复制工作。 问题提示主从使用了相同的server UUID，一个个的检查： 检查主从server_id 主库： mysql&gt; show variables like ‘server_id’;+—————+——-+| Variable_name | Value |+—————+——-+| server_id | 1 |+—————+——-+1 row in set (0.01 sec) 从库： mysql&gt; show variables like ‘server_id’;+—————+——-+| Variable_name | Value |+—————+——-+| server_id | 2 |+—————+——-+1 row in set (0.01 sec) server_id不一样，排除。 检查主从状态： 主库： mysql&gt; show master status;+——————+———-+————–+——————+——————-+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+——————+———-+————–+——————+——————-+| mysql-bin.000001 | 154 | | | |+——————+———-+————–+——————+——————-+1 row in set (0.00 sec)从库： mysql&gt; show master status;+——————+———-+————–+——————+——————-+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+——————+———-+————–+——————+——————-+| mysql-bin.000001 | 306 | | | |+——————+———-+————–+——————+——————-+1 row in set (0.00 sec) File一样，排除。 最后检查发现他们的auto.cnf中的server-uuid是一样的。。。 [root@localhost ~]# vim /var/lib/mysql/auto.cnf [auto] server-uuid=4f37a731-9b79-11e8-8013-000c29f0700f 修改uuid并重启服务 数据库中间MyCAT读写分离实现 (重要！！！)Mycat 是一个开源的分布式数据库系统，但是由于真正的数据库需要存储引擎，而 Mycat 并没有存 储引擎，所以并不是完全意义的分布式数据库系统。 那么 Mycat 是什么？Mycat 是数据库中间件，就是介于数据库与应用之间，进行数据处理与交互的中间服 务。 MyCAT 是使用 JAVA 语言进行编写开发，使用前需要先安装 JAVA 运行环境(JRE),由于 MyCAT 中使用了 JDK7 中的一些特性，所以要求必须在 JDK7 以上的版本上运行。 部署环境1下载JDK [root@mycat ~]# wget --no-cookies \\ --no-check-certificate \\ --header \\ &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; \\ http://download.oracle.com/otn-pub/java/jdk/8u181-\\ b13/96a7b8442fe848ef90c96a2fad6ed6d1/jdk-8u181-linux-\\ x64.tar.gz // --no-check-certificate 表示不校验SSL证书，因为中间的两个302会访问https，会涉及到证书的问题，不校验能快一点，影响不大.2.解压文件 [root@mycat ~]# tar -xf jdk-8u181-linux-x64.tar.gz -C /usr/local/ [root@mycat ~]# ln -s /usr/local/jdk1.8.0_181/ /usr/local/java3.配置环境变量 [root@mycat ~]# vim /etc/profile.d/java.sh export JAVA_HOME=/usr/local/java export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 使环境变量生效 [root@mycat ~]# source /etc/profile.d/java.sh部署Mycat 下载 [root@mycat ~]# wget http://dl.mycat.io/1.6.5/Mycat-server-1.6.5-release-20180122220033-linux.tar.gz 解压 [root@mycat ~]# tar xf Mycat-server-1.6.5-release-20180122220033-linux.tar.gz -C /usr/local/ 配置Mycat认识配置文件 MyCAT 目前主要通过配置文件的方式来定义逻辑库和相关配置: /usr/local/mycat/conf/server.xml 定义用户以及系统相关变量，如端口等。其中用户信息是前端应用程序连接 mycat 的用户信息。 /usr/local/mycat/conf/schema.xml 定义逻辑库，表、分片节点等内容。 配置 server.xml以下为代码片段 下面的用户和密码是应用程序连接到 MyCat 使用的，可以自定义配置 而其中的schemas 配置项所对应的值是逻辑数据库的名字，也可以自定义，但是这个名字需要和后面 schema.xml 文件中配置的一致。 vim server.xml &lt;!--下面的用户和密码是应用程序连接到 MyCat 使用的.schemas 配置项所对应的值是逻辑数据库的名字,这个名字需要和后面 schema.xml 文件中配置的一致。--&gt; &lt;user name=&quot;mycatdb&quot; defaultAccount=&quot;true&quot;&gt; &lt;property name=&quot;password&quot;&gt;1&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;mycat_db&lt;/property&gt; &lt;!-- 表级 DML 权限设置 --&gt; &lt;!-- &lt;privileges check=&quot;false&quot;&gt; &lt;schema name=&quot;TESTDB&quot; dml=&quot;0110&quot; &gt; &lt;table name=&quot;tb01&quot; dml=&quot;0000&quot;&gt;&lt;/table&gt; &lt;table name=&quot;tb02&quot; dml=&quot;1111&quot;&gt;&lt;/table&gt; &lt;/schema&gt; &lt;/privileges&gt; --&gt; &lt;/user&gt; &lt;!--下面是另一个用户，并且设置的访问 TESTED 逻辑数据库的权限是 只读 &lt;user name=&quot;mycatuser&quot;&gt; &lt;property name=&quot;password&quot;&gt;123&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;mycat_db&lt;/property&gt; &lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt; &lt;/user&gt; --&gt; &lt;/mycat:server&gt; == 上面的配置中，假如配置了用户访问的逻辑库，那么必须在 schema.xml 文件中也配置这个逻辑库，否则报错，启动 mycat 失败 == 配置schema.xml以下是配置文件中的每个部分的配置块儿 逻辑库和分表设置 &lt;schema name=&quot;mycat_db&quot; // 逻辑库名称,与server.xml的一致 checkSQLschema=&quot;false&quot; // 不检查 sqlMaxLimit=&quot;100&quot; // 最大连接数 dataNode=&quot;tiger1&quot;&gt; // 数据节点名称 &lt;!--这里定义的是分表的信息--&gt; &lt;/schema&gt; 数据节点 &lt;dataNode name=&quot;tiger1&quot; // 此数据节点的名称 dataHost=&quot;localhost1&quot; // 主机组 database=&quot;mycat_test&quot; /&gt; // 真实的数据库名称主机组 &lt;dataHost name=&quot;localhost1&quot; // 主机组 maxCon=&quot;1000&quot; minCon=&quot;10&quot; // 连接 balance=&quot;0&quot; // 负载均衡 writeType=&quot;0&quot; // 写模式配置 dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; // 数据库配置 switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;!--这里可以配置关于这个主机组的成员信息，和针对这些主机的健康检查语句--&gt; &lt;/dataHost&gt;balance 属性 负载均衡类型,目前的取值有 3 种: 1. balance=&quot;0&quot;, 不开启读写分离机制,所有读操作都发送到当前可用的 writeHost 上。 2. balance=&quot;1&quot;, 全部的 readHost 与 stand by writeHost 参与 select 语句的负载均衡,简单的说,当双主双从模式(M1-&gt;S1,M2-&gt;S2,并且 M1 与 M2 互为主备),正常情况下,M2,S1,S2 都参与 select 语句的负载均衡。 4. balance=&quot;2&quot;, 所有读操作都随机的在 writeHost、readhost 上分发。 5. balance=&quot;3&quot;, 所有读请求随机的分发到 wiriterHost 对应的 readhost 执行,writerHost 不负担读压力,注意 balance=3 只在 1.4 及其以后版本有,1.3 没有。 writeType 属性 负载均衡类型,目前的取值有 3 种: 1. writeType=&quot;0&quot;, 所有写操作发送到配置的第一个 writeHost,第一个挂了切到还生存的第二个writeHost,重新启动后已切换后的为准. 2. writeType=&quot;1&quot;,所有写操作都随机的发送到配置的 writeHost,1.5 以后废弃不推荐。健康检查 &lt;heartbeat&gt;select user()&lt;/heartbeat&gt;读写配置 &lt;writeHost host=&quot;hostM1&quot; url=&quot;192.168.19.176:3306&quot; user=&quot;root&quot; password=&quot;1&quot;&gt; &lt;!-- can have multi read hosts --&gt; &lt;readHost host=&quot;hostS2&quot; url=&quot;192.168.19.177:3306&quot; user=&quot;root&quot; password=&quot;1&quot; /&gt; &lt;/writeHost&gt;以下是组合为完整的配置文件，适用于一主一从的架构 &lt;?xml version=&quot;1.0&quot;?&gt; &lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt; &lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;schema name=&quot;mycat_db&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;tiger1&quot;&gt; &lt;!--这里定义的是分库分表的信息--&gt; &lt;/schema&gt; &lt;dataNode name=&quot;tiger1&quot; dataHost=&quot;localhost1&quot; database=&quot;mycat_test&quot; /&gt; &lt;dataHost name=&quot;localhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot; writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;!-- can have multi write hosts --&gt; &lt;writeHost host=&quot;hostM1&quot; url=&quot;192.168.19.176:3306&quot; user=&quot;root&quot; password=&quot;1&quot;&gt; &lt;!-- can have multi read hosts --&gt; &lt;readHost host=&quot;hostS2&quot; url=&quot;192.168.19.177:3306&quot; user=&quot;root&quot; password=&quot;1&quot; /&gt; &lt;/writeHost&gt; &lt;/dataHost&gt; &lt;/mycat:schema&gt;启动 mycat[root@mycat ~]# /usr/local/mycat/bin/mycat start 支持一下参数 start | restart |stop | status在真实的 master 数据库上给用户授权mysql&gt; grant all on mycat_test.* to root@&#39;%&#39; identified by &#39;1&#39;; mysql&gt; flush privileges;测试在 mycat 的机器上测试用户权限有效性 测试是否能正常登录上 主服务器 mysql -uroot -p&#39;123&#39; -h192.168.19.176继续测试是否能登录上从服务器 mysql -uroot -p&#39;123&#39; -h192.168.19.177通过客户端进行测试是否能登录到 mycat 上 192.168.19.178 是 mycat 的主机地址 注意端口号是 8066 [root@mysqlclient ~]# mysql -umycatdb -p1 -h192.168.19.178 -P8066 MySQL [(none)]&gt; show databases; +----------+ | DATABASE | +----------+ | mycat_db | +----------+ 1 row in set (0.00 sec)继续测试读写分离策略 使用 mysql 客户端工具使用 mycat 的账户和密码登录 mycat , 之后执行 select 语句。 之后查询 mycat 主机上 mycat 安装目录下的 logs/mycat.log 日志。 在日志重搜索查询的语句或者查询 从库的 ip 地址，应该能搜索到","categories":[{"name":"SQL","slug":"SQL","permalink":"https://cyylog.github.io/categories/SQL/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://cyylog.github.io/tags/Mysql/"},{"name":"Mycat","slug":"Mycat","permalink":"https://cyylog.github.io/tags/Mycat/"}]},{"title":"gitlab-ci","slug":"DevOPs/gitlab-ci入门","date":"2020-09-26T12:30:39.000Z","updated":"2020-10-01T14:49:40.342Z","comments":true,"path":"2020/09/26/devops/gitlab-ci-ru-men/","link":"","permalink":"https://cyylog.github.io/2020/09/26/devops/gitlab-ci-ru-men/","excerpt":"","text":"gitlab-ci入门示范：docker run -d --name gitlab-runner \\ -v /data/cyy/gitlab-runner:/etc/gitlab-runner \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /usr/bin/docker:/usr/bin/docker \\ -v /usr/lib64/libltdl.so.7:/usr/lib/x86_64-linux-gnu/libltdl.so.7 \\ gitlab/gitlab-runner 在宿主机上执行 ：chmod 666 /var/run/docker.sock # cat .gitlab-ci.yml stages: - test - build job1: stage: test script: - echo \"it is test\" job2: stage: build script: - echo \"it is build\" 各种参数说明script 由Runner执行的Shell脚本。 image 使用docker镜像， image：name service 使用docker services镜像, services：name before_script 执行作业前运行的脚本 after_script 作业完成后运行的脚本 stages 定义管道中的步骤，依次运行 stage 定义管道中步骤的作业段 only 指定作业限制only:refs，only:kubernetes，only:variables，和only:changes tags 指定执行作业的runner allow_failure 允许job失败 when 什么时候开始工作， on_success 只有当前一个阶段的所有工作都成功时（或者因为它们被标记而被认为是成功的allow_failure）才执行工作 。这是默认值。 on_failure 仅当前一阶段的至少一个作业失败时才执行作业。 always 无论先前阶段的工作状态如何，都可以执行工作。 manual 手动执行作业 delayed 延迟作业。后面跟start_in,start_in 30minutes(延迟30分钟)，不加单位，默认为秒。最长可延迟1小时。 environment 作业部署到的环境名称 #暂未搞清 cache key：\"$CI_JOB_STAGE-$CI_COMMIT_REF_SLUG\" #为每分支，每步骤启用缓存 artifacts job成功时附加到作业的文件或目录 dependencies 此job依赖其他jobz,主要作用于作业优先级 converage 给定作业代码覆盖率设置 retry 在发生故障时，可以自动重试作业的次数。 parallel 应该并行运行多少个作业实例 trigger 定义下游管道触发器 include 允许此作业包含外部YAML extends 此作业将继承的配置项 pages 上传作业结果用于gitlab pages variables 作业级别定义作业变量 案例 1 自动构建 go 程序stages: - test job1: stage: test script: - docker build -t mygo:v1 . tags: - go FROM golang:1.14.4-alpine3.12 RUN mkdir /src /app ADD . ../src ENV GOPROXY=\"https://goproxy.io\" RUN cd /src && ls && go build -o ../app/mygo main.go && cd /app && chmod +x mygo && cd / RUN rm src -fr WORKDIR /app ENTRYPOINT [\"/app/mygo\"] 压缩镜像FROM golang:1.14.4-alpine3.12 RUN mkdir /src /app ADD . ../src ENV GOPROXY=\"https://goproxy.io\" RUN cd /src && ls && go build -o ../app/mygo main.go && cd /app && chmod +x mygo && cd / FROM alpine:3.12 RUN mkdir /app COPY --from=0 /app/mygo /app ENTRYPOINT [\"/app/mygo\"] 单元测试FROM golang:1.14.4-alpine3.12 ADD . /src WORKDIR /src cmd [\"go\",\"test\"] stages: - test - build GoTest: stage: test script: - docker build -f DockerfileTest -t test-mygo:v1 . - docker run --rm test-mygo:v1 after_script: - docker rmi test-mygo:v1 tags: - go GoBuild: stage: build script: - docker build -t mygo:v1 . after_script: - docker rmi $(docker images -af \"dangling=true\" -q) tags: - go GoDeploy: #使用私有镜像仓库 stage: deploy script: - docker tag mygo:v1 10.0.0.169:5000/mygo:v1 - docker push 10.0.0.169:5000/mygo:v1 after_script: - docker rmi 10.0.0.169:5000/mygo:v1 - docker rmi mygo:v1 tags: - go 查看数据 curl http://10.0.0.169:5000/v2/_catalog (查看列表) curl http://10.0.0.169:5000/v2/mygo/manifests/v1 (查看redis镜像详情) curl http://10.0.0.169:5000/v2/mygo/tags/list 自动更新服务docker run -d --name gitlab-runner \\ -v /data/cyy/gitlab-runner:/etc/gitlab-runner \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /usr/bin/docker:/usr/bin/docker \\ -v /usr/lib64/libltdl.so.7:/usr/lib/x86_64-linux-gnu/libltdl.so.7 \\ -v /usr/local/bin/kubectl:/usr/local/bin/kubectl \\ -v /home/cyy/kubectl/config:/kubeconfig \\ -e KUBECONFIG=/kubeconfig \\ gitlab/gitlab-runner stages: - build - deploy - pub GoBuild: stage: build script: - docker build -t mygo:v1 . after_script: - docker rmi $(docker images -af \"dangling=true\" -q) tags: - go GoDeploy: stage: deploy script: - docker tag mygo:v1 10.0.0.169:5000/mygo:v1 - docker push 10.0.0.169:5000/mygo:v1 after_script: - docker rmi 10.0.0.169:5000/mygo:v1 - docker rmi mygo:v1 tags: - go GoPub: stage: pub script: - kubectl get pod -n myweb| grep mygo | awk '{print $1}' | xargs kubectl delete pod -n myweb tags: - go","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://cyylog.github.io/categories/DevOps/"}],"tags":[{"name":"Gitlab","slug":"Gitlab","permalink":"https://cyylog.github.io/tags/Gitlab/"}]},{"title":"搭建 NFS 服务","slug":"Linux/部署NFS服务","date":"2020-09-23T16:55:39.000Z","updated":"2020-09-24T12:57:54.644Z","comments":true,"path":"2020/09/24/linux/bu-shu-nfs-fu-wu/","link":"","permalink":"https://cyylog.github.io/2020/09/24/linux/bu-shu-nfs-fu-wu/","excerpt":"","text":"目的：部署 NFS 服务进行跨主机文件共享 Network File System 通过网络，让不同的机器、不同的操作系统可以共享彼此的文件。 1、环境准备1.系统版本：CentOS7.4 2.NFS版本：nfs-utils （使用当前最新版本） 3.初始化系统环境 4.关闭防火墙 [root@localhost ~]# systemctl stop iptables firewalld [root@localhost ~]# systemctl disable iptables firewalld 5.关闭SELinux [root@localhost ~]# sed -ri '/SELINUX=/cSELINUX=disabled' /etc/selinux/config [root@localhost ~]# setenforce 0 # 临时关闭SELinux 2、部署NFS 1、sudo yum -y install nfs-utils 2、配置 sudo vi /etc/sysconfig/nfs 加入 LOCKD_TCPPORT=30001 #TCP锁使用端口 LOCKD_UDPPORT=30002 #UDP锁使用端口 MOUNTD_PORT=30003 #挂载使用端口 STATD_PORT=30004 #状态使用端口 3、启动/重启服务1、sudo systemctl restart rpcbind.service 2、sudo systemctl restart nfs-server.service 开机启动： 1、sudo systemctl enable rpcbind.service 2、 sudo systemctl enable nfs-server.service 4、编辑共享目录编辑/etc/exports sudo vi /etc/exports 写入如下内容 /home/cyy/goapi 10.0.0.0/24(rw,async,insecure,no_root_squash) 参数说明 参数 作用 ro 只读 rw 读写 root_squash 当NFS客户端以root管理员访问时，映射为NFS服务器的匿名用户 no_root_squash 当NFS客户端以root管理员访问时，映射为NFS服务器的root管理员 all_squash 无论NFS客户端使用什么账户访问，均映射为NFS服务器的匿名用户 sync 同时将数据写入到内存与硬盘中，保证不丢失数据 async 优先将数据保存到内存，然后再写入硬盘;这样效率更高，但可能会丢失数据 secure（默认） 限制客户端只能从小于1024的tcp/ip端口连接服务器 insecure 允许客户端从大于1024的tcp/ip端口连接服务器 anonuid 匿名用户的UID值，通常是nobody或nfsnobody，可以在此处自行设定 anongid 匿名用户的GID值 no_subtree_check 如果NFS输出的是一个子目录，则无需检查其父目录的权限（可以提高效率） 5、查看挂载showmount -e localhost 发现没有 重启nfs服务 (sudo systemctl restart nfs-server.service) 接下来 创建刚才的文件夹 来到另外一台服务器上sudo yum -y install nfs-utils 不需要启动nfs服务 直接执行如下： showmount -e 10.0.0.145 尝试进行挂载 mount -t nfs 10.0.0.145:/home/cyy/goapi /home/cyy/goapi 卸载只需 umount /home/cyy/goapi","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"NFS","slug":"NFS","permalink":"https://cyylog.github.io/tags/NFS/"}]},{"title":"Mysql-游标","slug":"SQL/Mysql游标","date":"2020-07-16T03:54:04.000Z","updated":"2020-10-30T04:14:09.765Z","comments":true,"path":"2020/07/16/sql/mysql-you-biao/","link":"","permalink":"https://cyylog.github.io/2020/07/16/sql/mysql-you-biao/","excerpt":"","text":"[mysql游标的用法及作用]例子： 当前有三张表A、B、C其中A和B是一对多关系，B和C是一对多关系，现在需要将B中A表的主键存到C中；常规思路就是将B中查询出来然后通过一个update语句来更新C表就可以了，但是B表中有2000多条数据，难道要执行2000多次？显然是不现实的；最终找到写一个存储过程然后通过循环来更新C表，然而存储过程中的写法用的就是游标的形式。 【简介】​ 游标实际上是一种能从包括多条数据记录的结果集中每次提取一条记录的机制。 ​ 游标充当指针的作用。 ​ 尽管游标能遍历结果中的所有行，但他一次只指向一行。 ​ 游标的作用就是用于对查询数据库所返回的记录进行遍历，以便进行相应的操作。 【用法】​ 一、声明一个游标: declare 游标名称 CURSOR for table;(这里的table可以是你查询出来的任意集合)​ 二、打开定义的游标:open 游标名称;​ 三、获得下一行数据:FETCH 游标名称 into testrangeid,versionid;​ 四、需要执行的语句(增删改查):这里视具体情况而定​ 五、释放游标:CLOSE 游标名称; 注:mysql存储过程每一句后面必须用;结尾，使用的临时字段需要在定义游标之前进行声明。 【实例】- BEGIN --定义变量 declare testrangeid BIGINT; declare versionid BIGINT; declare done int; --创建游标，并存储数据 declare cur_test CURSOR for select id as testrangeid,version_id as versionid from tp_testrange; --游标中的内容执行完后将done设置为1 DECLARE CONTINUE HANDLER FOR NOT FOUND SET done=1; --打开游标 open cur_test; --执行循环 posLoop:LOOP --判断是否结束循环 IF done=1 THEN LEAVE posLoop; END IF; --取游标中的值 FETCH cur_test into testrangeid,versionid; --执行更新操作 update tp_data_execute set version_id=versionid where testrange_id = testrangeid; END LOOP posLoop; --释放游标 CLOSE cur_test; END - 例子2： 我们现在要用存储过程做一个功能，统计iphone的总库存是多少，并把总数输出到控制台。 --在windows系统中写存储过程时，如果需要使用declare声明变量，需要添加这个关键字，否则会报错。 delimiter // drop procedure if exists StatisticStore; CREATE PROCEDURE StatisticStore() BEGIN --创建接收游标数据的变量 declare c int; declare n varchar(20); --创建总数变量 declare total int default 0; --创建结束标志变量 declare done int default false; --创建游标 declare cur cursor for select name,count from store where name = 'iphone'; --指定游标循环结束时的返回值 declare continue HANDLER for not found set done = true; --设置初始值 set total = 0; --打开游标 open cur; --开始循环游标里的数据 read_loop:loop --根据游标当前指向的一条数据 fetch cur into n,c; --判断游标的循环是否结束 if done then leave read_loop; --跳出游标循环 end if; --获取一条数据时，将count值进行累加操作，这里可以做任意你想做的操作， set total = total + c; --结束游标循环 end loop; --关闭游标 close cur; --输出结果 select total; END; --调用存储过程 call StatisticStore(); fetch是获取游标当前指向的数据行，并将指针指向下一行，当游标已经指向最后一行时继续执行会造成游标溢出。使用loop循环游标时，他本身是不会监控是否到最后一条数据了，像下面代码这种写法，就会造成死循环； read_loop:loop fetch cur into n,c; set total = total+c; end loop; 在MySql中，造成游标溢出时会引发mysql预定义的NOT FOUND错误，所以在上面使用下面的代码指定了当引发not found错误时定义一个continue 的事件，指定这个事件发生时修改done变量的值。 declare continue HANDLER for not found set done = true; 所以在循环时加上了下面这句代码： --判断游标的循环是否结束 if done then leave read_loop; --跳出游标循环 end if; 如果done的值是true，就结束循环。继续执行下面的代码 使用方式 游标有三种使用方式：第一种就是上面的实现，使用loop循环；第二种方式如下，使用while循环： drop procedure if exists StatisticStore1; CREATE PROCEDURE StatisticStore1() BEGIN declare c int; declare n varchar(20); declare total int default 0; declare done int default false; declare cur cursor for select name,count from store where name = 'iphone'; declare continue HANDLER for not found set done = true; set total = 0; open cur; fetch cur into n,c; while(not done) do set total = total + c; fetch cur into n,c; end while; close cur; select total; END; call StatisticStore1(); 第三种方式是使用repeat执行： drop procedure if exists StatisticStore2; CREATE PROCEDURE StatisticStore2() BEGIN declare c int; declare n varchar(20); declare total int default 0; declare done int default false; declare cur cursor for select name,count from store where name = 'iphone'; declare continue HANDLER for not found set done = true; set total = 0; open cur; repeat fetch cur into n,c; if not done then set total = total + c; end if; until done end repeat; close cur; select total; END; call StatisticStore2(); 游标嵌套在mysql中，每个begin end 块都是一个独立的scope区域，由于MySql中同一个error的事件只能定义一次，如果多定义的话在编译时会提示Duplicate handler declared in the same block。 drop procedure if exists StatisticStore3; CREATE PROCEDURE StatisticStore3() BEGIN declare _n varchar(20); declare done int default false; declare cur cursor for select name from store group by name; declare continue HANDLER for not found set done = true; open cur; read_loop:loop fetch cur into _n; if done then leave read_loop; end if; begin declare c int; declare n varchar(20); declare total int default 0; declare done int default false; declare cur cursor for select name,count from store where name = 'iphone'; declare continue HANDLER for not found set done = true; set total = 0; open cur; iphone_loop:loop fetch cur into n,c; if done then leave iphone_loop; end if; set total = total + c; end loop; close cur; select _n,n,total; end; begin declare c int; declare n varchar(20); declare total int default 0; declare done int default false; declare cur cursor for select name,count from store where name = 'android'; declare continue HANDLER for not found set done = true; set total = 0; open cur; android_loop:loop fetch cur into n,c; if done then leave android_loop; end if; set total = total + c; end loop; close cur; select _n,n,total; end; begin end; end loop; close cur; END; call StatisticStore3(); 上面就是实现一个嵌套循环，当然这个例子比较牵强。凑合看看就行。 动态SQLMysql 支持动态SQL的功能 set @sqlStr='select * from table where condition1 = ?'; prepare s1 for @sqlStr; --如果有多个参数用逗号分隔 execute s1 using @condition1; --手工释放，或者是 connection 关闭时， server 自动回收 deallocate prepare s1;","categories":[{"name":"SQL","slug":"SQL","permalink":"https://cyylog.github.io/categories/SQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://cyylog.github.io/tags/MySQL/"}]},{"title":"Vue之入门篇_001","slug":"DevOPs/Vue入门之001","date":"2020-05-31T08:20:44.000Z","updated":"2020-09-30T20:11:01.238Z","comments":true,"path":"2020/05/31/devops/vue-ru-men-zhi-001/","link":"","permalink":"https://cyylog.github.io/2020/05/31/devops/vue-ru-men-zhi-001/","excerpt":"","text":"文档https://cli.vuejs.org/zh/ 以文档为主 安装https://cli.vuejs.org/zh/guide/installation.html 1. 开始 1、打开终端执行 npm install -g @vue/cli 接下来可以用vue -V 看版本，或where vue 看下装到哪了 2. 创建项目https://cli.vuejs.org/zh/guide/creating-a-project.html#vue-create 在你的当前文件夹下执行 vue create myvue 1、具体看文档操作（使用默认选项） 2、创建好后，会出现一个myvue文件夹，然后用webstorm打开它 运行和配置 首先运行在当前项目目录下执行 npm run serve 默认会启动webpack –devserver(仅仅开发使用),默认端口是 8080 配置进入https://cli.vuejs.org/zh/config/#vue-config-js 创建vue.config.js （注意和package.json同级、同级、同级） 写入如下内容 module.exports = { devServer: { port: 3000 } } 或者看https://cn.vuejs.org/v2/api/#%E5%AE%9E%E4%BE%8B%E5%B1%9E%E6%80%A7 new Vue({ data:{name:\"lisi\"} }).$mount('#app'); 使用templatenew Vue({ data:{ name:\"aa\" }, template:\"{{name}}\" } ).$mount('#app'); 使用渲染函数new Vue({ data:{ name:\"aa\" }, template:\"{{name}}\" } ).$mount('#app'); 已及 new Vue({ data:{ name:\"lisi\" }, render(c){ return c(\"h1\",{style:{color:'red'}},this.name) } } ).$mount('#app'); 路由入门1、安装https://router.vuejs.org/zh/installation.html 2、使用引导https://router.vuejs.org/zh/guide/#html 在项目目录下执行 npm install vue-router iview文档http://iview.talkingdata.com/#/components/guide/start 首先 搞一个 文件夹 ，执行 1、vue create myui 2、用webstorm 打开这个文件夹 3、npm install iview --save 安装iview框架 4 、 npm install --save vue-router 安装vue路由","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://cyylog.github.io/categories/DevOps/"}],"tags":[{"name":"Vue","slug":"Vue","permalink":"https://cyylog.github.io/tags/Vue/"}]},{"title":"python-fabric","slug":"DevOPs/python之fabric模块","date":"2020-05-31T08:20:44.000Z","updated":"2020-09-30T20:04:46.494Z","comments":true,"path":"2020/05/31/devops/python-zhi-fabric-mo-kuai/","link":"","permalink":"https://cyylog.github.io/2020/05/31/devops/python-zhi-fabric-mo-kuai/","excerpt":"","text":"python 之 fabric 模块Fabric 是一个用 Python 开发的部署工具，最大特点是不用登录远程服务器，在本地运行远程命令，几行 Python 脚本就可以轻松部署。 # doc http://docs.fabfile.org/en/2.5/getting-started.html # pip install fabric -i http://mirrors.aliyun.com/pypi/simple/G站部署脚本 参考 示范from fabric import Connection, task from fabric.api import env,hosts,run,execute @task def deploy(c): with Connection('root@x.x.x.x') as c: c.run(\"rm -rf giligili\") c.run(\"git clone https://github.com/bydmm/giligili.git\", pty=True) c.put(\"docker-compose.yml\", \"giligili/docker-compose.yml\") c.run(\"cd giligili &amp;&amp; docker-compose build &amp;&amp; docker-compose rm -fsv &amp;&amp; docker-compose up --build -d\", pty=True) c.run(\"sleep 15 &amp;&amp; docker logs -f gili-api\") # doc http://docs.fabfile.org/en/2.5/getting-started.html # apt install python-pip # pip install fabric -i http://mirrors.aliyun.com/pypi/simple/ # fab deploy 以上定义了pack和deploy两个任务，如果我们用Fabric部署，只需简单地输入两条命令： $ fab pack $ fab deployFabric提供几个简单的API来完成所有的部署，最常用的是local()和run()，分别在本地和远程执行命令，put()可以把本地文件上传到远程，当需要在远程指定当前目录时，只需用with cd(‘/path/to/dir/‘):即可。 默认情况下，当命令执行失败时，Fabric会停止执行后续命令。有时，我们允许忽略失败的命令继续执行，比如run(‘rm /tmp/abc’)在文件不存在的时候有可能失败，这时可以用with settings(warn_only=True):执行命令，这样Fabric只会打出警告信息而不会中断执行。 Fabric是如何在远程执行命令的呢？其实Fabric所有操作都是基于SSH执行的，必要时它会提示输入口令，所以非常安全。更好的办法是在指定的部署服务器上用证书配置无密码的ssh连接。 如果是基于团队开发，可以让Fabric利用版本库自动检出代码，自动执行测试、打包、部署的任务。由于Fabric运行的命令都是基本的Linux命令，所以根本不需要用Fabric本身来扩展，会敲Linux命令就能用Fabric部署。 利用Fabric部署Python、Ruby、PHP这样的非编译型网站应用非常方便，而对于编译型的Java、C#等就麻烦了，编译本身就是一个极其复杂的大工程，需要依赖特定工具或者IDE，很难做到自动化。 fab命令常用参数# fab --help 查看帮助 ## 常用参数 -l 显示定义好的任务函数名 -f 指定fab入口文件，默认入口文件名为fabfile.py.. 即指定fabfile文件 -g 指定网关（中转）设备，即HOST逗号分隔要操作的主机, 比如堡垒机环境，填写堡垒机IP即可. -H 指定目标主机，多台主机用‘,’号分隔 -p 远程账号的密码，fab执行时默认使用root账户 -P 以异步并行方式运行多主机任务，默认为串行运行 -R 指定role（角色），以角色名区分不同业务组设备 -t 设置设备连接超时时间（秒） -T 设置远程主机命令执行超时时间（秒） -w 当命令执行失败，发出警告，而非默认中止任务。 其他参数: --set=KEY=VALUE,... 逗号分隔，设置环境变量 --shortlist 简短打印可用命令 -c PATH 指定本地配置文件 -D 不加载用户known_hosts文件 -i PATH 指定私钥文件 -k 不加载来自~/.``ssh``下的私钥文件 --port=PORT 指定SSH连接端口 -R ROLES 根据角色操作，逗号分隔 -s SHELL 指定新shell，默认是``'/bin/bash -l -c' --show=LEVELS 以逗号分隔的输出 --ssh-config-path=PATH SSH配置文件路径 -T N 设置远程命令超时时间，单位秒 -u USER 连接远程主机用户名 -x HOSTS 以逗号分隔排除主机 -z INT 并发进程数 fabfile全局属性 (env对象)","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://cyylog.github.io/categories/DevOps/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://cyylog.github.io/tags/Python/"}]},{"title":"计划任务及日志管理","slug":"Linux/计划任务及日志管理","date":"2020-05-30T15:33:41.000Z","updated":"2020-05-30T15:34:52.084Z","comments":true,"path":"2020/05/30/linux/ji-hua-ren-wu-ji-ri-zhi-guan-li/","link":"","permalink":"https://cyylog.github.io/2020/05/30/linux/ji-hua-ren-wu-ji-ri-zhi-guan-li/","excerpt":"","text":"循环调度执行cron1.1简介cron crond的概念和crontab是不可分割的。crontab是一个命令，常见于Unix和类Unix的操作系统之中，用于设置周期性被执行的指令。该命令从标准输入设备读取指令，并将其存放于“crontab”文件中，以供之后读取和执行 1.2认识crond进程[root@JX01 ~]# systemctl status crond ● crond.service - Command Scheduler Loaded: loaded (/usr/lib/systemd/system/crond.service; enabled; vendor preset: enabled) Active: active (running) since 三 2018-08-01 07:03:23 CST; 17min ago Main PID: 671 (crond) CGroup: /system.slice/crond.service └─671 /usr/sbin/crond -n 8月 01 07:03:23 JX01 systemd[1]: Started Command Scheduler. 8月 01 07:03:23 JX01 systemd[1]: Starting Command Scheduler... 8月 01 07:03:23 JX01 crond[671]: (CRON) INFO (RANDOM_DELAY will be scaled with factor 2% if used.) 8月 01 07:03:23 JX01 crond[671]: (CRON) INFO (running with inotify support) [root@JX01 ~]# [root@JX01 ~]# ps aux | grep crond root 671 0.0 0.0 126280 1668 ? Ss 07:03 0:00 /usr/sbin/crond -n root 1440 0.0 0.0 112720 980 pts/0 R+ 07:21 0:00 grep --color=auto crond [root@JX01 ~]# 1.3创建计划任务#计划任务存储的位置 [root@JX01 ~]# ls /var/spool/cron/ root jack alice #管理计划任务的命令 crontab: -l Displays the current crontab on standard output. -r Removes the current crontab. -e Edits the current crontab using the editor specified. #计划任务书写的格式 .---------------- minute (0 - 59) | .-------------- hour (0 - 23) | | .------------ day of month (1 - 31) | | | .---------- month (1 - 12) OR jan,feb,mar,apr ... | | | | .-------- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat | | | | | * * * * * command #计划任务案例 00 02 * * * ls //每天2:00整 00 02 1 * * ls //每月1号2:00整 00 02 14 2 * ls //每年2月14号2:00整 00 02 * * 7 ls //每周日2:00整 00 02 * 6 5 ls //每年6月的周五2:00整（特殊） 00 02 14 * 7 ls //每月14号2:00整 或者 每周日2:00整，这两个时间都执行 00 02 14 2 7 ls //每年2月14号2:00整 或者 每周日2:00整，这两个时间都执行 00 02 * * * ls //每天2:00整 * 02 * * * ls //每天2:00中的每一分钟 * * * * * ls //每分钟执行ls * * 14 2 * ls //2月14号的每分钟 1440分钟 */5 * * * * ls //每隔5分钟 00 02 1,5,8 * * ls //每月1,5,8号的2:00整 00 02 1-8 * * ls //每月1到8号的2:00整 00 02 * 1-10 * ls #测试计划任务的执行效果 1 编写执行脚本. vim /crontab.sh touch /root/`date +%F-%X`.txt 2 编排任务计划 [root@localhost ~]# crontab -e * * 1 1 * bash /crontab.sh 3 修改日期时间为1月2日3点4分 date 01020304 修改时间为1点2分3秒 date -s 01:02:03 4 监控当前目录 watch -n 0.5 'ls /root/*.txt' 5 测试目标 * * * * 1 //每周1 每分钟会执行 * * * 1 * //1月每日 每分钟会执行 * * * 1 1 //1月的周1 每分钟会执行 * * 1 * * //每月1日 每分钟会执行 * * 1 * 1 //每月1日和每月周1 每分钟会执行 * * 1 1 * //1月1日 每分钟会执行 * * 1 1 1 //1月1日和1月的周1 每分钟都会执行 日志管理 日志：在现代社会里,为了维护自身系统资源的运行状况,计算机系统一般都会有相应的日志记录系统有关日常事件或者误操作警报的日期及时间戳信息。这些日志信息对计算机犯罪调查人员非常有用,但计算机日记是按正常工作状态记录的,所以冗余量很大,对查找与分析有用信息造成很大困难。 #Linux系统中存在的日志都在哪里？/var/log/ # tail /var/log/messages //系统主日志文件 # tail -20 /var/log/messages # tail -f /var/log/messages //动态查看日志文件的尾部 # tailf /var/log/secure //认证、安全 # tail /var/log/maillog //跟邮件postfix相关 prefix # tail /var/log/cron //crond、at进程产生的日志 # tail /var/log/dmesg //和系统启动相关 # tail /var/log/audit/audit.log //系统审计日志 # tail /var/log/yum.log //yum # tail /var/log/mysqld.log //MySQL # tail /var/log/xferlog //和访问FTP服务器相关 # tail /var/log/wtmp //当前登录的用户（命令：w） # tail /var/log/btmp //最近登录的用户（命令last） # tail /var/log/lastlog //所有用户的登录情况（命令lastlog） #Linux系统是什么进程程序在管理日志？rsyslog##rsyslog rsyslog：linux系统中管理日志的服务 所产生的进程是: rsyslogd -n linux中的配置文件: linux中所有的服务或者工具,都是由配置文件驱动工作的; Linux中的工具或服务都是遵循配置文件中的规则工作的; /etc/rsyslog.conf: 这个文件定义了系统中所有的服务或者工具,它们所产生的日志,根据特定的级别需要存储在特定的位置 日志等级: 等级由低到高：debug","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/tags/Linux/"}]},{"title":"Jenkins_流水线语法_002","slug":"DevOPs/Jenkins-流水线语法-002","date":"2020-05-30T04:43:37.000Z","updated":"2020-05-30T13:56:43.585Z","comments":true,"path":"2020/05/30/devops/jenkins-liu-shui-xian-yu-fa-002/","link":"","permalink":"https://cyylog.github.io/2020/05/30/devops/jenkins-liu-shui-xian-yu-fa-002/","excerpt":"","text":"参数parameters 指令提供了一个用户在触发流水线时应该提供的参数列表。这些用户指定参数的值可通过 params 对象提供给流水线步骤, 了解更多请参考示例。 Required No Parameters None Allowed Only once, inside the pipeline block. 可用参数 string 字符串类型的参数, 例如: parameters { string(name: &#39;DEPLOY_ENV&#39;, defaultValue: &#39;staging&#39;, description: &#39;&#39;) } booleanParam 布尔参数, 例如: parameters { booleanParam(name: &#39;DEBUG_BUILD&#39;, defaultValue: true, description: &#39;&#39;) } 示例Jenkinsfile (Declarative Pipeline) pipeline { agent any parameters { string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?') } stages { stage('Example') { steps { echo \"Hello ${params.PERSON}\" } } } } 一份完整的可用参数列表正在等待 INFRA-1503的完成。 触发器triggers 指令定义了流水线被重新触发的自动化方法。对于集成了源（ 比如 GitHub 或 BitBucket）的流水线, 可能不需要 triggers ，因为基于 web 的集成很肯能已经存在。 当前可用的触发器是 cron, pollSCM 和 upstream。 Required No Parameters None Allowed Only once, inside the pipeline block. cron 接收 cron 样式的字符串来定义要重新触发流水线的常规间隔 ,比如: triggers { cron(&#39;H */4 * * 1-5&#39;) } pollSCM 接收 cron 样式的字符串来定义一个固定的间隔，在这个间隔中，Jenkins 会检查新的源代码更新。如果存在更改, 流水线就会被重新触发。例如: triggers { pollSCM(&#39;H */4 * * 1-5&#39;) } upstream 接受逗号分隔的工作字符串和阈值。 当字符串中的任何作业以最小阈值结束时，流水线被重新触发。例如: triggers { upstream(upstreamProjects: &#39;job1,job2&#39;, threshold: hudson.model.Result.SUCCESS) } pollSCM 只在Jenkins 2.22 及以上版本中可用。 示例Jenkinsfile (Declarative Pipeline) pipeline { agent any triggers { cron('H */4 * * 1-5') } stages { stage('Example') { steps { echo 'Hello World' } } } } stagestage 指令在 stages 部分进行，应该包含一个 实际上, 流水巷所做的所有实际工作都将封装进一个或多个 stage 指令中。 Required At least one Parameters One mandatory parameter, a string for the name of the stage. Allowed Inside the stages section. 示例Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage('Example') { steps { echo 'Hello World' } } } } 工具定义自动安装和放置 PATH 的工具的一部分。如果 agent none 指定，则忽略该操作。 Required No Parameters None Allowed Inside the pipeline block or a stage block. 支持工具 maven jdk gradle 示例Jenkinsfile (Declarative Pipeline) pipeline { agent any tools { maven 'apache-maven-3.0.1' } stages { stage('Example') { steps { sh 'mvn --version' } } } } The tool name must be pre-configured in Jenkins under Manage Jenkins → Global Tool Configuration. 工具名称必须在Jenkins中的Manage Jenkins→全局工具配置下预先配置。 inputstage 的 input 指令允许你使用 input step提示输入。 在应用了 options 后，进入 stage 的 agent 或评估 when 条件前， stage 将暂停。 如果 input 被批准, stage 将会继续。 作为 input 提交的一部分的任何参数都将在环境中用于其他 stage。 配置项 message 必需的。 这将在用户提交 input 时呈现给用户。 id input 的可选标识符， 默认为 stage 名称。 ok input表单上的”ok” 按钮的可选文本。 submitter 可选的以逗号分隔的用户列表或允许提交 input 的外部组名。默认允许任何用户。 submitterParameter 环境变量的可选名称。如果存在，用 submitter 名称设置。 parameters 提示提交者提供的一个可选的参数列表。 更多信息参见 [parameters]。 示例Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage('Example') { input { message \"Should we continue?\" ok \"Yes, we should.\" submitter \"alice,bob\" parameters { string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?') } } steps { echo \"Hello, ${PERSON}, nice to meet you.\" } } } } whenwhen 指令允许流水线根据给定的条件决定是否应该执行阶段。 when 指令必须包含至少一个条件。 如果 when 指令包含多个条件, 所有的子条件必须返回True，阶段才能执行。 这与子条件在 allOf 条件下嵌套的情况相同 (参见下面的示例)。 使用诸如 not, allOf, 或 anyOf 的嵌套条件可以构建更复杂的条件结构 can be built 嵌套条件刻意潜逃到任意深度。 Required No Parameters None Allowed Inside a stage directive 内置条件 branch 当正在构建的分支与模式给定的分支匹配时，执行这个阶段, 例如: when { branch &#39;master&#39; }。注意，这只适用于多分支流水线。 environment 当指定的环境变量是给定的值时，执行这个步骤, 例如: when { environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; } expression 当指定的Groovy表达式评估为true时，执行这个阶段, 例如: when { expression { return params.DEBUG_BUILD } } not 当嵌套条件是错误时，执行这个阶段,必须包含一个条件，例如: when { not { branch &#39;master&#39; } } allOf 当所有的嵌套条件都正确时，执行这个阶段,必须包含至少一个条件，例如: when { allOf { branch &#39;master&#39;; environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; } } anyOf 当至少有一个嵌套条件为真时，执行这个阶段,必须包含至少一个条件，例如: when { anyOf { branch &#39;master&#39;; branch &#39;staging&#39; } } 在进入 stage 的 agent 前评估 when默认情况下, 如果定义了某个阶段的代理，在进入该stage 的 agent 后该 stage 的 when 条件将会被评估。但是, 可以通过在 when 块中指定 beforeAgent 选项来更改此选项。 如果 beforeAgent 被设置为 true, 那么就会首先对 when 条件进行评估 , 并且只有在 when 条件验证为真时才会进入 agent 。 示例Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage('Example Build') { steps { echo 'Hello World' } } stage('Example Deploy') { when { branch 'production' } steps { echo 'Deploying' } } } } Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage('Example Build') { steps { echo 'Hello World' } } stage('Example Deploy') { when { branch 'production' environment name: 'DEPLOY_TO', value: 'production' } steps { echo 'Deploying' } } } } Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage('Example Build') { steps { echo 'Hello World' } } stage('Example Deploy') { when { allOf { branch 'production' environment name: 'DEPLOY_TO', value: 'production' } } steps { echo 'Deploying' } } } } Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage('Example Build') { steps { echo 'Hello World' } } stage('Example Deploy') { when { branch 'production' anyOf { environment name: 'DEPLOY_TO', value: 'production' environment name: 'DEPLOY_TO', value: 'staging' } } steps { echo 'Deploying' } } } } Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage('Example Build') { steps { echo 'Hello World' } } stage('Example Deploy') { when { expression { BRANCH_NAME ==~ /(production|staging)/ } anyOf { environment name: 'DEPLOY_TO', value: 'production' environment name: 'DEPLOY_TO', value: 'staging' } } steps { echo 'Deploying' } } } } Jenkinsfile (Declarative Pipeline) pipeline { agent none stages { stage('Example Build') { steps { echo 'Hello World' } } stage('Example Deploy') { agent { label \"some-label\" } when { beforeAgent true branch 'production' } steps { echo 'Deploying' } } } } 并行声明式流水线的阶段可以在他们内部声明多隔嵌套阶段, 它们将并行执行。 注意，一个阶段必须只有一个 steps 或 parallel 的阶段。 嵌套阶段本身不能包含进一步的 parallel 阶段, 但是其他的阶段的行为与任何其他 stage 相同。任何包含 parallel 的阶段不能包含 agent 或 tools 阶段, 因为他们没有相关 steps。 另外, 通过添加 failFast true 到包含 parallel的 stage 中， 当其中一个进程失败时，你可以强制所有的 parallel 阶段都被终止。 示例Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage('Non-Parallel Stage') { steps { echo 'This stage will be executed first.' } } stage('Parallel Stage') { when { branch 'master' } failFast true parallel { stage('Branch A') { agent { label \"for-branch-a\" } steps { echo \"On Branch A\" } } stage('Branch B') { agent { label \"for-branch-b\" } steps { echo \"On Branch B\" } } } } } } 步骤声明式流水线可能使用在 流水线步骤引用中记录的所有可用的步骤, 它包含一个完整的步骤列表, 其中添加了下面列出的步骤，这些步骤只在声明式流水线中 only supported 。 脚本script 步骤需要 [scripted-pipeline]块并在声明式流水线中执行。 对于大多数用例来说,应该声明式流水线中的“脚本”步骤是不必要的， 但是它可以提供一个有用的”逃生出口”。 非平凡的规模和/或复杂性的 script 块应该被转移到 共享库 。 示例Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage('Example') { steps { echo 'Hello World' script { def browsers = ['chrome', 'firefox'] for (int i = 0; i &lt; browsers.size(); ++i) { echo \"Testing the ${browsers[i]} browser\" } } } } } } 脚本化流水线脚本化流水线, 与[declarative-pipeline]一样的是, 是建立在底层流水线的子系统上的。与声明式不同的是, 脚本化流水线实际上是由 Groovy构建的通用 DSL [2]。 Groovy 语言提供的大部分功能都可以用于脚本化流水线的用户。这意味着它是一个非常有表现力和灵活的工具，可以通过它编写持续交付流水线。 流控制脚本化流水线从 Jenkinsfile 的顶部开始向下串行执行, 就像 Groovy 或其他语言中的大多数传统脚本一样。 因此，提供流控制取决于 Groovy 表达式, 比如 if/else 条件, 例如: Jenkinsfile (Scripted Pipeline) node { stage('Example') { if (env.BRANCH_NAME == 'master') { echo 'I only execute on the master branch' } else { echo 'I execute elsewhere' } } } 另一种方法是使用Groovy的异常处理支持来管理脚本化流水线流控制。当 步骤 失败 ，无论什么原因，它们都会抛出一个异常。处理错误的行为必须使用Groovy中的 try/catch/finally 块 , 例如: Jenkinsfile (Scripted Pipeline) node { stage('Example') { try { sh 'exit 1' } catch (exc) { echo 'Something failed, I should sound the klaxons!' throw } } } 步骤正如 本章开始所讨论的, 流水线最基础的部分是”步骤”。从根本上说, 步骤告诉 Jenkins要做 what ，并作为声明式和脚本化流水线已发的基本构建块。 脚本化流水线 not 不引入任何特定于其语法的步骤; 流水线步骤引用 包括流水线和插件提供的步骤的完整列表。 区别普通 Groovy为了提供 durability, 这意味着运行流水线可以在Jenkins master 重启后继续运行，脚本化的流水线序列化数据到主服务器。由于这个设计需求, 一些Groovy 习惯用语，比如 collection.each { item -&gt; /* perform operation */ } 都不完全支持。详情参见 JENKINS-27421 和 JENKINS-26481。 语法比较当Jenkins 流水线第一次构建时, Groovy 被选为基础。 Jenkins长期使用嵌入式 Groovy引擎来为管理员和用户提供 高级脚本功能。另外, Jenkins流水线的实现者发现 Groovy是 构建现在成为 “脚本化流水线” DSL的坚实基础 [2]。 由于它是一个功能齐全的编程环境, 脚本化流水线为Jenkins用户提供了 大量的灵活性性和可扩展性。 Groovy学习曲线通常不适合给定团队的所有成员, 因此创造了声明式流水线来为编写Jenkins流水线提供一种更简单、更有主见的语法。 两者本质上是相同的流水线子系统。 underneath. 他们都是 “流水线即代码” 的持久实现。它们都能够使用构建到流水线中或插件提供的步骤。它们都能够使用 共享库 但是它们的区别在于语法和灵活性。 声明式限制了用户使用更严格和预定义的结构， 使其成为更简单的持续交付流水线的理想选择。 脚本化提供了很少的限制, 以至于对脚本和语法的唯一限制往往是由Groovy子集本身定义的，而不是任何特定于流水线的系统, 这使他成为权利用户和那些有更复杂需求的人的理想选择。 顾名思义, 声明式流水线鼓励 声明式编程模型。 [3] 而脚本化流水线遵循一个更命令式的编程模型 [4]","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://cyylog.github.io/categories/DevOps/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://cyylog.github.io/tags/Jenkins/"}]},{"title":"Jenkins_流水线语法_001","slug":"DevOPs/Jenkins-流水线语法-001","date":"2020-05-28T13:43:32.000Z","updated":"2020-05-30T13:54:45.129Z","comments":true,"path":"2020/05/28/devops/jenkins-liu-shui-xian-yu-fa-001/","link":"","permalink":"https://cyylog.github.io/2020/05/28/devops/jenkins-liu-shui-xian-yu-fa-001/","excerpt":"","text":"流水线语法本节是建立在 流水线入门内容的基础上，而且，应当被当作一个参考。 对于在实际示例中如何使用流水线语法的更多信息, 请参阅本章在流水线插件的2.5版本中的 使用 Jenkinsfile部分, 流水线支持两种离散的语法，具体如下对于每种的优缺点, 参见语法比较。 正如 本章开始讨论的, 流水线最基础的部分是 “步骤”。基本上, 步骤告诉 Jenkins 要做什么，以及作为声明式和脚本化流水线语法的基本构建块。 对于可用步骤的概述, 请参考 流水线步骤引用，它包含了一个构建到流水线的步骤和 插件提供的步骤的全面的列表。 声明式流水线声明式流水线是最近添加到 Jenkins 流水线的 [1]，它在流水线子系统之上提供了一种更简单，更有主见的语法。 所有有效的声明式流水线必须包含在一个 pipeline 块中, 比如: pipeline { /* insert Declarative Pipeline here */ }在声明式流水线中有效的基本语句和表达式遵循与 Groovy的语法同样的规则， 有以下例外: 流水线顶层必须是一个 block, 特别地: pipeline { } 没有分号作为语句分隔符，，每条语句都必须在自己的行上。 块只能由 节段, 指令, 步骤, 或赋值语句组成。 *属性引用语句被视为无参方法调用。 例如, input被视为 input() 节段声明式流水线中的节段通常包含一个或多个 指令 或 步骤。 代理agent 部分指定了整个流水线或特定的部分, 将会在Jenkins环境中执行的位置，这取决于 agent 区域的位置。该部分必须在 pipeline 块的顶层被定义, 但是 stage 级别的使用是可选的。 Required Yes Parameters Described below Allowed In the top-level pipeline block and each stage block. 参数为了支持作者可能有的各种各样的用例流水线, agent 部分支持一些不同类型的参数。这些参数应用在pipeline块的顶层, 或 stage 指令内部。 any 在任何可用的代理上执行流水线或阶段。例如: agent any none 当在 pipeline 块的顶部没有全局代理， 该参数将会被分配到整个流水线的运行中并且每个 stage 部分都需要包含他自己的 agent 部分。比如: agent none label 在提供了标签的 Jenkins 环境中可用的代理上执行流水线或阶段。 例如: agent { label &#39;my-defined-label&#39; } node agent { node { label &#39;labelName&#39; } } 和 agent { label &#39;labelName&#39; } 一样, 但是 node 允许额外的选项 (比如 customWorkspace )。 docker 使用给定的容器执行流水线或阶段。该容器将在预置的 node上，或在匹配可选定义的label 参数上，动态的供应来接受基于Docker的流水线。 docker 也可以选择的接受 args 参数，该参数可能包含直接传递到 docker run 调用的参数, 以及 alwaysPull 选项, 该选项强制 docker pull ，即使镜像名称已经存在。 比如: agent { docker { image &#39;maven:3-alpine&#39; label &#39;my-defined-label&#39; args &#39;-v /tmp:/tmp&#39; } } dockerfile 执行流水线或阶段, 使用从源代码库包含的 Dockerfile 构建的容器。为了使用该选项， Jenkinsfile 必须从多个分支流水线中加载, 或者加载 “Pipeline from SCM.” 通常，这是源代码仓库的根目录下的 Dockerfile : agent { dockerfile true }. 如果在另一个目录下构建 Dockerfile , 使用 dir 选项: agent { dockerfile {dir &#39;someSubDir&#39; } }。如果 Dockerfile 有另一个名称, 你可以使用 filename 选项指定该文件名。你可以传递额外的参数到 docker build ... 使用 additionalBuildArgs 选项提交, 比如 agent { dockerfile {additionalBuildArgs &#39;--build-arg foo=bar&#39; } }。 例如, 一个带有 build/Dockerfile.build 的仓库,期望一个构建参数 version: agent { // Equivalent to &quot;docker build -f Dockerfile.build --build-arg version=1.0.2 ./build/ dockerfile { filename &#39;Dockerfile.build&#39; dir &#39;build&#39; label &#39;my-defined-label&#39; additionalBuildArgs &#39;--build-arg version=1.0.2&#39; } } 常见选项有一些应用于两个或更多 agent 的实现的选项。他们不被要求，除非特别规定。 label 一个字符串。该标签用于运行流水线或个别的 stage。该选项对 node, docker 和 dockerfile 可用, node要求必须选择该选项。 customWorkspace 一个字符串。在自定义工作区运行应用了 agent 的流水线或个别的 stage, 而不是默认值。 它既可以是一个相对路径, 在这种情况下，自定义工作区会存在于节点工作区根目录下, 或者一个绝对路径。比如: agent { node { label &#39;my-defined-label&#39; customWorkspace &#39;/some/other/path&#39; } }该选项对 node, docker 和 dockerfile 有用 。 reuseNode 一个布尔值, 默认为false。 如果是true, 则在流水线的顶层指定的节点上运行该容器, 在同样的工作区, 而不是在一个全新的节点上。这个选项对 docker 和 dockerfile 有用, 并且只有当 使用在个别的 stage 的 agent 上才会有效。 示例Jenkinsfile (Declarative Pipeline) pipeline { agent { docker 'maven:3-alpine' } stages { stage('Example Build') { steps { sh 'mvn -B clean verify' } } } } 在一个给定名称和标签(maven:3-alpine)的新建的容器上执行定义在流水线中的所有步骤 。 阶段级别的 agent 部分Jenkinsfile (Declarative Pipeline) pipeline { agent none stages { stage('Example Build') { agent { docker 'maven:3-alpine' } steps { echo 'Hello, Maven' sh 'mvn --version' } } stage('Example Test') { agent { docker 'openjdk:8-jre' } steps { echo 'Hello, JDK' sh 'java -version' } } } } 在流水线顶层定义 agent none 确保 an Executor 没有被分配。 使用 agent none 也会强制 stage 部分包含他自己的 agent 部分。 使用镜像在一个新建的容器中执行该阶段的该步骤。 使用一个与之前阶段不同的镜像在一个新建的容器中执行该阶段的该步骤。 postpost 部分定义一个或多个steps ，这些阶段根据流水线或阶段的完成情况而 运行(取决于流水线中 post 部分的位置). post 支持以下 post-condition 块中的其中之一: always, changed, failure, success, unstable, 和 aborted。这些条件块允许在 post 部分的步骤的执行取决于流水线或阶段的完成状态。 Required No Parameters None Allowed In the top-level pipeline block and each stage block. Conditions always 无论流水线或阶段的完成状态如何，都允许在 post 部分运行该步骤。 changed 只有当前流水线或阶段的完成状态与它之前的运行不同时，才允许在 post 部分运行该步骤。 failure 只有当前流水线或阶段的完成状态为”failure”，才允许在 post 部分运行该步骤, 通常web UI是红色。 success 只有当前流水线或阶段的完成状态为”success”，才允许在 post 部分运行该步骤, 通常web UI是蓝色或绿色。 unstable 只有当前流水线或阶段的完成状态为”unstable”，才允许在 post 部分运行该步骤, 通常由于测试失败,代码违规等造成。通常web UI是黄色。 aborted 只有当前流水线或阶段的完成状态为”aborted”，才允许在 post 部分运行该步骤, 通常由于流水线被手动的aborted。通常web UI是灰色。 示例Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage('Example') { steps { echo 'Hello World' } } } post { always { echo 'I will always say Hello again!' } } } 按照惯例, post 部分应该放在流水线的底部。 Post-condition 块包含与 steps 部分相同的steps。 stages包含一系列一个或多个 stage 指令, stages 部分是流水线描述的大部分”work” 的位置。 建议 stages 至少包含一个 stage 指令用于连续交付过程的每个离散部分,比如构建, 测试, 和部署。 Required Yes Parameters None Allowed Only once, inside the pipeline block. 示例Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage('Example') { steps { echo 'Hello World' } } } } stages 部分通常会遵循诸如 agent, options 等的指令。 stepssteps 部分在给定的 stage 指令中执行的定义了一系列的一个或多个steps。 Required Yes Parameters None Allowed Inside each stage block. 示例Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage('Example') { steps { echo 'Hello World' } } } } steps 部分必须包含一个或多个步骤。 指令environmentenvironment 指令制定一个 键-值对序列，该序列将被定义为所有步骤的环境变量，或者是特定于阶段的步骤， 这取决于 environment 指令在流水线内的位置。 该指令支持一个特殊的助手方法 credentials() ，该方法可用于在Jenkins环境中通过标识符访问预定义的凭证。对于类型为 “Secret Text”的凭证, credentials() 将确保指定的环境变量包含秘密文本内容。对于类型为 “SStandard username and password”的凭证, 指定的环境变量指定为 username:password ，并且两个额外的环境变量将被自动定义 :分别为 MYVARNAME_USR 和 MYVARNAME_PSW 。 Required No Parameters None Allowed Inside the pipeline block, or within stage directives. 示例Jenkinsfile (Declarative Pipeline) pipeline { agent any environment { CC = 'clang' } stages { stage('Example') { environment { AN_ACCESS_KEY = credentials('my-prefined-secret-text') } steps { sh 'printenv' } } } } 顶层流水线块中使用的 environment 指令将适用于流水线中的所有步骤。 在一个 stage 中定义的 environment 指令只会将给定的环境变量应用于 stage 中的步骤。 environment 块有一个 助手方法 credentials() 定义，该方法可以在 Jenkins 环境中用于通过标识符访问预定义的凭证。 optionsoptions 指令允许从流水线内部配置特定于流水线的选项。 流水线提供了许多这样的选项, 比如 buildDiscarder,但也可以由插件提供, 比如 timestamps. Required No Parameters None Allowed Only once, inside the pipeline block. 可用选项 buildDiscarder 为最近的流水线运行的特定数量保存组件和控制台输出。例如: options { buildDiscarder(logRotator(numToKeepStr: &#39;1&#39;)) } disableConcurrentBuilds 不允许同时执行流水线。 可被用来防止同时访问共享资源等。 例如: options { disableConcurrentBuilds() } overrideIndexTriggers 允许覆盖分支索引触发器的默认处理。 如果分支索引触发器在多分支或组织标签中禁用, options { overrideIndexTriggers(true) } 将只允许它们用于促工作。否则, options { overrideIndexTriggers(false) } 只会禁用改作业的分支索引触发器。 skipDefaultCheckout 在agent 指令中，跳过从源代码控制中检出代码的默认情况。例如: options { skipDefaultCheckout() } skipStagesAfterUnstable 一旦构建状态变得UNSTABLE，跳过该阶段。例如: options { skipStagesAfterUnstable() } checkoutToSubdirectory 在工作空间的子目录中自动地执行源代码控制检出。例如: options { checkoutToSubdirectory(&#39;foo&#39;) } timeout 设置流水线运行的超时时间, 在此之后，Jenkins将中止流水线。例如: options { timeout(time: 1, unit: &#39;HOURS&#39;) } retry 在失败时, 重新尝试整个流水线的指定次数。 For example: options { retry(3) } timestamps 预谋所有由流水线生成的控制台输出，与该流水线发出的时间一致。 例如: options { timestamps() } ExampleJenkinsfile (Declarative Pipeline) pipeline { agent any options { timeout(time: 1, unit: 'HOURS') } stages { stage('Example') { steps { echo 'Hello World' } } } } 指定一个小时的全局执行超时, 在此之后，Jenkins 将中止流水线运行。 一个完整的可用选项列表正在等待完成第 INFRA-1503次。 阶段选项stage 的 options 指令类似于流水线根目录上的 options 指令。然而， stage -级别 options 只能包括 retry, timeout, 或 timestamps 等步骤, 或与 stage 相关的声明式选项，如 skipDefaultCheckout。 在stage, options 指令中的步骤在进入 agent 之前被调用或在 when 条件出现时进行检查。 可选的阶段选项 skipDefaultCheckout 在 agent 指令中跳过默认的从源代码控制中检出代码。例如: options { skipDefaultCheckout() } timeout 设置此阶段的超时时间, 在此之后， Jenkins 会终止该阶段。 例如: options { timeout(time: 1, unit: &#39;HOURS&#39;) } retry 在失败时, 重试此阶段指定次数。 例如: options { retry(3) } timestamps 预谋此阶段生成的所有控制台输出以及该行发出的时间一致。例如: options { timestamps() } 示例Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage('Example') { options { timeout(time: 1, unit: 'HOURS') } steps { echo 'Hello World' } } } } 指定 Example 阶段的执行超时时间, 在此之后，Jenkins 将中止流水线运行。","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://cyylog.github.io/categories/DevOps/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://cyylog.github.io/tags/Jenkins/"}]},{"title":"Prometheus-入门","slug":"监控/Prometheus/Prometheus-入门","date":"2020-05-25T17:48:56.000Z","updated":"2020-10-30T04:10:58.589Z","comments":true,"path":"2020/05/26/jian-kong/prometheus/prometheus-ru-men/","link":"","permalink":"https://cyylog.github.io/2020/05/26/jian-kong/prometheus/prometheus-ru-men/","excerpt":"","text":"Prometheus搭建简单的监控告警系统说明 使用的操作系统为CentOS 7.6.1810，其他系统请自己根据差异做对应调整 仅用于记录部署过程 告警通知对接了邮件和钉钉机器人 安装 在监控告警系统主机上安装prometheus、alertmanager和grafana 被监控的主机安装各种各样的exporter，每个exporter只监控自身的服务状态 环境准备# 创建prometheus配置目录 mkdir -p /etc/prometheus /etc/prometheus/rules.d # 创建prometheus用户 useradd prometheus # 修改属主属组 chown -R prometheus:prometheus /etc/prometheus软件包prometheusprometheus-v2.9.2 # 下载解压 wget -O - https://github.com/prometheus/prometheus/releases/download/v2.9.2/prometheus-2.9.2.linux-amd64.tar.gz | tar xz # 创建数据目录 mkdir -p /var/lib/prometheus chwon -R prometheus:prometheus /var/lib/prometheus # 拷贝文件到对应位置 cd prometheus-2.9.2.linux-amd64 chown -R prometheus:prometheus prometheus promtool console_libraries consoles prometheus.yml mv prometheus promtool /usr/local/bin/ mv console_libraries consoles prometheus.yml /etc/prometheus/alertmanageralertmanager-v0.17.0 # 下载解压 wget -O - https://github.com/prometheus/alertmanager/releases/download/v0.17.0/alertmanager-0.17.0.linux-amd64.tar.gz | tar xz # 创建数据目录 mkdir -p /var/lib/prometheus chwon -R prometheus:prometheus /var/lib/prometheus # 拷贝文件 cd alertmanager-0.17.0.linux-amd64 chown -R prometheus:prometheus alertmanager alertmanager.yml amtool mv alertmanager amtool /usr/local/bin/ mv alertmanager.yml /etc/prometheusnode_exporternode_exporter-v0.18.0 # 下载解压 wget -O - https://github.com/prometheus/node_exporter/releases/download/v0.18.0/node_exporter-0.18.0.linux-amd64.tar.gz | tar xz cd node_exporter-0.18.0.linux-amd64 chown prometheus:prometheus node_exporter # 拷贝文件 mv node_exporter /usr/local/bin/mysqld_exportermysqld_exporter-v0.11.0 # 下载解压 wget -O - https://github.com/prometheus/mysqld_exporter/releases/download/v0.11.0/mysqld_exporter-0.11.0.linux-amd64.tar.gz | tar xz cd mysqld_exporter-0.11.0.linux-amd64 chown prometheus:prometheus mysqld_exporter # 拷贝文件 mv mysqld_exporter /usr/local/bin/postgresql_exporter# 下载解压 wget -O - https://github.com/wrouesnel/postgres_exporter/releases/download/v0.4.7/postgres_exporter_v0.4.7_linux-amd64.tar.gz | tar xz cd postgres_exporter_v0.4.7_linux-amd64 chown prometheus:prometheus postgres_exporter # 拷贝文件 mv postgres_exporter /usr/local/bin/blackbox_exporterblackbox_exporter-v0.14.0 # 下载解压 wget -O - https://github.com/prometheus/blackbox_exporter/releases/download/v0.14.0/blackbox_exporter-0.14.0.linux-amd64.tar.gz | tar xz cd blackbox_exporter-0.14.0.linux-amd64 chown prometheus:prometheus blackbox_exporter blackbox.yml # 拷贝文件 mv blackbox_exporter /usr/local/bin/ mv blackbox.yml /etc/prometheus/grafanagrafana-v6.1.6 # 下载 wget https://dl.grafana.com/oss/release/grafana-6.1.6-1.x86_64.rpm yum localinstall grafana-6.1.6-1.x86_64.rpm配置Prometheus/etc/prometheus/prometheus.yml global: scrape_interval: 15s scrape_timeout: 10s evaluation_interval: 15s alerting: alertmanagers: - static_configs: - targets: - localhost:9093 scheme: http timeout: 10s rule_files: - /etc/prometheus/rules.d/*.rules scrape_configs: - job_name: prometheus honor_timestamps: true scrape_interval: 5s scrape_timeout: 5s metrics_path: /metrics scheme: http static_configs: - targets: - localhost:9090 - job_name: node-exporter honor_timestamps: true scrape_interval: 5s scrape_timeout: 5s metrics_path: /metrics scheme: http static_configs: - targets: - localhost:9100 - job_name: mysqld-exporter honor_timestamps: true scrape_interval: 5s scrape_timeout: 5s metrics_path: /metrics scheme: http static_configs: - targets: - localhost:9104 - job_name: postgresql-exporter honor_timestamps: true scrape_interval: 5s scrape_timeout: 5s metrics_path: /metrics scheme: http static_configs: - targets: - localhost:9187 /etc/prometheus/rules.d/host-status.rules groups: - name: host-status-rule rules: - alert: NodeFilesystemSpaceUsage expr: ( 1 - (node_filesystem_avail_bytes{fstype=~&quot;ext[234]|btrfs|xfs|zfs&quot;} / node_filesystem_size_bytes{fstype=~&quot;ext[234]|btrfs|xfs|zfs&quot;}) ) * 100 &gt; 85 for: 1m labels: team: node annotations: summary: &quot;{{$labels.instance}}: 文件系统空间使用率过高&quot; description: &quot;{{$labels.instance}}: 文件系统空间使用率超过 85% (当前使用率: {{ $value }})&quot; - alert: NodeFilesystemInodeUsage expr: ( 1 - (node_filesystem_files_free{fstype=~&quot;ext[234]|btrfs|xfs|zfs&quot;} / node_filesystem_files{fstype=~&quot;ext[234]|btrfs|xfs|zfs&quot;}) ) * 100 &gt; 80 for: 1m labels: team: node annotations: summary: &quot;{{$labels.instance}}: 文件系统inode使用率过高&quot; description: &quot;{{$labels.instance}}: 文件系统inode使用率超过 80% (当前使用率: {{ $value }})&quot; - alert: NodeFilesystemReadOnly expr: node_filesystem_readonly{job=&quot;node-exporter&quot;,device!~&#39;rootfs&#39;} == 1 for: 1m labels: team: node annotations: summary: &quot;{{$labels.instance}}: 文件系统只读状态&quot; description: &quot;{{$labels.instance}}: 文件系统只读状态&quot; - alert: NodeMemoryUsage expr: (node_memory_MemTotal - (node_memory_MemFree+node_memory_Buffers+node_memory_Cached )) / node_memory_MemTotal * 100 &gt; 80 for: 1m labels: team: node annotations: summary: &quot;{{$labels.instance}}: 内存使用率过高&quot; description: &quot;{{$labels.instance}}: 内存使用率过高超过80% (当前使用率: {{ $value }})&quot; - alert: NodeCPUUsage expr: (100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=&#39;idle&#39;,job=&quot;node-exporter&quot;}[1m])) * 100)) &gt; 80 for: 1m labels: team: node annotations: summary: &quot;{{$labels.instance}}: CPU使用率过高&quot; description: &quot;{{$labels.instance}}: CPU使用率超过80% (当前使用率: {{ $value }})&quot; /etc/prometheus/rules.d/mysql-status.rules groups: - name: MySQLStatsAlert rules: - alert: MySQL is down expr: mysql_up == 0 for: 1m labels: severity: critical annotations: summary: &quot;Instance {{ $labels.instance }} MySQL is down&quot; description: &quot;MySQL database is down. This requires immediate action!&quot; - alert: open files high expr: mysql_global_status_innodb_num_open_files &gt; (mysql_global_variables_open_files_limit) * 0.75 for: 1m labels: severity: warning annotations: summary: &quot;Instance {{ $labels.instance }} open files high&quot; description: &quot;Open files is high. Please consider increasing open_files_limit.&quot; - alert: Read buffer size is bigger than max. allowed packet size expr: mysql_global_variables_read_buffer_size &gt; mysql_global_variables_slave_max_allowed_packet for: 1m labels: severity: warning annotations: summary: &quot;Instance {{ $labels.instance }} Read buffer size is bigger than max. allowed packet size&quot; description: &quot;Read buffer size (read_buffer_size) is bigger than max. allowed packet size (max_allowed_packet).This can break your replication.&quot; - alert: Sort buffer possibly missconfigured expr: mysql_global_variables_innodb_sort_buffer_size &lt;256*1024 or mysql_global_variables_read_buffer_size &gt; 4*1024*1024 for: 1m labels: severity: warning annotations: summary: &quot;Instance {{ $labels.instance }} Sort buffer possibly missconfigured&quot; description: &quot;Sort buffer size is either too big or too small. A good value for sort_buffer_size is between 256k and 4M.&quot; - alert: Thread stack size is too small expr: mysql_global_variables_thread_stack &lt;196608 for: 1m labels: severity: warning annotations: summary: &quot;Instance {{ $labels.instance }} Thread stack size is too small&quot; description: &quot;Thread stack size is too small. This can cause problems when you use Stored Language constructs for example. A typical is 256k for thread_stack_size.&quot; - alert: Used more than 80% of max connections limited expr: mysql_global_status_max_used_connections &gt; mysql_global_variables_max_connections * 0.8 for: 1m labels: severity: warning annotations: summary: &quot;Instance {{ $labels.instance }} Used more than 80% of max connections limited&quot; description: &quot;Used more than 80% of max connections limited&quot; - alert: InnoDB Force Recovery is enabled expr: mysql_global_variables_innodb_force_recovery != 0 for: 1m labels: severity: warning annotations: summary: &quot;Instance {{ $labels.instance }} InnoDB Force Recovery is enabled&quot; description: &quot;InnoDB Force Recovery is enabled. This mode should be used for data recovery purposes only. It prohibits writing to the data.&quot; - alert: InnoDB Log File size is too small expr: mysql_global_variables_innodb_log_file_size &lt; 16777216 for: 1m labels: severity: warning annotations: summary: &quot;Instance {{ $labels.instance }} InnoDB Log File size is too small&quot; description: &quot;The InnoDB Log File size is possibly too small. Choosing a small InnoDB Log File size can have significant performance impacts.&quot; - alert: InnoDB Flush Log at Transaction Commit expr: mysql_global_variables_innodb_flush_log_at_trx_commit != 1 for: 1m labels: severity: warning annotations: summary: &quot;Instance {{ $labels.instance }} InnoDB Flush Log at Transaction Commit&quot; description: &quot;InnoDB Flush Log at Transaction Commit is set to a values != 1. This can lead to a loss of commited transactions in case of a power failure.&quot; - alert: Table definition cache too small expr: mysql_global_status_open_table_definitions &gt; mysql_global_variables_table_definition_cache for: 1m labels: severity: page annotations: summary: &quot;Instance {{ $labels.instance }} Table definition cache too small&quot; description: &quot;Your Table Definition Cache is possibly too small. If it is much too small this can have significant performance impacts!&quot; - alert: Table open cache too small expr: mysql_global_status_open_tables &gt;mysql_global_variables_table_open_cache * 99/100 for: 1m labels: severity: page annotations: summary: &quot;Instance {{ $labels.instance }} Table open cache too small&quot; description: &quot;Your Table Open Cache is possibly too small (old name Table Cache). If it is much too small this can have significant performance impacts!&quot; - alert: Thread stack size is possibly too small expr: mysql_global_variables_thread_stack &lt; 262144 for: 1m labels: severity: page annotations: summary: &quot;Instance {{ $labels.instance }} Thread stack size is possibly too small&quot; description: &quot;Thread stack size is possibly too small. This can cause problems when you use Stored Language constructs for example. A typical is 256k for thread_stack_size.&quot; - alert: InnoDB Plugin is enabled expr: mysql_global_variables_ignore_builtin_innodb == 1 for: 1m labels: severity: page annotations: summary: &quot;Instance {{ $labels.instance }} InnoDB Plugin is enabled&quot; description: &quot;InnoDB Plugin is enabled&quot; - alert: Binary Log is disabled expr: mysql_global_variables_log_bin != 1 for: 1m labels: severity: warning annotations: summary: &quot;Instance {{ $labels.instance }} Binary Log is disabled&quot; description: &quot;Binary Log is disabled. This prohibits you to do Point in Time Recovery (PiTR).&quot; - alert: Binlog Cache size too small expr: mysql_global_variables_binlog_cache_size &lt; 1048576 for: 1m labels: severity: page annotations: summary: &quot;Instance {{ $labels.instance }} Binlog Cache size too small&quot; description: &quot;Binlog Cache size is possibly to small. A value of 1 Mbyte or higher is OK.&quot; - alert: Binlog Transaction Cache size too small expr: mysql_global_variables_binlog_cache_size &lt; 1048576 for: 1m labels: severity: page annotations: summary: &quot;Instance {{ $labels.instance }} Binlog Transaction Cache size too small&quot; description: &quot;Binlog Transaction Cache size is possibly to small. A value of 1 Mbyte or higher is typically OK.&quot; /etc/prometheus/rules.d/postgresql-status.rules groups: - name: PostgreSQL-Status-Alert rules: ########## EXPORTER RULES ########## - alert: PGExporterScrapeError expr: pg_exporter_last_scrape_error &gt; 0 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: summary: &#39;Postgres Exporter running on {{ $labels.job }} (instance: {{ $labels.instance }}) is encountering scrape errors processing queries. Error count: ( {{ $value }} )&#39; - alert: NodeExporterScrapeError expr: node_textfile_scrape_error &gt; 0 for: 60s labels: service: system severity: critical severity_num: 300 annotations: summary: &#39;Node Exporter running on {{ $labels.job }} (instance: {{ $labels.instance }}) is encountering scrape errors processing custom metrics. Error count: ( {{ $value }} )&#39; ########## POSTGRESQL RULES ########## - alert: PGIsUp expr: pg_up &lt; 1 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: summary: &#39;postgres_exporter running on {{ $labels.job }} is unable to communicate with the configured database&#39; # Whether a system switches from primary to replica or vice versa must be configured per named job. # No way to tell what value a system is supposed to be without a rule expression for that specific system # 2 to 1 means it changed from primary to replica. 1 to 2 means it changed from replica to primary # Set this alert for each system that you want to monitor a recovery status change # Below is an example for a target job called &quot;Replica&quot; and watches for the value to change above 1 which means it&#39;s no longer a replica # # - alert: PGRecoveryStatusSwitch_Replica # expr: ccp_is_in_recovery_status{job=&quot;Replica&quot;} &gt; 1 # for: 60s # labels: # service: postgresql # severity: critical # severity_num: 300 # annotations: # summary: &#39;{{ $labels.job }} has changed from replica to primary&#39; # Absence alerts must be configured per named job, otherwise there&#39;s no way to know which job is down # Below is an example for a target job called &quot;Prod&quot; # - alert: PGConnectionAbsent # expr: absent(ccp_connection_stats_max_connections{job=&quot;Prod&quot;}) # for: 10s # labels: # service: postgresql # severity: critical # severity_num: 300 # annotations: # description: &#39;Connection metric is absent from target (Prod). Check that postgres_exporter can connect to PostgreSQL.&#39; - alert: PGIdleTxn expr: ccp_connection_stats_max_idle_in_txn_time &gt; 300 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: &#39;{{ $labels.job }} has at least one session idle in transaction for over 5 minutes.&#39; summary: &#39;PGSQL Instance idle transactions&#39; - alert: PGIdleTxn expr: ccp_connection_stats_max_idle_in_txn_time &gt; 900 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: &#39;{{ $labels.job }} has at least one session idle in transaction for over 15 minutes.&#39; summary: &#39;PGSQL Instance idle transactions&#39; - alert: PGQueryTime expr: ccp_connection_stats_max_query_time &gt; 43200 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: &#39;{{ $labels.job }} has at least one query running for over 12 hours.&#39; summary: &#39;PGSQL Max Query Runtime&#39; - alert: PGQueryTime expr: ccp_connection_stats_max_query_time &gt; 86400 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: &#39;{{ $labels.job }} has at least one query running for over 1 day.&#39; summary: &#39;PGSQL Max Query Runtime&#39; - alert: PGConnPerc expr: 100 * (ccp_connection_stats_total / ccp_connection_stats_max_connections) &gt; 75 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: &#39;{{ $labels.job }} is using 75% or more of available connections ({{ $value }}%)&#39; summary: &#39;PGSQL Instance connections&#39; - alert: PGConnPerc expr: 100 * (ccp_connection_stats_total / ccp_connection_stats_max_connections) &gt; 90 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: &#39;{{ $labels.job }} is using 90% or more of available connections ({{ $value }}%)&#39; summary: &#39;PGSQL Instance connections&#39; - alert: PGDBSize expr: ccp_database_size &gt; 1.073741824e+11 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: &#39;PGSQL Instance {{ $labels.job }} over 100GB in size: {{ $value }} bytes&#39; summary: &#39;PGSQL Instance size warning&#39; - alert: PGDBSize expr: ccp_database_size &gt; 2.68435456e+11 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: &#39;PGSQL Instance {{ $labels.job }} over 250GB in size: {{ $value }} bytes&#39; summary: &#39;PGSQL Instance size critical&#39; - alert: PGReplicationByteLag expr: ccp_replication_status_byte_lag &gt; 5.24288e+07 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: &#39;PGSQL Instance {{ $labels.job }} has at least one replica lagging over 50MB behind.&#39; summary: &#39;PGSQL Instance replica lag warning&#39; - alert: PGReplicationByteLag expr: ccp_replication_status_byte_lag &gt; 1.048576e+08 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: &#39;PGSQL Instance {{ $labels.job }} has at least one replica lagging over 100MB behind.&#39; summary: &#39;PGSQL Instance replica lag warning&#39; - alert: PGReplicationSlotsInactive expr: ccp_replication_slots_active == 0 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: &#39;PGSQL Instance {{ $labels.job }} has one or more inactive replication slots&#39; summary: &#39;PGSQL Instance inactive replication slot&#39; - alert: PGXIDWraparound expr: ccp_transaction_wraparound_percent_towards_wraparound &gt; 50 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: &#39;PGSQL Instance {{ $labels.job }} is over 50% towards transaction id wraparound.&#39; summary: &#39;PGSQL Instance {{ $labels.job }} transaction id wraparound imminent&#39; - alert: PGXIDWraparound expr: ccp_transaction_wraparound_percent_towards_wraparound &gt; 75 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: &#39;PGSQL Instance {{ $labels.job }} is over 75% towards transaction id wraparound.&#39; summary: &#39;PGSQL Instance transaction id wraparound imminent&#39; - alert: PGEmergencyVacuum expr: ccp_transaction_wraparound_percent_towards_emergency_autovac &gt; 75 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: &#39;PGSQL Instance {{ $labels.job }} is over 75% towards emergency autovacuum processes beginning&#39; summary: &#39;PGSQL Instance emergency vacuum imminent&#39; - alert: PGEmergencyVacuum expr: ccp_transaction_wraparound_percent_towards_emergency_autovac &gt; 90 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: &#39;PGSQL Instance {{ $labels.job }} is over 90% towards emergency autovacuum processes beginning&#39; summary: &#39;PGSQL Instance emergency vacuum imminent&#39; - alert: PGArchiveCommandStatus expr: ccp_archive_command_status_seconds_since_last_fail &gt; 300 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: &#39;PGSQL Instance {{ $labels.job }} has a recent failing archive command&#39; summary: &#39;Seconds since the last recorded failure of the archive_command&#39; - alert: PGSequenceExhaustion expr: ccp_sequence_exhaustion_count &gt; 0 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: &#39;Count of sequences on instance {{ $labels.job }} at over 75% usage: {{ $value }}. Run following query to see full sequence status: SELECT * FROM monitor.sequence_status() WHERE percent &gt;= 75&#39; ########## SYSTEM RULES ########## - alert: ExporterDown expr: avg_over_time(up[5m]) &lt; 0.9 for: 10s labels: service: system severity: critical severity_num: 300 annotations: description: &#39;Metrics exporter service for {{ $labels.job }} running on {{ $labels.instance }} has been down at least 50% of the time for the last 5 minutes. Service may be flapping or down.&#39; summary: &#39;Prometheus Exporter Service Down&#39; - alert: DiskUsagePerc expr: (100 - 100 * sum(node_filesystem_avail_bytes{device!~&quot;tmpfs|by-uuid&quot;,fstype=~&quot;xfs|ext&quot;} / node_filesystem_size_bytes{device!~&quot;tmpfs|by-uuid&quot;,fstype=~&quot;xfs|ext&quot;}) BY (job,device)) &gt; 70 for: 2m labels: service: system severity: warning severity_num: 200 annotations: description: &#39;Disk usage on target {{ $labels.job }} at {{ $value }}%&#39; - alert: DiskUsagePerc expr: (100 - 100 * sum(node_filesystem_avail_bytes{device!~&quot;tmpfs|by-uuid&quot;,fstype=~&quot;xfs|ext&quot;} / node_filesystem_size_bytes{device!~&quot;tmpfs|by-uuid&quot;,fstype=~&quot;xfs|ext&quot;}) BY (job,device)) &gt; 85 for: 2m labels: service: system severity: critical severity_num: 300 annotations: description: &#39;Disk usage on target {{ $labels.job }} at {{ $value }}%&#39; - alert: DiskFillPredict expr: predict_linear(node_filesystem_free_bytes{device!~&quot;tmpfs|by-uuid&quot;,fstype=~&quot;xfs|ext&quot;}[1h], 4 * 3600) &lt; 0 for: 5m labels: service: system severity: warning severity_num: 200 annotations: description: &#39;(EXPERIMENTAL) Disk {{ $labels.device }} on target {{ $labels.job }} is predicted to fill in 4 hrs based on current usage&#39; - alert: SystemLoad5m expr: node_load5 &gt; 5 for: 10m labels: service: system severity: warning severity_num: 200 annotations: description: &#39;System load for target {{ $labels.job }} is high ({{ $value }})&#39; - alert: SystemLoad5m expr: node_load5 &gt; 10 for: 10m labels: service: system severity: critical severity_num: 300 annotations: description: &#39;System load for target {{ $labels.job }} is high ({{ $value }})&#39; - alert: MemoryAvailable expr: (100 * (node_memory_Available_bytes) / node_memory_MemTotal_bytes) &lt; 25 for: 1m labels: service: system severity: warning severity_num: 200 annotations: description: &#39;Memory available for target {{ $labels.job }} is at {{ $value }}%&#39; - alert: MemoryAvailable expr: (100 * (node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) &lt; 10 for: 1m labels: service: system severity: critical severity_num: 300 annotations: description: &#39;Memory available for target {{ $labels.job }} is at {{ $value }}%&#39; - alert: SwapUsage expr: (100 - (100 * (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes))) &gt; 60 for: 1m labels: service: system severity: warning severity_num: 200 annotations: description: &#39;Swap usage for target {{ $labels.job }} is at {{ $value }}%&#39; - alert: SwapUsage expr: (100 - (100 * (node_memory_SwapFree_byte / node_memory_SwapTotal_bytes))) &gt; 80 for: 1m labels: service: system severity: critical severity_num: 300 annotations: description: &#39;Swap usage for target {{ $labels.job }} is at {{ $value }}%&#39; ########## PGBACKREST RULES ########## # # Uncomment and customize one or more of these rules to monitor your pgbackrest backups. # Full backups are considered the equivalent of both differentials and incrementals since both are based on the last full # And differentials are considered incrementals since incrementals will be based off the last diff if one exists # This avoid false alerts, for example when you don&#39;t run diff/incr backups on the days that you run a full # Stanza should also be set if different intervals are expected for each stanza. # Otherwise rule will be applied to all stanzas returned on target system if not set. # Otherwise, all backups returned by the pgbackrest info command run from where the database exists will be checked # # Relevant metric names are: # ccp_backrest_last_full_time_since_completion_seconds # ccp_backrest_last_incr_time_since_completion_seconds # ccp_backrest_last_diff_time_since_completion_seconds # # - alert: PGBackRestLastCompletedFull_main # expr: ccp_backrest_last_full_backup_time_since_completion_seconds{stanza=&quot;main&quot;} &gt; 604800 # for: 60s # labels: # service: postgresql # severity: critical # severity_num: 300 # annotations: # summary: &#39;Full backup for stanza [main] on system {{ $labels.job }} has not completed in the last week.&#39; # # - alert: PGBackRestLastCompletedIncr_main # expr: ccp_backrest_last_incr_backup_time_since_completion_seconds{stanza=&quot;main&quot;} &gt; 86400 # for: 60s # labels: # service: postgresql # severity: critical # severity_num: 300 # annotations: # summary: &#39;Incremental backup for stanza [main] on system {{ $labels.job }} has not completed in the last 24 hours.&#39; # # # Runtime monitoring is handled with a single metric: # # ccp_backrest_last_runtime_backup_runtime_seconds # # Runtime monitoring should have the &quot;backup_type&quot; label set. # Otherwise the rule will apply to the last run of all backup types returned (full, diff, incr) # Stanza should also be set if runtimes per stanza have different expected times # # - alert: PGBackRestLastRuntimeFull_main # expr: ccp_backrest_last_runtime_backup_runtime_seconds{backup_type=&quot;full&quot;, stanza=&quot;main&quot;} &gt; 14400 # for: 60s # labels: # service: postgresql # severity: critical # severity_num: 300 # annotations: # summary: &#39;Expected runtime of full backup for stanza [main] has exceeded 4 hours&#39; # # - alert: PGBackRestLastRuntimeDiff_main # expr: ccp_backrest_last_runtime_backup_runtime_seconds{backup_type=&quot;diff&quot;, stanza=&quot;main&quot;} &gt; 3600 # for: 60s # labels: # service: postgresql # severity: critical # severity_num: 300 # annotations: # summary: &#39;Expected runtime of diff backup for stanza [main] has exceeded 1 hour&#39; ## # ## If the pgbackrest command fails to run, the metric disappears from the exporter output and the alert never fires. ## An absence alert must be configured explicitly for each target (job) that backups are being monitored. ## Checking for absence of just the full backup type should be sufficient (no need for diff/incr). ## Note that while the backrest check command failing will likely also cause a scrape error alert, the addition of this ## check gives a clearer answer as to what is causing it and that something is wrong with the backups. # # - alert: PGBackrestAbsentFull_Prod # expr: absent(ccp_backrest_last_full_backup_time_since_completion_seconds{job=&quot;Prod&quot;}) # for: 10s # labels: # service: postgresql # severity: critical # severity_num: 300 # annotations: # description: &#39;Backup Full status missing for Prod. Check that pgbackrest info command is working on target system.&#39; Alertmanager/etc/prometheus/alertmanager.yml global: resolve_timeout: 5m http_config: {} smtp_from: monitor@example.com smtp_hello: localhost smtp_smarthost: smtp.example.com:465 smtp_auth_username: monitor@example.com smtp_auth_password: &#39;这里写密码&#39; smtp_require_tls: false pagerduty_url: https://events.pagerduty.com/v2/enqueue hipchat_api_url: https://api.hipchat.com/ opsgenie_api_url: https://api.opsgenie.com/ wechat_api_url: https://qyapi.weixin.qq.com/cgi-bin/ victorops_api_url: https://alert.victorops.com/integrations/generic/20131114/alert/ route: # 这里配置默认路由到&#39;default-receiver&#39; receiver: default-receiver group_by: - alertname - cluster group_wait: 10s group_interval: 10s repeat_interval: 1h inhibit_rules: - source_match: severity: critical target_match: severity: warning equal: - alertname - dev - instance receivers: - name: default-receiver email_configs: - send_resolved: false to: monitor@example.com from: monitor@example.com hello: localhost smarthost: smtp.example.com:465 auth_username: monitor@example.com auth_password: &#39;这里写密码&#39; headers: From: monitor@example.com Subject: &#39;{{ template \"email.default.subject\" . }}&#39; To: monitor@example.com html: &#39;{{ template \"email.default.html\" . }}&#39; require_tls: false # 配置钉钉机器人 webhook_configs: - send_resolved: false url: http://localhost:8060/dingtalk/webhook001/send # 配置钉钉机器人 - name: dingtalk002 webhook_configs: - send_resolved: false url: http://localhost:8060/dingtalk/webhook002/send templates: [] 配置dingtalk webhook程序 # 这里偷懒用docker跑钉钉的webhook docker run -d \\ --restart=always \\ --name prometheus-webhook-dingtalk \\ -p 8060:8060 \\ -v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro \\ timonwong/prometheus-webhook-dingtalk \\ --ding.profile=&quot;webhook001=https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxx&quot; \\ --ding.profile=&quot;webhook002=https://oapi.dingtalk.com/robot/send?access_token=yyyyyyyyyyy&quot; blackbox_exporter/etc/prometheus/backbox_exporter.yml 没空搞，占个位 mysqld_exporter需要创建用于监控的数据库用户 CREATE USER &#39;prometheus&#39;@&#39;127.0.0.1&#39; IDENTIFIED BY &#39;prometheus_password&#39; WITH MAX_USER_CONNECTIONS 3; GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO &#39;prometheus&#39;@&#39;127.0.0.1&#39;; flush privileges; postgresql_exporter根据需求决定是否使用superuser作为postgresql_exporter的数据库用户 如果要创建专用用户可以参照下面的方式创建用户 # 创建postgresql_exporter专用用户 CREATE USER postgres_exporter PASSWORD &#39;password&#39;; ALTER USER postgres_exporter SET SEARCH_PATH TO postgres_exporter,pg_catalog; # 创建schema CREATE SCHEMA postgres_exporter; # 授权schema GRANT USAGE ON SCHEMA postgres_exporter TO postgres_exporter; # 创建函数 CREATE FUNCTION get_pg_stat_activity() RETURNS SETOF pg_stat_activity AS $$ SELECT * FROM pg_catalog.pg_stat_activity; $$ LANGUAGE sql VOLATILE SECURITY DEFINER; # 创建视图 CREATE VIEW postgres_exporter.pg_stat_activity AS SELECT * from get_pg_stat_activity(); # 视图授权 GRANT SELECT ON postgres_exporter.pg_stat_activity TO postgres_exporter; # 创建函数 CREATE FUNCTION get_pg_stat_replication() RETURNS SETOF pg_stat_replication AS $$ SELECT * FROM pg_catalog.pg_stat_replication; $$ LANGUAGE sql VOLATILE SECURITY DEFINER; # 创建视图 CREATE VIEW postgres_exporter.pg_stat_replication AS SELECT * FROM get_pg_stat_replication(); # 视图授权 GRANT SELECT ON postgres_exporter.pg_stat_replication TO postgres_exporter; grafana/etc/grafana/grafana.ini app_mode = production [paths] data = /var/lib/grafana temp_data_lifetime = 24h logs = /var/log/grafana plugins = /var/lib/grafana/plugins [server] protocol = http http_port = 3000 domain = gkht root_url = http://localhost:3000 enable_gzip = true [database] log_queries = [remote_cache] [session] provider = file [dataproxy] [analytics] reporting_enabled = false check_for_updates = false [security] admin_user = admin admin_password = admin secret_key = SW2YcwTIb9zpOOhoPsMm [snapshots] [dashboards] versions_to_keep = 10 [users] default_theme = dark [auth] [auth.anonymous] enabled = true org_role = Viewer [auth.github] [auth.google] [auth.generic_oauth] [auth.grafana_com] [auth.proxy] [auth.basic] [auth.ldap] [smtp] [emails] [log] mode = console file level = info [log.console] [log.file] log_rotate = true daily_rotate = true max_days = 7 [log.syslog] [alerting] enabled = true execute_alerts = true [explore] [metrics] enabled = true interval_seconds = 10 [metrics.graphite] [tracing.jaeger] [grafana_com] url = https://grafana.com [external_image_storage] [external_image_storage.s3] [external_image_storage.webdav] [external_image_storage.gcs] [external_image_storage.azure_blob] [external_image_storage.local] [rendering] [enterprise] [panels] systemd服务prometheus.service/usr/lib/systemd/system/prometheus.service [Unit] Description=prometheus After=network.target [Service] Type=simple User=prometheus ExecStart=/usr/local/bin/prometheus \\ --config.file=/etc/prometheus/prometheus.yml \\ --storage.tsdb.path=/var/lib/prometheus \\ --storage.tsdb.retention.time=15d \\ --storage.tsdb.retention.size=40GB \\ --web.console.templates=/etc/prometheus/consoles \\ --web.console.libraries=/etc/prometheus/console_libraries ExecReload=/bin/kill -HUP $MAINPID Restart=on-failure RestartSec=60s [Install] WantedBy=multi-user.target alertmanager.servicce/usr/lib/systemd/system/alertmanager.service [Unit] Description=alertmanager After=network.target [Service] Type=simple User=prometheus ExecStart=/usr/local/bin/alertmanager \\ --config.file=/etc/prometheus/alertmanager.yml \\ --storage.path=/var/lib/alertmanager \\ --data.retention=120h Restart=on-failure RestartSec=60s [Install] WantedBy=multi-user.target node_exporter.service/usr/lib/systemd/system/node_exporter.service [Unit] Description=node_exporter After=network.target [Service] Type=simple User=prometheus ExecStart=/usr/local/bin/node_exporter Restart=on-failure RestartSec=60s [Install] WantedBy=multi-user.target blackbox_exporter.service/usr/lib/systemd/system/balckbox_exporter.service [Unit] Description=blackbox_exporter After=network.target [Service] Type=simple User=prometheus ExecStart=/usr/local/bin/blackbox_exporter \\ --config.file=/etc/prometheus/blackbox.yml \\ --web.listen-address=:9115 \\ --log.level=info Restart=on-failure RestartSec=60s [Install] WantedBy=multi-user.target mysqld_exporter.service/usr/lib/systemd/system/mysqld_exporter.service [Unit] Description=mysqld_exporter After=network.target [Service] Type=simple User=prometheus Environment=&#39;DATA_SOURCE_NAME=prometheus:prometheus_password@tcp(127.0.0.1:3306)&#39; ExecStart=/usr/local/bin/mysqld_exporter \\ --collect.engine_innodb_status \\ --collect.info_schema.innodb_metrics \\ --collect.info_schema.userstats \\ --collect.perf_schema.eventsstatements \\ --collect.perf_schema.indexiowaits \\ --collect.perf_schema.tableiowaits \\ --collect.slave_status \\ --log.level=info \\ --web.listen-address=:9104 \\ --web.telemetry-path=/metrics Restart=on-failure RestartSec=60s [Install] WantedBy=multi-user.target postgresql_exporter.service/usr/lib/systemd/system/postgresql_exporter.service [Unit] Description=postgresql_exporter After=network.target [Service] Type=simple User=prometheus Environment=DATA_SOURCE_NAME=postgresql://postgres_exporter:password@localhost:5432/postgres?sslmode=disable ExecStart=/usr/local/bin/postgresql_exporter \\ --web.listen-address=:9187 \\ --web.telemetry-path=/metrics \\ --log.level=info \\ --log.format=logger:stderr Restart=on-failure RestartSec=60s [Install] WantedBy=multi-user.target 启动服务修改了systemd脚本之后需要reload一下 systemctl daemon-reload prometheussystemctl enable --now prometheus.service alertmanagersystemctl enable --now alertmanager.service node_exportersystemctl enable --now node_exporter.service grafanasystemctl enable --now grafana.service 其他 其他服务同理，择需启动对应的服务即可 验证服务prometheus浏览器访问http://prometheus_server_ip:9090 alertmanager浏览器访问http://prometheus_server_ip:9093 node_exporter浏览器访问http://prometheus_server_ip:9100 grafana浏览器访问http://prometheus_server_ip:3000 默认用户密码admin/admin，初次登录需要改密码 配置grafana监控面板这里很多作业可以抄，这里简单列举几个我用到的面板 node_exporter面板1 Node Exporter 0.16–0.18 for Prometheus 监控展示看板 mysqld_exporter面板Percona出品的dashboard 第三方人员提供的dashboard","categories":[{"name":"监控","slug":"监控","permalink":"https://cyylog.github.io/categories/%E7%9B%91%E6%8E%A7/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://cyylog.github.io/tags/Prometheus/"}]},{"title":"网络解析与抓包工具","slug":"网络解析与抓包工具","date":"2020-05-20T16:24:11.000Z","updated":"2020-10-01T14:25:36.005Z","comments":true,"path":"2020/05/21/wang-luo-jie-xi-yu-zhua-bao-gong-ju/","link":"","permalink":"https://cyylog.github.io/2020/05/21/wang-luo-jie-xi-yu-zhua-bao-gong-ju/","excerpt":"","text":"网络解析与抓包–简介本文简述一下内容： TCP三次握手和四次挥手 网络分析工具tcpdump抓包 No.1 tcp三次握手和四次挥手—确认ACK, 仅当ACK=1时, 确认号字段才有效. TCP规定在连接建立后所有报文的传输都必须把ACK置1 —同步SYN, 在连接建立时用来同步序号. 当SYN=1 ACK=0 表明是连接请求报文, 若同意连接则响应报文中应该使SYN=1 ACK=1 —终止FIN, 用来释放连接. 当FIN=1表明此报文的发送方的数据已经发送完毕并且要求释放 三次握手_建立连接 第一次握手:Client将标志位SYN置为1, 随机产生一个值seq=x, 并将该数据包发送给Server, Client进入SYN_SENT状态, 等待Server确认 第二次握手:Server收到数据包后由标志位SYN=1知道Client请求建立连接, Server将标志位SYN和ACK都置为1, ack=x+1, 随机产生一个值seq=y, 并将该数据包发送给Client以确认连接请求, Server进入SYN_RCVD状态 第三次握手: Client收到确认后, 检查ack是否为x+1, ACK是否为1, 如果正确则将标志位ACK置为1, ack=y+1并将该数据包发送给Server, Server检查ack是否为y+1, ACK是否为1, 如果正确则连接建立成功, Client和Server进入ESTABLISHED状态, 完成三次握手, 随后Client与Server之间可以开始传输数据了 四次挥手_断开连接 第一次挥手：Client发送一个FIN, 用来关闭Client到Server的数据传送, Client进入FIN_WAIT_1状态 第二次挥手：Server收到FIN后, 发送一个ACK给Client, 确认序号为收到序号+1, Server进入CLOSE_WAIT状 第三次挥手：Server发送一个FIN, 用来关闭Server到Client的数据传送, Server进入LAST_ACK状态 第四次挥手：Client收到FIN后, Client进入TIME_WAIT状态, 接着发送一个ACK给Server, 确认序号为收到序号+1, Server进入CLOSED状态, 完成四次挥手 No.2 网络分析工具tcpdump抓包抓取tcp的三次握手 tcpdump -S host 192.168.0.108 and 151.101.100.133 抓取tcp的四次挥手（算了，懒得写了）在上方的基础上断开连接即可","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Tools","slug":"Tools","permalink":"https://cyylog.github.io/tags/Tools/"}]},{"title":"Interview Go_001--undone","slug":"Interview/Interview_Go_001","date":"2020-04-21T21:55:39.000Z","updated":"2020-09-28T05:06:55.642Z","comments":true,"path":"2020/04/22/interview/interview-go-001/","link":"","permalink":"https://cyylog.github.io/2020/04/22/interview/interview-go-001/","excerpt":"","text":"go： 超时退出 context的初始形态？怎么使用contenxt？ wait group的使用？不加add直接wait是否会panic？ new和make有什么区别？哪个可以初始化内存为0？ runtime是什么？runtime属于线程还是协程 项目： 什么是JWT？JWT有几部分组成？用什么签名算法？如何实现把一个用户从登陆状态踢出？JWT和session什么区别？ 什么是前缀树？ gin中间件原理？中间件用到了什么设计模式？ 跨域 gin文件传送的时候怎么接收？http头部的content-type写什么","categories":[{"name":"Interview","slug":"Interview","permalink":"https://cyylog.github.io/categories/Interview/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://cyylog.github.io/tags/Go/"}]},{"title":"Interview MySQL_001--undone","slug":"Interview/Interview_MySQL_001","date":"2020-04-21T20:55:39.000Z","updated":"2020-09-28T05:06:49.484Z","comments":true,"path":"2020/04/22/interview/interview-mysql-001/","link":"","permalink":"https://cyylog.github.io/2020/04/22/interview/interview-mysql-001/","excerpt":"","text":"mysql： mysql事务的隔离级别 什么是乐观锁和悲观锁？乐观锁的一致性怎么保证","categories":[{"name":"Interview","slug":"Interview","permalink":"https://cyylog.github.io/categories/Interview/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://cyylog.github.io/tags/MySQL/"}]},{"title":"Interview Network_001--undone","slug":"Interview/Interview_Network_001","date":"2020-04-21T19:55:39.000Z","updated":"2020-09-28T05:06:40.754Z","comments":true,"path":"2020/04/22/interview/interview-network-001/","link":"","permalink":"https://cyylog.github.io/2020/04/22/interview/interview-network-001/","excerpt":"","text":"网络 http的3次握手 https的连接过程？双方使用的加密套件是由客户端决定的，还是服务端决定的？","categories":[{"name":"Interview","slug":"Interview","permalink":"https://cyylog.github.io/categories/Interview/"}],"tags":[{"name":"Network","slug":"Network","permalink":"https://cyylog.github.io/tags/Network/"}]},{"title":"Interview System_001--undone","slug":"Interview/Interview_System_001","date":"2020-04-21T18:55:39.000Z","updated":"2020-10-20T21:11:58.866Z","comments":true,"path":"2020/04/22/interview/interview-system-001/","link":"","permalink":"https://cyylog.github.io/2020/04/22/interview/interview-system-001/","excerpt":"","text":"操作系统： 进程、线程、协程有什么区别？4个线程执行3个文件句柄是共享这3个文件么？ epoll的原理过程 1、首先执行epoll_create在内核专属于epoll的高速cache区，并在该缓冲区建立红黑树和就绪链表，用户态传入的文件句柄将被放到红黑树中（第一次拷贝）。 2、内核针对读缓冲区和写缓冲区来判断是否可读可写 3、epoll_ctl执行add动作时除了将文件句柄放到红黑树上之外，还向内核注册了该文件句柄的回调函数，内核在检测到某句柄可读可写时则调用该回调函数，回调函数将文件句柄放到就绪链表。 4、epoll_wait只监控就绪链表就可以，如果就绪链表有文件句柄，则表示该文件句柄可读可写，并返回到用户态（少量的拷贝）； 5、由于内核不修改文件句柄的位，因此只需要在第一次传入就可以重复监控，直到使用epoll_ctl删除，否则不需要重新传入，因此无多次拷贝。 总结：epoll是继承了select/poll的I/O复用的思想，并在二者的基础上从监控IO流、查找I/O事件等角度来提高效率，具体地说就是内核句柄列表、红黑树、就绪list链表来实现的。","categories":[{"name":"Interview","slug":"Interview","permalink":"https://cyylog.github.io/categories/Interview/"}],"tags":[{"name":"System","slug":"System","permalink":"https://cyylog.github.io/tags/System/"}]},{"title":"分布式-undone","slug":"Interview/分布式见解","date":"2020-04-21T17:55:39.000Z","updated":"2020-09-30T20:03:15.966Z","comments":true,"path":"2020/04/22/interview/fen-bu-shi-jian-jie/","link":"","permalink":"https://cyylog.github.io/2020/04/22/interview/fen-bu-shi-jian-jie/","excerpt":"","text":"三大 协议raft zab gossip raft是etcd用的. zab是zookeepr用的 gossip是redis集群用的 raft 一言以蔽之 就是 状态机+ 操作日志 +快照 见解： 所谓的分布式 最关键的 就是 各个节点之间“产生关联”，所谓的分布式 最关键的 就是 各个节点之间“产生关联” 所谓的分布式 最关键的 就是 各个节点之间“产生关联” 如果 没有“关联” 那么 这只能叫做分开部署 一旦有了关联 才叫 “分布式“ 分布式 有简单的 场景 最基础的是 主从 譬如MySQL主从 redis主从 后来逐步演化为 选主。 一旦有了选主 那么就真正叫做集群了","categories":[{"name":"Interview","slug":"Interview","permalink":"https://cyylog.github.io/categories/Interview/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://cyylog.github.io/tags/Go/"}]},{"title":"Tomcat__05_JVM_排障工具","slug":"Linux/Tomcat/Tomcat-05","date":"2020-04-01T16:36:51.000Z","updated":"2020-05-25T13:58:25.372Z","comments":true,"path":"2020/04/02/linux/tomcat/tomcat-05/","link":"","permalink":"https://cyylog.github.io/2020/04/02/linux/tomcat/tomcat-05/","excerpt":"","text":"JVM 运维实用排障工具1、jps用来查看Java进程的具体状态, 包括进程ID，进程启动的路径及启动参数等等，与unix上的ps类似，只不过jps是用来显示java进程，可以把jps理解为ps的一个子集。 常用参数如下: -q：忽略输出的类名、Jar名以及传递给main方法的参数，只输出pid -m：输出传递给main方法的参数，如果是内嵌的JVM则输出为null -l：输出完全的包名，应用主类名，jar的完全路径名 -v：输出传给jvm的参数 注意: 使用jps 时的运行账户要和JVM 虚拟机启动的账户一致。若启动JVM虚拟机是运行的账户为www，那使用jps指令时，也要使用www 用户去指定。 sudo -u www jpsExample // 查看已经运行的JVM 进程的实际启动参数 [root@mouse03 bin]# jps -v 38372 Jps -Dapplication.home=/usr/local/jdk -Xms8m 38360 Bootstrap -Djava.util.logging.config.file=/data0/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Xms4096m -Xmx4096m -XX:PermSize=1024m -XX:MaxPermSize=2048m -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dignore.endorsed.dirs= -Dcatalina.base=/data0/tomcat -Dcatalina.home=/data0/tomcat -Djava.io.tmpdir=/data0/tomcat/temp 2、jstackjstack用于打印出给定的java进程ID或core file或远程调试服务的Java堆栈信息。如果现在运行的java程序呈现hung的状态，jstack是非常有用的。此信息通常在运维的过程中被保存起来(保存故障现场)，以供RD们去分析故障。 常用参数如下: jstack &lt;pid&gt; jstack [-l] &lt;pid&gt; //长列表. 打印关于锁的附加信息 jstack [-F] &lt;pid&gt; //当’jstack [-l] pid’没有响应的时候强制打印栈信息Example // 打印JVM 的堆栈信息，以供问题排查 [root@mouse03 ~]# jstack -F 38360 > /tmp/jstack.log 3、jinfo可以查看或修改运行时的JVM进程的参数。 常用参数: jinfo [option] pid where &lt;option&gt; is one of: -flag &lt;name&gt; to print the value of the named VM flag -flag [+|-]&lt;name&gt; to enable or disable the named VM flag -flag &lt;name&gt;=&lt;value&gt; to set the named VM flag to the given value -flags to print VM flagsExample // 根据 PID 查看目前分配的最大堆栈 [root@mouse03 ~]# jinfo -flag MaxHeapSize 38360 -XX:MaxHeapSize=4294967296 // 动态更改 JVM 的最大堆栈值 [root@mouse03 ~]# jinfo -flag MaxHeapSize=4294967296 38360 Exception in thread \"main\" com.sun.tools.attach.AttachOperationFailedException: flag 'MaxHeapSize' cannot be changed at sun.tools.attach.LinuxVirtualMachine.execute(LinuxVirtualMachine.java:229) at sun.tools.attach.HotSpotVirtualMachine.executeCommand(HotSpotVirtualMachine.java:261) at sun.tools.attach.HotSpotVirtualMachine.setFlag(HotSpotVirtualMachine.java:234) at sun.tools.jinfo.JInfo.flag(JInfo.java:134) at sun.tools.jinfo.JInfo.main(JInfo.java:81) // jinfo 并不能动态的改变所有的JVM 参数。 那到底有哪些参数能够被动态的改变呢? // java -XX:+PrintFlagsFinal -version 答应JVM 的所有参数 // java -XX:+PrintFlagsFinal -version | grep manageable [root@mouse03 ~]# java -XX:+PrintFlagsFinal -version | grep manageable intx CMSAbortablePrecleanWaitMillis = 100 {manageable} intx CMSTriggerInterval = -1 {manageable} intx CMSWaitDuration = 2000 {manageable} bool HeapDumpAfterFullGC = false {manageable} bool HeapDumpBeforeFullGC = false {manageable} bool HeapDumpOnOutOfMemoryError = false {manageable} ccstr HeapDumpPath = {manageable} uintx MaxHeapFreeRatio = 70 {manageable} uintx MinHeapFreeRatio = 40 {manageable} bool PrintClassHistogram = false {manageable} bool PrintClassHistogramAfterFullGC = false {manageable} bool PrintClassHistogramBeforeFullGC = false {manageable} bool PrintConcurrentLocks = false {manageable} bool PrintGC = false {manageable} bool PrintGCDateStamps = false {manageable} bool PrintGCDetails = false {manageable} bool PrintGCID = false {manageable} bool PrintGCTimeStamps = false {manageable} // 也只有以上这些值才能够动态的被改变 [root@mouse03 ~]# jinfo -flag CMSWaitDuration=1900 38360 # 查看， jinfo -flags 查看 JVM 的 flags [root@mouse03 ~]# jinfo -flags 38360 Attaching to process ID 38360, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.91-b14 Non-default VM flags: -XX:CICompilerCount=2 -XX:CMSWaitDuration=1900 -XX:InitialHeapSize=4294967296 -XX:MaxHeapSize=4294967296 -XX:MaxNewSize=1431633920 -XX:MinHeapDeltaBytes=196608 -XX:NewSize=1431633920 -XX:OldSize=2863333376 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseFastUnorderedTimeStamps Command line: -Djava.util.logging.config.file=/data0/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Xms4096m -Xmx4096m -XX:PermSize=1024m -XX:MaxPermSize=2048m -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dignore.endorsed.dirs= -Dcatalina.base=/data0/tomcat -Dcatalina.home=/data0/tomcat -Djava.io.tmpdir=/data0/tomcat/temp 4、jstat// 监控JVM 的状态，常用指令: # jstat -gc 113059 1000 10 // 打印PID 为 113059 JVM 状态，一共打印10次，每次间隔时间为1s(1000ms) // 注 jstat 的用法超级强大， 我们这里只是列举出列其中一个简单的应用。 Example # jstat -gc 113059 1000 10 S0C S1C S0U S1U EC EU OC OU PC PU YGC YGCT FGC FGCT GCT 195904.0 195904.0 0.0 21610.3 1567680.0 1516721.9 8526272.0 3557507.8 1048576.0 163148.4 2577 92.033 0 0.000 92.033 195904.0 195904.0 23600.9 0.0 1567680.0 142541.6 8526272.0 3558435.8 1048576.0 163148.4 2578 92.060 0 0.000 92.060 195904.0 195904.0 23600.9 0.0 1567680.0 266338.1 8526272.0 3558435.8 1048576.0 163148.4 2578 92.060 0 0.000 92.060 195904.0 195904.0 23600.9 0.0 1567680.0 413941.8 8526272.0 3558435.8 1048576.0 163148.4 2578 92.060 0 0.000 92.060 195904.0 195904.0 23600.9 0.0 1567680.0 642390.6 8526272.0 3558435.8 1048576.0 163148.4 2578 92.060 0 0.000 92.060 195904.0 195904.0 23600.9 0.0 1567680.0 813957.3 8526272.0 3558435.8 1048576.0 163148.4 2578 92.060 0 0.000 92.060 195904.0 195904.0 23600.9 0.0 1567680.0 984223.2 8526272.0 3558435.8 1048576.0 163148.4 2578 92.060 0 0.000 92.060 195904.0 195904.0 23600.9 0.0 1567680.0 1155472.7 8526272.0 3558435.8 1048576.0 163148.4 2578 92.060 0 0.000 92.060 195904.0 195904.0 23600.9 0.0 1567680.0 1399228.5 8526272.0 3558435.8 1048576.0 163148.4 2578 92.060 0 0.000 92.060 195904.0 195904.0 0.0 23866.6 1567680.0 38005.6 8526272.0 3559196.7 1048576.0 163148.4 2579 92.092 0 0.000 92.092 字段意义如下 列名 说明 S0C 新生代中Survivor space中S0当前容量的大小（KB） S1C 新生代中Survivor space中S1当前容量的大小（KB） S0U 新生代中Survivor space中S0容量使用的大小（KB） S1U 新生代中Survivor space中S1容量使用的大小（KB） EC Eden space当前容量的大小（KB） EU Eden space容量使用的大小（KB） OC Old space当前容量的大小（KB） OU Old space使用容量的大小（KB） PC Permanent space当前容量的大小（KB） PU Permanent space使用容量的大小（KB） YGC 从应用程序启动到采样时发生 Young GC 的次数 YGCT 从应用程序启动到采样时 Young GC 所用的时间(秒) FGC 从应用程序启动到采样时发生 Full GC 的次数 FGCT 从应用程序启动到采样时 Full GC 所用的时间(秒) GCT T从应用程序启动到采样时用于垃圾回收的总时间(单位秒)，它的值等于YGC+FGC 5、jvmtop以上介绍的jps、jstack、jinfo等都是安装JDK 时自带的系统分析工具，而jvmtop是一款开源的JVM工具。 它的下载地址如下: https://github.com/patric-r/jvmtop 顾名思义，它是一个只针对JVM的工具，展示的方式和unix的top命令相似. jvmtop 提供了两个视图，一个是概览视图，可以展示出当前机器的所有的 JVM 的情况. 还有一个视图是详情视图，展示一个 JVM 的详细情况.概览视图 jvmtop.sh 其中，各个字段的意义分别如下： PID：进程 ID MAIN-CLASS：main 类的名字 HPCUR：当前被使用的 heap 的大小 HPMAX：最大可用的 heap 的大小 NHCUR：当前被使用的非 heap 大小（比如：perm gen） NHMAX：最大可用的非 heap 大小 CPU：CPU 的使用情况 GC：消耗在 GC 上的时间比例 VM：JVM 的提供者，大版本号，小版本号，图中的意思是 Apple 提供的 JDK 6U51 版本。 USERNAME：当前的用户名 #T：线程数量 DL：是否有现成发生死锁详情视图 jvmtop.sh &lt;pid&gt; 其中，各个字段的意义如下： TID：线程 ID NAME：线程名 STATE：线程状态 CPU：线程当前的 CPU 占用情况 TOTALCPU：从线程被创建开始总体的 CPU 占用情况 BLOCKBY：阻塞这个线程的线程 ID","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Tomcat","slug":"Tomcat","permalink":"https://cyylog.github.io/tags/Tomcat/"}]},{"title":"Tomcat_04_安全优化","slug":"Linux/Tomcat/Tomcat-04","date":"2020-03-29T16:36:31.000Z","updated":"2020-05-25T13:58:16.512Z","comments":true,"path":"2020/03/30/linux/tomcat/tomcat-04/","link":"","permalink":"https://cyylog.github.io/2020/03/30/linux/tomcat/tomcat-04/","excerpt":"","text":"10、Tomcat安全优化1、telnet管理端口保护（强制） 类别 配置内容及说明 标准配置 备注 telnet管理端口保护 1.修改默认的8005管理端口为不易猜测的端口（大于1024）；2.修改SHUTDOWN指令为其他字符串； 1.以上配置项的配置内容只是建议配置，可以按照服务实际情况进行合理配置，但要求端口配置在8000~8999之间； 2、 ajp连接端口保护（推荐） 类别 配置内容及说明 标准配置 备注 Ajp 连接端口保护 1.修改默认的ajp 8009端口为不易冲突的大于1024端口；2.通过iptables规则限制ajp端口访问的权限仅为线上机器； &lt;Connector port=”8528“protocol=”AJP/1.3” /&gt; 以上配置项的配置内容仅为建议配置，请按照服务实际情况进行合理配置，但要求端口配置在8000~8999之间；；保护此端口的目的在于防止线下的测试流量被mod_jk转发至线上tomcat服务器； 3、禁用管理端（强制） 类别 配置内容及说明 标准配置 备注 禁用管理端 1. 删除默认的{Tomcat安装目录}/conf/tomcat-users.xml文件，重启tomcat后将会自动生成新的文件；2. 删除{Tomcat安装目录}/webapps下默认的所有目录和文件；3.将tomcat 应用根目录配置为tomcat安装目录以外的目录； &lt;Context path=”” docBase=”/home/work/local/tomcat**_webapps**”debug=”0”reloadable=”false”crossContext=”true”/&gt; 对于前段web模块，Tomcat管理端属于tomcat的高危安全隐患，一旦被攻破，黑客通过上传web shell的方式将会直接取得服务器的控制权，后果极其严重； 4、降权启动（强制） 类别 配置内容及说明 标准配置 备注 降权启动 1.tomcat启动用户权限必须为非root权限，尽量降低tomcat启动用户的目录访问权限；2.如需直接对外使用80端口，可通过普通账号启动后，配置iptables规则进行转发； 避免一旦tomcat 服务被入侵，黑客直接获取高级用户权限危害整个server的安全； [root@web03 ~]# useradd tomcat [root@web03 ~]# cp -a /application/tools/tomcat8_1 /home/tomcat/ [root@web03 ~]# chown -R tomcat.tomcat /home/tomcat/tomcat8_1/ [root@web03 ~]# su -c &#39;/home/tomcat/tomcat8_1/bin/startup.sh&#39; tomcat Using CATALINA_BASE: /home/tomcat/tomcat8_1 Using CATALINA_HOME: /home/tomcat/tomcat8_1 Using CATALINA_TMPDIR: /home/tomcat/tomcat8_1/temp Using JRE_HOME: /application/jdk Using CLASSPATH: /home/tomcat/tomcat8_1/bin/bootstrap.jar:/home/tomcat/tomcat8_1/bin/tomcat-juli.jar Tomcat started. [root@web03 ~]# ps -ef|grep tomcat5、文件列表访问控制（强制） 类别 配置内容及说明 标准配置 备注 文件列表访问控制 1.conf/web.xml文件中default部分listings的配置必须为false； listingsfalse false为不列出目录文件，true为允许列出，默认为false； 6、版本信息隐藏（强制） 类别 配置内容及说明 标准配置 备注 版本信息隐藏 1.修改conf/web.xml，重定向403、404以及500等错误到指定的错误页面；2.也可以通过修改应用程序目录下的WEB-INF/web.xml下的配置进行错误页面的重定向； 403/forbidden.jsp404/notfound.jsp500/systembusy.jsp 在配置中对一些常见错误进行重定向，避免当出现错误时tomcat默认显示的错误页面暴露服务器和版本信息；必须确保程序根目录下的错误页面已经存在； 7、Server header重写（推荐） 类别 配置内容及说明 标准配置 备注 Server header重写 在HTTP Connector配置中加入server的配置； server=”webserver“ 当tomcat HTTP端口直接提供web服务时此配置生效，加入此配置，将会替换http 响应Server header部分的默认配置，默认是Apache-Coyote/1.1 8、访问限制（可选） 类别 配置内容及说明 标准配置或操作 备注 访问限制 通过配置，限定访问的ip来源 &lt;Valve className=”org.apache.catalina.valves.RemoteAddrValve” allow=”61.148.18.138,61.135.165.*“ deny=”*.*.*.*“/&gt; 通过配置信任ip的白名单，拒绝非白名单ip的访问，此配置主要是针对高保密级别的系统，一般产品线不需要； 9、起停脚本权限回收（推荐） 类别 配置内容及说明 标准配置或操作 备注 起停脚本权限回收 去除其他用户对Tomcat的bin目录下shutdown.sh、startup.sh、catalina.sh的可执行权限； chmod -R 744 tomcat/bin/* 防止其他用户有起停线上Tomcat的权限； 10、 访问日志格式规范（推荐） 类别 配置内容及说明 标准配置或操作 备注 访问日志格式规范 开启Tomcat默认访问日志中的Referer和User-Agent记录 开启Referer和User-Agent是为了一旦出现安全问题能够更好的根据日志进行问题排查； 11、 附录：建议配置及标准执行方案1. 配置部分（**${ CATALINA_HOME }conf/server.xml**） &lt;Server port=&quot;8527&quot; shutdown=&quot; dangerous&quot;&gt; &lt;!-- Define a non-SSL HTTP/1.1 Connector on port 8080 --&gt; &lt;Connector port=&quot;8080&quot; server=&quot;webserver&quot;/&gt; &lt;!-- Define an AJP 1.3 Connector on port 8528 --&gt; &lt;!--Define an accesslog --&gt; &lt;Valve className=&quot;org.apache.catalina.valves.AccessLogValve&quot; directory=&quot;logs&quot; prefix=&quot;localhost_access_log.&quot; suffix=&quot;.txt&quot; pattern=&quot;%h %l %u %t %r %s %b %{Referer}i %{User-Agent}i %D&quot; resolveHosts=&quot;false&quot;/&gt; &lt;Connector port=&quot;8528&quot; protocol=&quot;AJP/1.3&quot; /&gt; &lt;Context path=&quot;&quot; docBase=&quot;/home/work/local/tomcat_webapps&quot; debug=&quot;0&quot; reloadable=&quot;false&quot; crossContext=&quot;true&quot;/&gt;2. 配置部分（**${ CATALINA_HOME }conf/web.xml或者WEB-INF/web.xml**） &lt;init-param&gt; &lt;param-name&gt;listings&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;error-page&gt; &lt;error-code&gt;403&lt;/error-code&gt; &lt;location&gt;/forbidden.jsp&lt;/location&gt; &lt;/error-page&gt; &lt;error-page&gt; &lt;error-code&gt;404&lt;/error-code&gt; &lt;location&gt;/notfound.jsp&lt;/location&gt; &lt;/error-page&gt; &lt;error-page&gt; &lt;error-code&gt;500&lt;/error-code&gt; &lt;location&gt;/systembusy.jsp&lt;/location&gt; &lt;/error-page&gt;3. 删除如下**tomcat**的默认目录和默认文件 tomcat/webapps/* tomcat/conf/tomcat-user.xml4. 去除其他用户对**tomcat** 起停脚本的执行权限 chmod 744 –R tomcat/bin/*11、Tomcat性能优化tomcat性能取决于 内存大小 上策：优化代码 该项需要开发经验足够丰富，对开发人员要求较高 中策：jvm**优化机制** 垃圾回收机制 把不需要的内存回收 ​ 优化jvm–优化垃圾回收策略 优化catalina.sh配置文件。在catalina.sh配置文件中添加以下代码 # tomcat分配1G内存模板 JAVA_OPTS=&quot;-Djava.awt.headless=true -Dfile.encoding=UTF-8 -server -Xms1024m -Xmx1024m -XX:NewSize=512m -XX:MaxNewSize=512m -XX:PermSize=512m -XX:MaxPermSize=512m&quot; JAVA_OPTS=&quot;-Djava.awt.headless=true -Dfile.encoding=UTF-8 -server -Xms800m -Xmx800m -XX:NewSize=400m -XX:MaxNewSize=400m -XX:PermSize=400m -XX:MaxPermSize=400m&quot; # 重启服务 su -c &#39;/home/tomcat/tomcat8_1/bin/shutdown.sh&#39; tomcat su -c &#39;/home/tomcat/tomcat8_1/bin/startup.sh&#39; tomcat​ 修改之前 ​ 修改之后 下策：加足够大的内存 该项的资金投入较大 下下策：每天0**点定时重启tomcat** 使用较为广泛","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Tomcat","slug":"Tomcat","permalink":"https://cyylog.github.io/tags/Tomcat/"}]},{"title":"Kubernetes常用命令","slug":"容器/Kubernetes常用命令","date":"2020-03-28T17:21:16.000Z","updated":"2020-05-25T13:43:46.812Z","comments":true,"path":"2020/03/29/rong-qi/kubernetes-chang-yong-ming-ling/","link":"","permalink":"https://cyylog.github.io/2020/03/29/rong-qi/kubernetes-chang-yong-ming-ling/","excerpt":"","text":"常用命令Kubernetes Cheat SheetViewing Resource Information //查看资源信息 Nodes$ kubectl get no $ kubectl get no -o wide $ kubectl describe no $ kubectl get no - o yaml $ kubectl get node - - sel ect or =[ l abel _name] $ kubectl get nodes -o jsonpath=&#39; {.items[*].status.addresses [?(@.type==&quot;ExternalIP&quot;)].address}&#39; $ kubectl top node [node_name]Pods$ kubectl get po -o wide $ kubectl describe po $ kubectl get po $ kubectl get po --show-labels $ kubectl get po -l app=nginx $ kubectl get po -o yaml $ kubectl get pod [pod_name] - o yaml --export $ kubect l get pod [ pod_name] - o yaml --export &gt; nameoffile.yaml $ kubectl get pods -- field-selector status.phase=Running Namespaces$ kubectl get ns $ kubectl get ns - o yaml $ kubectl describe nsDeployments$ kubectl get deploy $ kubectl describe deploy $ kubectl get deploy - o wide $ kubectl get deploy - o yamlServices$ kubectl get svc $ kubectl describe svc $ kubectl get svc - o wide $ kubectl get svc - o yaml $ kubectl get svc --show-labelsDaemonSets$ kubectl get ds $ kubectl get ds --all-namespaces $ kubectl describe ds [daemonset_name] - n [namespce_name] $ kubectl get ds [ds_name] -n [ns_name] -o yamlEvents$ kubectl get events $ kubectl get events -n kube-system $ kubectl get events -wlogs$ kubectl logs [pod_name] $ kubectl logs --since=1h [pod_name] $ kubectl logs --tail =20 [pod_name] $ kubectl logs -f -c [container_name] [pod_name] $ kubectl logs [pod_name] &gt; pod.logService Accounts$ kubectl get sa $ kubectl get sa -o yaml $ kubectl get serviceaccounts default -o yaml &gt;./sa.yaml $ kubectl replace serviceaccount default -f ./sa.yamlReplicaSets$ kubectl get rs $ kubectl describe rs $ kubectl get rs -o wide $ kubectl get rs -o yamlRoles$ kubectl get roles --all -namespaces $ kubectl get roles --all -namespaces -o yamlSecrets$ kubectl get secrets $ kubectl get secrets --all -namespaces $ kubectl get secrets -o yamlConfigMaps$ kubectl get cm $ kubectl get cm --all-namespaces $ kubectl get cm --all-namespaces -o yamlIngress$ kubectl get ing $ kubectl get ing --all-namespacesPersistentVolume$ kubectl get pv $ kubectl describe pvPersistentVolumeClaim$ kubectl get pvc $ kubectl describe pvcStorageClass$ kubectl get sc $ kubectl get sc -o yamlMultiple Resources$ kubectl get svc,po $ kubectl get deploy,no $ kubectl get all $ kubectl get all --all -namespacesChanging Resource Attributes //改变资源属性 Taint$ kubectl taint [node_name] [taint_name] Labels$ kubectl label [node_name] disktype=ssd $ kubectl label [pod_name] env=prodCordon/Uncordon$ kubectl cordon [node_name] $ kunectl uncordon [node_name]Drain$ kubectl drain [node_name]Nodes/Pods$ kubectl delet enode [node_name] $ kubectl delet epod [pod_name] $ kubectl edit node [node_name] $ kubectl edit pod [pod_name] Deployments/Namespaces$ kubectl edit deploy [deploy_name] $ kubectl delete deploy [deploy_name] $ kubectl expse deploy [deploy_name] --por=80 -type=NodePort $ kubectl scale deploy [deploy_name] --repicas=5 $ kubectl delete ns $ kubectl edit ns [ns_name]Services$ kubectl edit svc [svc_name] $ kubectl delete svc [svc_name]DaemonSets$ kubectl edit ds [ds_name] -n kube-system $ kubectl delete ds [ds_name]Service Accounts$ kubectl edit sa [sa_name] $ kubectl delete sa [sa_name]Annotate$ kubectl annotate po [pod_name] [annotation] $ kubectl annotate no [node_name]Adding Resources //添加资源 Creating a Pod$ kubectl create -f [name_of_file] $ kubectl apply -f [name_of_file] $ kubectl run [pod_name] --image=nginx --resart=Never $ kubectl run [pod_name] --geneator=run-pod/v1 --image=nginx $ kubectl run [pod_name] --image=nginx --restart=NeverCreating a Service$ kubectl create svc nodeport [svc_name] --tcp=8080: 80Creating a Deployment$ kubectl create -f [name_of_file] $ kubectl apply -f [name_of_file] $ kubectl create deploy [deploy_name] --image=ngi nxInteractive Pod$ kubectl run [pod_name] --image=busybox --rm -it --restart=Never -- shOutput YAML to a File$ kubectl create deploy [deploy_name] --image=nginx --dry-run -o yaml &gt; depl oy. yaml $ kubectl get po [pod_name] -o yaml --export &gt; pod. yamlGetting Help$ kubectl -h $ kubectl create -h $ kubectl run -h $ kubectl explain deploy.specRequests //请求 API Call$ kubectl get --raw /apis/metrics.k8s.io/Cluster Info$ kubectl config $ kubectl cluster-info $ kubectl get componentstatuses","categories":[{"name":"容器","slug":"容器","permalink":"https://cyylog.github.io/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://cyylog.github.io/tags/Kubernetes/"}]},{"title":"Tomcat_03_监控","slug":"Linux/Tomcat/Tomcat-03","date":"2020-03-26T16:36:10.000Z","updated":"2020-05-25T13:58:04.737Z","comments":true,"path":"2020/03/27/linux/tomcat/tomcat-03/","link":"","permalink":"https://cyylog.github.io/2020/03/27/linux/tomcat/tomcat-03/","excerpt":"","text":"9、监控tomcat集群状态1、方法一：开发java监控页面[root@web03 tomcat8_1]# cat /application/tomcat/webapps/memtest/meminfo.jsp &lt;% Runtime rtm = Runtime.getRuntime(); long mm = rtm.maxMemory()/1024/1024; long tm = rtm.totalMemory()/1024/1024; long fm = rtm.freeMemory()/1024/1024; out.println(&quot;JVM memory detail info :&lt;br&gt;&quot;); out.println(&quot;Max memory:&quot;+mm+&quot;MB&quot;+&quot;&lt;br&gt;&quot;); out.println(&quot;Total memory:&quot;+tm+&quot;MB&quot;+&quot;&lt;br&gt;&quot;); out.println(&quot;Free memory:&quot;+fm+&quot;MB&quot;+&quot;&lt;br&gt;&quot;); out.println(&quot;Available memory can be used is :&quot;+(mm+fm-tm)+&quot;MB&quot;+&quot;&lt;br&gt;&quot;); %&gt;2、方法二：使用jps命令进行监控[root@web03 ~]# jps -lvm 31906 org.apache.catalina.startup.Bootstrap start -Djava.util.logging.config.file=/application/tomcat8_1/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djava.endorsed.dirs=/application/tomcat8_1/endorsed -Dcatalina.base=/application/tomcat8_1 -Dcatalina.home=/application/tomcat8_1 -Djava.io.tmpdir=/application/tomcat8_1/temp 31812 org.apache.catalina.startup.Bootstrap start -Djava.util.logging.config.file=/application/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djava.endorsed.dirs=/application/tomcat/endorsed -Dcatalina.base=/application/tomcat -Dcatalina.home=/application/tomcat -Djava.io.tmpdir=/application/tomcat/temp 31932 org.apache.catalina.startup.Bootstrap start -Djava.util.logging.config.file=/application/tomcat8_2/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djava.endorsed.dirs=/application/tomcat8_2/endorsed -Dcatalina.base=/application/tomcat8_2 -Dcatalina.home=/application/tomcat8_2 -Djava.io.tmpdir=/application/tomcat8_2/temp 32079 sun.tools.jps.Jps -lvm -Denv.class.path=.:/application/jdk/lib:/application/jdk/jre/lib:/application/jdk/lib/tools.jar -Dapplication.home=/application/jdk1.8.0_60 -Xms8m3、Tomcat远程监控功能修改配置文件，开启远程监控 vim /application/tomcat8_1/bin/catalina.sh +97 CATALINA_OPTS=&quot;$CATALINA_OPTS -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=12345 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=10.0.0.17&quot;​ 重启服务，检查12345端口是否开启 /application/tomcat8_1/bin/shutdown.sh /application/tomcat8_1/bin/startup.sh netstat -tunlp|grep 12345​ 检查端口 [root@web03 ~]# netstat -tunlp|grep 12345 tcp6 0 0 :::12345 :::* LISTEN 33158/java 在windows**上监控tomcat** 注意：windwos**需要安装jdk**环境！ 查考：http://www.oracle.com/technetwork/java/javase/downloads/index.html 4、zabbix监控tomcat程序zabbix搭建详情参考：https://www.toutiao.com/i6808897883299906059/ 若是有问题，请移步官网 ： https://www.zabbix.com/documentation/4.0/zh/manual/installation 服务端安装配置java**监控服务** [root@m01 ~]# yum install zabbix-java-gateway -y查看配置文件 配置文件路径：/etc/zabbix/zabbix_java_gateway.conf sed -i -e &#39;220a JavaGateway=127.0.0.1&#39; -e &#39;236a StartJavaPollers=5&#39; /etc/zabbix/zabbix_server.conf启动zabbix-java-gateway服务，与zabbix服务 systemctl start zabbix-java-gateway.service systemctl restart zabbix-server.service检查java端口是否开启 [root@m01 ~]# netstat -lntup |grep java tcp6 0 0 :::10052 :::* LISTEN 72971/java ​ 检查java进程是否存在 [root@m01 ~]# ps -ef |grep [j]ava zabbix 72971 1 0 11:29 ? 00:00:00 java -server -Dlogback.configurationFile=/etc/zabbix/zabbix_java_gateway_logback.xml -classpath lib:lib/android-json-4.3_r3.1.jar:lib/logback-classic-0.9.27.jar:lib/logback-core-0.9.27.jar:lib/slf4j-api-1.6.1.jar:bin/zabbix-java-gateway-3.0.13.jar -Dzabbix.pidFile=/var/run/zabbix/zabbix_java.pid -Dzabbix.timeout=3 -Dsun.rmi.transport.tcp.responseTimeout=3000 com.zabbix.gateway.JavaGateway zabbix 73255 73226 0 11:35 ? 00:00:00 /usr/sbin/zabbix_server: java poller #1 [got 0 values in 0.000002 sec, idle 5 sec] zabbix 73256 73226 0 11:35 ? 00:00:00 /usr/sbin/zabbix_server: java poller #2 [got 0 values in 0.000002 sec, idle 5 sec] zabbix 73257 73226 0 11:35 ? 00:00:00 /usr/sbin/zabbix_server: java poller #3 [got 0 values in 0.000002 sec, idle 5 sec] zabbix 73258 73226 0 11:35 ? 00:00:00 /usr/sbin/zabbix_server: java poller #4 [got 0 values in 0.000002 sec, idle 5 sec] zabbix 73259 73226 0 11:35 ? 00:00:00 /usr/sbin/zabbix_server: java poller #5 [got 0 values in 0.000004 sec, idle 5 sec]web**界面添加** ​ 添加主机 ​ 主机管理模板，注意是JMX模板 监控完成 5、排除tomcat故障步骤a. 查看catalina.out b. 使用sh show-busy-java-threads.sh脚本进行检测 脚本下载地址 https://files.cnblogs.com/files/clsn/show-busy-java-threads.sh","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Tomcat","slug":"Tomcat","permalink":"https://cyylog.github.io/tags/Tomcat/"}]},{"title":"Tomcat_02_应用部署","slug":"Linux/Tomcat/Tomcat-02","date":"2020-03-24T16:36:05.000Z","updated":"2020-05-25T13:57:56.521Z","comments":true,"path":"2020/03/25/linux/tomcat/tomcat-02/","link":"","permalink":"https://cyylog.github.io/2020/03/25/linux/tomcat/tomcat-02/","excerpt":"","text":"6、WEB站点部署上线的代码有两种方式： 第一种方式是直接将程序目录放在webapps目录下面，这种方式大家已经明白了，就不多说了。 第二种方式是使用开发工具将程序打包成war包，然后上传到webapps目录下面。 1、使用war包部署web站点[root@web03 webapps]# pwd /application/tomcat/webapps [root@web03 webapps]# wget http://10.0.0.1/apache/tomcat/memtest.war站点主动解压部署 [root@web03 webapps]# ls docs examples host-manager logs manager memtest memtest.war ROOT浏览器访问： http://10.0.0.17:8080//memtest/meminfo.jsp 2、自定义默认网站目录上面访问的网址为 http://10.0.0.3:8080/memtest/meminfo.jsp 现在想访问格式为http://10.0.0.3:8080/meminfo.jsp 方法一 将meminfo.jsp或其他程序放在tomcat/webapps/ROOT目录下即可。因为默认网站根目录为tomcat/webapps/ROOT 方法二 [root@web03 ~]# vim /application/tomcat/conf/server.xml +125 …… #添加上这两行 &lt;Context path=&quot;&quot; docBase=&quot;/application/tomcat/webapps/memtest&quot; debug=&quot;0&quot; reloadable=&quot;false&quot; crossContext=&quot;true&quot;/&gt; &lt;Context path=&quot;/40team&quot; docBase=&quot;/application/tomcat/webapps/memtest&quot; debug=&quot;0&quot; reloadable=&quot;false&quot; crossContext=&quot;true&quot;/&gt; ……修改配置文件后，要重启服务 [root@web03 ~]# /application/tomcat/bin/shutdown.sh [root@web03 ~]# /application/tomcat/bin/startup.sh 3、部署开源站点（jpress）jpress官网：http://jpress.io 下载地址：https://github.com/JpressProjects/jpress ​ 第一个里程碑：安装配置数据库 yum -y install mariadb-server systemctl start mariadb.service​ #配置数据库 mysql create database jpress DEFAULT CHARACTER SET utf8; grant all on jpress.* to jpress@&#39;localhost&#39; identified by &#39;123456&#39;; exit​ 第二个里程碑：jpress站点上线 [root@web03 webapps]# pwd /application/tomcat/webapps [root@web03 webapps]# wget http://10.0.0.1/apache/tomcat/jpress-web-newest.war​ 第三个里程碑：浏览器访问 浏览器访问： http://10.0.0.17:8080/jpress-web-newest/install 填写数据库信息 设置站点名称等 安装完成 重启tomcat服务 [root@web03 ~]# /application/tomcat/bin/shutdown.sh [root@web03 ~]# /application/tomcat/bin/startup.sh 7、Tomcat多实例配置多虚拟主机：nginx 多个Server标签（域名，ip，端口） 进程数量固定 master+worker 多实例（多进程）：同一个程序启动多次，分为两种情况: 第一种：一台机器跑多个站点； 第二种：一个机器跑一个站点多个实例，配合负载均衡 1、复制程序文件 cd /application/tools/ tar xf apache-tomcat-8.0.27.tar.gz cp -a apache-tomqcat-8.0.27 tomcat8_1 cp -a apache-tomcat-8.0.27 tomcat8_2修改端口，以启动多实例。多实例之间端口不能一致 sed -i &#39;s#8005#8011#;s#8080#8081#&#39; tomcat8_1/conf/server.xml sed -i &#39;s#8005#8012#;s#8080#8082#&#39; tomcat8_2/conf/server.xml [root@web03 application]# diff tomcat8_1/conf/server.xml tomcat8_2/conf/server.xml 22c22 &lt; &lt;Server port=&quot;8011&quot; shutdown=&quot;SHUTDOWN&quot;&gt; --- &gt; &lt;Server port=&quot;8012&quot; shutdown=&quot;SHUTDOWN&quot;&gt; 67c67 &lt; Define a non-SSL/TLS HTTP/1.1 Connector on port 8081 --- &gt; Define a non-SSL/TLS HTTP/1.1 Connector on port 8082 69c69 &lt; &lt;Connector port=&quot;8081&quot; protocol=&quot;HTTP/1.1&quot; --- &gt; &lt;Connector port=&quot;8082&quot; protocol=&quot;HTTP/1.1&quot; 75c75 &lt; port=&quot;8081&quot; protocol=&quot;HTTP/1.1&quot; --- &gt; port=&quot;8082&quot; protocol=&quot;HTTP/1.1&quot; 将配置好的tomcat程序打包，以备之后使用 tar zcf muti_tomcat8.tar.gz ./tomcat8_1 ./tomcat8_2启动tomcat多实例 /application/tomcat8_1/bin/startup.sh /application/tomcat8_2/bin/startup.sh检查端口是否启动 [root@web03 tomcat8_1]# netstat -lntup |grep java tcp6 0 0 127.0.0.1:8011 :::* LISTEN 31906/java tcp6 0 0 127.0.0.1:8012 :::* LISTEN 31932/java tcp6 0 0 :::8080 :::* LISTEN 31812/java tcp6 0 0 :::8081 :::* LISTEN 31906/java tcp6 0 0 :::8082 :::* LISTEN 31932/java tcp6 0 0 127.0.0.1:8005 :::* LISTEN 31812/java tcp6 0 0 :::8009 :::* LISTEN 31812/java将每个实例的网页进行区分 echo 8081 &gt;&gt;/application/tomcat8_1/webapps/ROOT/index.jsp echo 8082 &gt;&gt;/application/tomcat8_2/webapps/ROOT/index.jsp2、在浏览器访问，进行测试检查多实例的启动 ​ http://10.0.0.17:8082 http://10.0.0.17:8081 8、tomcat反向代理集群1、负载均衡器说明[root@lb01 ~]# cat /etc/redhat-release CentOS release 6.9 (Final) [root@lb01 ~]# uname -a Linux lb01 2.6.32-696.el6.x86_64 #1 SMP Tue Mar 21 19:29:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux [root@lb01 ~]# getenforce Disabled [root@lb01 ~]# /etc/init.d/iptables status iptables: Firewall is not running.负载均衡软件使用nginx，详情参照 http://www.cnblogs.com/clsn/p/7750615.html 2、配置负载均衡器备份原配置文件 mv /application/nginx/conf/nginx.conf{,.20171127} egrep -v &#39;#|^$&#39; /application/nginx/conf/nginx.conf.default &gt; /application/nginx/conf/nginx.conf​ 配置文件内容 [root@lb01 ~]# cat /application/nginx/conf/nginx.conf worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; upstream web_pools { server 10.0.0.17:8081; server 10.0.0.17:8082; } server { listen 80; server_name localhost; location / { root html; index index.jsp index.htm; proxy_pass http://web_pools; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } }​ 配置完成后重启nginx服务 /application/nginx/sbin/nginx -s stop /application/nginx/sbin/nginx3、使用命令进行访问测试使用curl 命令进行测试，tail进行关键字提取 [root@lb01 ~]# curl -s 10.0.0.5|tail -1 8081 [root@lb01 ~]# curl -s 10.0.0.5|tail -1 8082使用curl 命令进行测试，awk进行关键字提取 [root@lb01 ~]# curl -s 10.0.0.5|awk &#39;END{print}&#39; 8082 [root@lb01 ~]# curl -s 10.0.0.5|awk &#39;END{print}&#39; 8081 使用curl 命令进行测试，sed进行关键字提取 [root@lb01 ~]# curl -s 10.0.0.5|sed -n &#39;$p&#39; 8082 [root@lb01 ~]# curl -s 10.0.0.5|sed -n &#39;$p&#39; 80814、在浏览器上进行访问测试 ​ 建议使用google浏览器chrome 的隐身模式进行访问，使用ctrl+f5 进行强制刷新","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Tomcat","slug":"Tomcat","permalink":"https://cyylog.github.io/tags/Tomcat/"}]},{"title":"Tomcat_01_简介","slug":"Linux/Tomcat/Tomcat-01","date":"2020-03-20T16:35:55.000Z","updated":"2020-05-25T13:57:47.342Z","comments":true,"path":"2020/03/21/linux/tomcat/tomcat-01/","link":"","permalink":"https://cyylog.github.io/2020/03/21/linux/tomcat/tomcat-01/","excerpt":"","text":"1、Tomcat 简介Tomcat是Apache软件基金会（Apache Software Foundation）的Jakarta 项目中的一个核心项目，由Apache、Sun和其他一些公司及个人共同开发而成。 Tomcat服务器是一个免费的开放源代码的Web应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试JSP程序的首选。 Tomcat和Nginx、Apache(httpd)、lighttpd等Web服务器一样，具有处理HTML页面的功能，另外它还是一个Servlet和JSP容器，独立的Servlet容器是Tomcat的默认模式。不过，Tomcat处理静态HTML的能力不如Nginx/Apache服务器。 目前Tomcat最新版本为9.0。Java容器还有resin、weblogic等。 Tomcat**官网：** http://tomcat.apache.org 1、Tomcat好帮手—JDKJDK是 Java 语言的软件开发工具包，主要用于移动设备、嵌入式设备上的java应用程序。JDK是整个java开发的核心，它包含了JAVA的运行环境（JVM+Java系统类库）和JAVA工具。 JDK**包含了一批用于Java**开发的组件，其中包括： javac：编译器，将后缀名为.java的源代码编译成后缀名为“.class”的字节码 java：运行工具，运行.class的字节码 jar：打包工具，将相关的类文件打包成一个文件 javadoc：文档生成器，从源码注释中提取文档，注释需匹配规范 jdb debugger：调试工具 jps：显示当前java程序运行的进程状态 javap：反编译程序 appletviewer：运行和调试applet程序的工具，不需要使用浏览器 javah：从Java类生成C头文件和C源文件。这些文件提供了连接胶合，使Java和C代码可进行交互。 javaws：运行JNLP程序 extcheck：一个检测jar包冲突的工具 apt：注释处理工具 jhat：java堆分析工具 jstack：栈跟踪程序 jstat：JVM检测统计工具 jstatd：jstat守护进程 jinfo：获取正在运行或崩溃的java程序配置信息 jmap：获取java进程内存映射信息 idlj：IDL-to-Java编译器。将IDL语言转化为java文件 policytool：一个GUI的策略文件创建和管理工具 jrunscript：命令行脚本运行JDK中还包括完整的JRE（Java Runtime Environment），Java运行环境，也被称为private runtime。包括了用于产品环境的各种库类，如基础类库rt.jar，以及给开发人员使用的补充库，如国际化与本地化的类库、IDL库等等。 JDK中还包括各种样例程序，用以展示Java API中的各部分。 JDK**下载面页： **http://www.oracle.com/technetwork/java/javase/downloads/index.html ** 2、安装Tomcat &amp; JDK安装时候选择tomcat软件版本要与程序开发使用的版本一致。jdk版本要进行与tomcat保持一致。 1、系统环境说明[root@web03 ~]# cat /etc/redhat-release CentOS Linux release 7.4.1708 (Core) [root@web03 ~]# uname -a Linux web03 3.10.0-693.el7.x86_64 #1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux [root@web03 ~]# getenforce Disabled [root@web03 ~]# systemctl status firewalld.service ● firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled) Active: inactive (dead) Docs: man:firewalld(1)2 、安装JDK命令集： tar xf jdk-8u60-linux-x64.tar.gz -C /application/ ln -s /application/jdk1.8.0_60 /application/jdk # 设置环境变量 sed -i.ori &#39;$a export JAVA_HOME=/application/jdk\\nexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH\\nexport CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$JAVA_HOME/lib/tools.jar&#39; /etc/profile source /etc/profile测试jdk是否安装成功↓ [root@web03 ~]# java -version java version &quot;1.8.0_60&quot; Java(TM) SE Runtime Environment (build 1.8.0_60-b27) Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)3、安装Tomcat命令集： tar xf apache-tomcat-8.0.27.tar.gz -C /application/ ln -s /application/apache-tomcat-8.0.27 /application/tomcat # 设置环境变量 echo &#39;export TOMCAT_HOME=/application/tomcat&#39;&gt;&gt;/etc/profile source /etc/profile # 注意授权，统一权限 chown -R root.root /application/jdk/ /application/tomcat/检查tomcat是否安装成功 [root@web03 ~]# /application/tomcat/bin/version.sh Using CATALINA_BASE: /application/tomcat Using CATALINA_HOME: /application/tomcat Using CATALINA_TMPDIR: /application/tomcat/temp Using JRE_HOME: /application/jdk Using CLASSPATH: /application/tomcat/bin/bootstrap.jar:/application/tomcat/bin/tomcat-juli.jar Server version: Apache Tomcat/8.0.27 Server built: Sep 28 2015 08:17:25 UTC Server number: 8.0.27.0 OS Name: Linux OS Version: 3.10.0-693.el7.x86_64 Architecture: amd64 JVM Version: 1.8.0_60-b27 JVM Vendor: Oracle Corporation2、Tomcat目录介绍1、tomcat主目录介绍[root@web03 ~]# cd /application/tomcat/ [root@web03 tomcat]# tree -L 1 . ├── bin #存放tomcat管理脚本 ├── conf # tomcat 配置文件存放目录 ├── lib # web应用调用的jar包存放路径 ├── LICENSE ├── logs # tomcat 日志存放目录，catalina.out 为主要输出日志 ├── NOTICE ├── RELEASE-NOTES ├── RUNNING.txt ├── temp # 存放临时文件 ├── webapps # web程序存放目录 └── work # 存放编译产生的.java 与 .class文件 7 directories, 4 files2、webapps目录介绍[root@web03 tomcat]# cd webapps/ [root@web03 webapps]# tree -L 1 . ├── docs # tomcat 帮助文档 ├── examples # web应用实例 ├── host-manager # 主机管理 ├── manager # 管理 └── ROOT # 默认站点根目录 5 directories, 0 files3、Tomcat配置文件目录介绍（conf）[root@web03 conf]# tree -L 1 . ├── Catalina ├── catalina.policy ├── catalina.properties ├── context.xml ├── logging.properties ├── logs ├── server.xml # tomcat 主配置文件 ├── server.xml.bak ├── server.xml.bak2 ├── tomcat-users.xml # tomcat 管理用户配置文件 ├── tomcat-users.xsd └── web.xml 2 directories, 10 files4、Tomcat的管理# 启动程序/application/tomcat/bin/startup.sh # 关闭程序/application/tomcat/bin/shutdown.sh启动停止 [root@web03 ~]# /application/tomcat/bin/shutdown.sh Using CATALINA_BASE: /application/tomcat Using CATALINA_HOME: /application/tomcat Using CATALINA_TMPDIR: /application/tomcat/temp Using JRE_HOME: /application/jdk Using CLASSPATH: /application/tomcat/bin/bootstrap.jar:/application/tomcat/bin/tomcat-juli.jar [root@web03 ~]# /application/tomcat/bin/startup.sh Using CATALINA_BASE: /application/tomcat Using CATALINA_HOME: /application/tomcat Using CATALINA_TMPDIR: /application/tomcat/temp Using JRE_HOME: /application/jdk Using CLASSPATH: /application/tomcat/bin/bootstrap.jar:/application/tomcat/bin/tomcat-juli.jar Tomcat started.​ 注意：tomcat未启动的情况下使用shutdown脚本，会有大量的输出信息。 检查tomcat是否启动正常 [root@web03 ~]# netstat -lntup |grep java tcp6 0 0 :::8080 :::* LISTEN 30560/java tcp6 0 0 127.0.0.1:8005 :::* LISTEN 30560/java tcp6 0 0 :::8009 :::* LISTEN 30560/java 说明：所有与java相关的，服务启动都是java命名的进程 启动完成浏览器进行访问 http://10.0.0.17:8080/ 3、Tomcat日志说明1、查看日志[root@web03 ~]# tailf /application/tomcat/logs/catalina.out 24-Nov-2017 15:09:51.654 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler [&quot;http-nio-8080&quot;] 24-Nov-2017 15:09:51.665 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler [&quot;ajp-nio-8009&quot;] 24-Nov-2017 15:09:51.670 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 60037 ms​ 发现启动时间较长，其中有一项的启动时间占据了绝大多数 24-Nov-2017 15:09:50.629 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployWAR Deployment of web application archive /application/apache-tomcat-8.0.27/webapps/memtest.war has finished in 58,892 ms​ 发现耗时在这里：是session引起的随机数问题导致的。Tocmat的Session ID是通过SHA1算法计算得到的，计算Session ID的时候必须有一个密钥。为了提高安全性Tomcat在启动的时候会通过随机生成一个密钥。 2、解决Tomcat启动慢的方法Tomcat启动慢主要原因是生成随机数的时候卡住了,导致tomcat启动不了。 是否有足够的熵来用于产生随机数，可以通过如下命令来查看 [root@web03 ~]# cat /proc/sys/kernel/random/entropy_avail 6为了加速/dev/random提供随机数的速度，你可以通过操作设备的外设，让其产生大量的中断，网络传输数据，按键，移动鼠标，在命令行敲几个不同的命令，俗称聚气。 cat /dev/random 会消耗能量 方法1**：** vim $JAVA_HOME/jre/lib/security/java.security securerandom.source=file:/dev/random改为 securerandom.source=file:/dev/urandom方法2**：** vim $TOMCAT_HOME/bin/catalina.sh if [[ &quot;$JAVA_OPTS&quot; != *-Djava.security.egd=* ]]; then JAVA_OPTS=&quot;$JAVA_OPTS -Djava.security.egd=file:/dev/urandom&quot; fi这个系统属性egd表示熵收集守护进程(entropy gathering daemon) 方法3**：（推荐）** yum install rng-tools # 安装rngd服务（熵服务，增大熵池） systemctl start rngd # 启动服务4、Tomcat管理功能使用注意：测试功能，生产环境不要用 Tomcat管理功能用于对Tomcat自身以及部署在Tomcat上的应用进行管理的web应用。在默认情况下是处于禁用状态的。如果需要开启这个功能，就需要配置管理用户，即配置tomcat-users.xml 文件。 [root@web03 ~]# vim /application/tomcat/conf/tomcat-users.xml …… 39 &lt;role rolename=&quot;manager-gui&quot;/&gt; 40 &lt;role rolename=&quot;admin-gui&quot;/&gt; 41 &lt;user username=&quot;tomcat&quot; password=&quot;tomcat&quot; roles=&quot;manager-gui,admin-gui&quot;/&gt; 42 &lt;/tomcat-users&gt; # 在此行前加入上面三行未修改文件前进行访问 &lt;role rolename=&quot;manager-gui&quot;/&gt; &lt;user username=&quot;tomcat&quot; password=&quot;s3cret&quot; roles=&quot;manager-gui&quot;/&gt; &lt;role rolename=&quot;admin-gui&quot;/&gt; &lt;user username=&quot;tomcat&quot; password=&quot;s3cret&quot; roles=&quot;admin-gui&quot;/&gt;​ 从而得出上面的配置文件信息。 1、在web界面访问管理界面 ​ 输入之前配置的账户与密码即可 5、Tomcat主配置文件详解1、server.xml组件类别顶级组件：位于整个配置的顶层，如server。 容器类组件：可以包含其它组件的组件，如service、engine、host、context。 连接器组件：连接用户请求至tomcat，如connector。 被嵌套类组件：位于一个容器当中，不能包含其他组件，如Valve、logger。 &lt;server&gt; &lt;service&gt; &lt;connector /&gt; &lt;engine&gt; &lt;host&gt; &lt;context&gt;&lt;/context&gt; &lt;/host&gt; &lt;host&gt; &lt;context&gt;&lt;/context&gt; &lt;/host&gt; &lt;/engine&gt; &lt;/service&gt; &lt;/server&gt;2、组件介绍 组件名称 功能介绍 engine 核心容器组件，catalina引擎，负责通过connector接收用户请求，并处理请求，将请求转至对应的虚拟主机host。 host 类似于httpd中的虚拟主机，一般而言支持基于FQDN的虚拟主机。 context 定义一个应用程序，是一个最内层的容器类组件（不能再嵌套）。配置context的主要目的指定对应对的webapp的根目录，类似于httpd的alias，其还能为webapp指定额外的属性，如部署方式等。 connector 接收用户请求，类似于httpd的listen配置监听端口的。 service**（服务）** 将connector关联至engine，因此一个service内部可以有多个connector，但只能有一个引擎engine。service内部有两个connector，一个engine。因此，一般情况下一个server内部只有一个service，一个service内部只有一个engine，但一个service内部可以有多个connector。 server 表示一个运行于JVM中的tomcat实例。 Valve 阀门，拦截请求并在将其转至对应的webapp前进行某种处理操作，可以用于任何容器中，比如记录日志(access log valve)、基于IP做访问控制(remote address filter valve)。 logger 日志记录器，用于记录组件内部的状态信息，可以用于除context外的任何容器中。 realm 可以用于任意容器类的组件中，关联一个用户认证库，实现认证和授权。可以关联的认证库有两种：UserDatabaseRealm、MemoryRealm和JDBCRealm。 UserDatabaseRealm 使用JNDI自定义的用户认证库。 MemoryRealm 认证信息定义在tomcat-users.xml中。 JDBCRealm 认证信息定义在数据库中，并通过JDBC连接至数据库中查找认证用户。 3、server.xml配置文件注释&lt;?xml version=&#39;1.0&#39; encoding=&#39;utf-8&#39;?&gt; &lt;!-- &lt;Server&gt;元素代表整个容器,是Tomcat实例的顶层元素.由org.apache.catalina.Server接口来定义.它包含一个&lt;Service&gt;元素.并且它不能做为任何元素的子元素. port指定Tomcat监听shutdown命令端口.终止服务器运行时,必须在Tomcat服务器所在的机器上发出shutdown命令.该属性是必须的. shutdown指定终止Tomcat服务器运行时,发给Tomcat服务器的shutdown监听端口的字符串.该属性必须设置 --&gt; &lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt; &lt;Listener className=&quot;org.apache.catalina.startup.VersionLoggerListener&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.core.AprLifecycleListener&quot; SSLEngine=&quot;on&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.core.JreMemoryLeakPreventionListener&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.mbeans.GlobalResourcesLifecycleListener&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.core.ThreadLocalLeakPreventionListener&quot; /&gt; &lt;GlobalNamingResources&gt; &lt;Resource name=&quot;UserDatabase&quot; auth=&quot;Container&quot; type=&quot;org.apache.catalina.UserDatabase&quot; description=&quot;User database that can be updated and saved&quot; factory=&quot;org.apache.catalina.users.MemoryUserDatabaseFactory&quot; pathname=&quot;conf/tomcat-users.xml&quot; /&gt; &lt;/GlobalNamingResources&gt; &lt;!--service服务组件--&gt; &lt;Service name=&quot;Catalina&quot;&gt; &lt;!-- Connector主要参数说明（见下表） --&gt; &lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; &lt;Connector port=&quot;8009&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt; &lt;!--engine,核心容器组件,catalina引擎,负责通过connector接收用户请求,并处理请求,将请求转至对应的虚拟主机host defaultHost指定缺省的处理请求的主机名，它至少与其中的一个host元素的name属性值是一样的 --&gt; &lt;Engine name=&quot;Catalina&quot; defaultHost=&quot;localhost&quot;&gt; &lt;!--Realm表示存放用户名，密码及role的数据库--&gt; &lt;Realm className=&quot;org.apache.catalina.realm.LockOutRealm&quot;&gt; &lt;Realm className=&quot;org.apache.catalina.realm.UserDatabaseRealm&quot; resourceName=&quot;UserDatabase&quot;/&gt; &lt;/Realm&gt; &lt;!-- 详情常见下表（host参数详解）--&gt; &lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;&gt; &lt;!-- 详情常见下表（Context参数说明 ）--&gt; &lt;Context path=&quot;&quot; docBase=&quot;&quot; debug=&quot;&quot;/&gt; &lt;Valve className=&quot;org.apache.catalina.valves.AccessLogValve&quot; directory=&quot;logs&quot; prefix=&quot;localhost_access_log&quot; suffix=&quot;.txt&quot; pattern=&quot;%h %l %u %t &amp;quot;%r&amp;quot; %s %b&quot; /&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt; &lt;/Server&gt;4、Connector主要参数说明 参数 参数说明 connector 接收用户请求，类似于httpd的listen配置监听端口. port 指定服务器端要创建的端口号，并在这个端口监听来自客户端的请求。 address 指定连接器监听的地址，默认为所有地址（即0.0.0.0） protocol 连接器使用的协议，支持HTTP和AJP。AJP（Apache Jserv Protocol）专用于tomcat与apache建立通信的， 在httpd反向代理用户请求至tomcat时使用（可见Nginx反向代理时不可用AJP协议）。 minProcessors 服务器启动时创建的处理请求的线程数 maxProcessors 最大可以创建的处理请求的线程数 enableLookups 如果为true，则可以通过调用request.getRemoteHost()进行DNS查询来得到远程客户端的实际主机名，若为false则不进行DNS查询，而是返回其ip地址 redirectPort 指定服务器正在处理http请求时收到了一个SSL传输请求后重定向的端口号 acceptCount 指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理 connectionTimeout 指定超时的时间数(以毫秒为单位) 5、host参数详解 参数 参数说明 host 表示一个虚拟主机 name 指定主机名 appBase 应用程序基本目录，即存放应用程序的目录.一般为appBase=”webapps”，相对于CATALINA_HOME而言的，也可以写绝对路径。 unpackWARs 如果为true，则tomcat会自动将WAR文件解压，否则不解压，直接从WAR文件中运行应用程序 autoDeploy 在tomcat启动时，是否自动部署。 xmlValidation 是否启动xml的校验功能，一般xmlValidation=”false”。 xmlNamespaceAware 检测名称空间，一般xmlNamespaceAware=”false”。 6、Context参数说明 参数 参数说明 Context 表示一个web应用程序，通常为WAR文件 docBase 应用程序的路径或者是WAR文件存放的路径,也可以使用相对路径，起始路径为此Context所属Host中appBase定义的路径。 path 表示此web应用程序的url的前缀，这样请求的url为http://localhost:8080/path/**** reloadable 这个属性非常重要，如果为true，则tomcat会自动检测应用程序的/WEB-INF/lib和/WEB-INF/classes目录的变化，自动装载新的应用程序，可以在不重启tomcat的情况下改变应用程序","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Tomcat","slug":"Tomcat","permalink":"https://cyylog.github.io/tags/Tomcat/"}]},{"title":"xshell 配置","slug":"[xshell突出显示集]","date":"2020-03-20T16:24:11.000Z","updated":"2020-10-01T14:25:51.889Z","comments":true,"path":"2020/03/21/xshell-tu-chu-xian-shi-ji/","link":"","permalink":"https://cyylog.github.io/2020/03/21/xshell-tu-chu-xian-shi-ji/","excerpt":"","text":"[xshell配色方案][cyylog] text=00ff80 cyan(bold)=00ffff text(bold)=e9e9e9 magenta=c000c0 green=80ff00 green(bold)=3c5a38 background=042028 cyan=00c0c0 red(bold)=ff0000 yellow=c0c000 magenta(bold)=ff00ff yellow(bold)=ffff00 red=ff4500 white=c0c0c0 blue(bold)=1e90ff white(bold)=fdf6e3 black=000000 blue=00bfff black(bold)=808080 [Names] name0=cyylog count=1 ！！！ 请自行创建 xcs 后缀的文件[xshell突出显示集]xshell突出显示集（参考mobaxterm，直接拷贝过来不行，应该是xshell对正则表达式的支持不够好）:Underline: \\b(http(s)?://[A-Za-z0-9_./&?=%~#{}()@+-]+)\\b Red: (\\b((bad|wrong|incorrect|improper|invalid|unsupported|bad)( file| memory)? (descriptor|alloc(ation)?|addr(ess)?|owner(ship)?|arg(ument)?|param(eter)?|setting|length|filename)|not properly|improperly|(operation |connection |authentication |access |permission )?(false|no|ko|denied|disallowed|not allowed|refused|problem|failed|failure|not permitted)|no [A-Za-z]+( [A-Za-z]+)? found|invalid|unsupported|not supported|seg(mentation )?fault|corruption|corrupted|corrupt|overflow|underrun|not ok|unimplemented|unsuccessfull|not implemented|permerrors?|fehlers?|errore|errors?|erreurs?|fejl|virhe|greška|erro|fel|\\(ee\\)|\\(ni\\))\\b) Green: (\\b(true|yes|ok|accepted|allowed|enabled|connected|erfolgreich|exitoso|successo|sucedido|framgångsrik|successfully|successful|succeeded|success)\\b) Yellow: (\\b(\\[\\-w[A-Za-z-]+\\]|caught signal [0-9]+|cannot|(connection (to (remote host|[a-z0-9.]+) )?)?(closed|terminated|stopped|not responding)|exited|no more [A-Za-z] available|unexpected|(command |binary |file )?not found|(o)+ps|out of (space|memory)|low (memory|disk)|unknown|disabled|disconnected|deprecated|refused|disconnect(ion)?|advertencia|avvertimento|attention|warnings?|achtung|exclamation|alerts?|warnungs?|advarsel|pedwarn|aviso|varoitus|upozorenje|peringatan|uyari|varning|avertissement|\\(ww\\)|\\(\\?\\?\\)|could not|unable to)\\b)shellMagenta: (\\b(localhost|([1-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-4])\\.[0-9]+\\.[0-9]+\\.[0-9]+|null|none)\\b) Cyan: (\\b(last (failed )?login:|launching|checking|loading|creating|building|important|booting|starting|notice|informational|informationen|informazioni|informação|oplysninger|informations?|info|información|informasi|note|\\(ii\\)|\\(\\!\\!\\))\\b)","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Tools","slug":"Tools","permalink":"https://cyylog.github.io/tags/Tools/"}]},{"title":"kubeadm快速部署kubernetes集群","slug":"容器/kubeadm快速部署kubernetes集群","date":"2020-03-16T16:14:19.000Z","updated":"2020-05-25T13:55:58.174Z","comments":true,"path":"2020/03/17/rong-qi/kubeadm-kuai-su-bu-shu-kubernetes-ji-qun/","link":"","permalink":"https://cyylog.github.io/2020/03/17/rong-qi/kubeadm-kuai-su-bu-shu-kubernetes-ji-qun/","excerpt":"","text":"kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。 这个工具能通过两条指令完成一个kubernetes集群的部署： # 创建一个 Master 节点 $ kubeadm init # 将一个 Node 节点加入到当前集群中 $ kubeadm join &lt;Master节点的IP和端口 &gt;1. 安装要求在开始之前，部署Kubernetes集群机器需要满足以下几个条件： 一台或多台机器，操作系统 CentOS7.x-86_x64 硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像 禁止swap分区 2. 学习目标 在所有节点上安装Docker和kubeadm 部署Kubernetes Master 部署容器网络插件 部署 Kubernetes Node，将节点加入Kubernetes集群中 部署Dashboard Web页面，可视化查看Kubernetes资源 3. 准备环境 Kubernetes 架构图 Kubernetes 架构图 关闭防火墙： $ systemctl stop firewalld $ systemctl disable firewalld 关闭selinux： $ sed -i &#39;s/enforcing/disabled/&#39; /etc/selinux/config $ setenforce 0 关闭swap： $ swapoff -a $ 临时 $ vim /etc/fstab $ 永久 添加主机名与IP对应关系（记得设置主机名）： $ cat /etc/hosts 192.168.31.63 k8s-master 192.168.31.65 k8s-node1 192.168.31.66 k8s-node2 将桥接的IPv4流量传递到iptables的链： $ cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl --system4. 所有节点安装Docker/kubeadm/kubeletKubernetes默认CRI（容器运行时）为Docker，因此先安装Docker。 4.1 安装Docker$ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo $ yum -y install docker-ce-18.06.1.ce-3.el7 $ systemctl enable docker &amp;&amp; systemctl start docker $ docker --version Docker version 18.06.1-ce, build e68fc7a4.2 添加阿里云YUM软件源$ cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF4.3 安装kubeadm，kubelet和kubectl由于版本更新频繁，这里指定版本号部署： $ yum install -y kubelet-1.14.0 kubeadm-1.14.0 kubectl-1.14.0 $ systemctl enable kubelet5. 部署Kubernetes Master在192.168.31.63（Master）执行。 $ kubeadm init \\ --apiserver-advertise-address=192.168.31.63 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.14.0 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。 使用kubectl工具： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config $ kubectl get nodes6. 安装Pod网络插件（CNI）$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml确保能够访问到quay.io这个registery。 master执行 # kubectl get pods -n kube-system 7. 加入Kubernetes Node在192.168.31.65/66（Node）执行。 向集群添加新节点，执行在kubeadm init输出的kubeadm join命令： $ kubeadm join 192.168.31.63:6443 --token l79g5t.6ov4jkddwqki1dxe --discovery-token-ca-cert-hash sha256:4f07f9068c543130461c9db368d62b4aabc22105451057f887defa35f47fa0768. 测试kubernetes集群在Kubernetes集群中创建一个pod，验证是否正常运行： $ kubectl create deployment nginx --image=nginx $ kubectl expose deployment nginx --port=80 --type=NodePort $ kubectl get pod,svc $ kubectl get pod,svc -o wide访问地址：http://NodeIP:Port 9. 部署 Dashboard$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml 镜像下载因为网络的原因： 镜像难以下载，需要修改以下两个地方 image: tigerfive/kubernetes-dashboard-amd64:v1.10.1 spec: type: NodePort ports: - port: 443 targetPort: 8443 默认Dashboard只能集群内部访问，修改Service为NodePort类型，暴露到外部： kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard $ kubectl apply -f kubernetes-dashboard.yaml访问地址：http://NodeIP:30001 创建service account并绑定默认cluster-admin管理员集群角色： $ kubectl create serviceaccount dashboard-admin -n kube-system $ kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin $ kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk &#39;/dashboard-admin/{print $1}&#39;)使用输出的token登录Dashboard。","categories":[{"name":"容器","slug":"容器","permalink":"https://cyylog.github.io/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://cyylog.github.io/tags/Kubernetes/"}]},{"title":"Kubernetes-入门","slug":"容器/Kubernetes入门","date":"2020-03-14T12:24:13.000Z","updated":"2020-10-30T04:13:22.164Z","comments":true,"path":"2020/03/14/rong-qi/kubernetes-ru-men/","link":"","permalink":"https://cyylog.github.io/2020/03/14/rong-qi/kubernetes-ru-men/","excerpt":"","text":"Kubernetes功能​ 官方定义k8s能够对容器化软件进行部署管理，在不停机的前提下提供简单快速的发布和更新方式。换句话说，如果项目需要多机器节点的微服务架构，并且采用Docker image（镜像）进行容器化部署，那么k8s可以帮助我们屏蔽掉集群的复杂性，自动选择最优资源分配方式进行部署。在此基础上，k8s还提供简单的多实例部署及更新方案，仅需几个操作命令就可以轻松实现。 k8s集群简单介绍 Master 负责管理集群 负责协调集群中的所有活动，例如调度应用程序，维护应用程序的状态，扩展和更新应用程序。 Worker节点(即图中的Node)是VM(虚拟机)或物理计算机，充当k8s集群中的工作计算机。 每个Worker节点都有一个Kubelet，它管理该Worker节点并负责与Master节点通信。该Worker节点还应具有用于处理容器操作的工具，例如Docker。 1.部署一个应用程序前提已经 完成 Kubernetes 集群的安装，请参考文档 安装 Kubernetes 单Master节点 目标 使用 kubectl 在 k8s 上部署第一个应用程序。 TIP kubectl 是 k8s 的客户端工具，可以使用命令行管理集群。 如果参考文档 安装 Kubernetes 单Master节点，您可以在 master 节点的 root 用户使用 kubectl 操作您的集群 您也可以尝试 从客户端电脑远程管理 Kubernetes Kubernetes 部署在 k8s 上进行部署前，首先需要了解一个基本概念 Deployment Deployment 译名为 部署。在k8s中，通过发布 Deployment，可以创建应用程序 (docker image) 的实例 (docker container)，这个实例会被包含在称为 Pod 的概念中，Pod 是 k8s 中最小可管理单元。 在 k8s 集群中发布 Deployment 后，Deployment 将指示 k8s 如何创建和更新应用程序的实例，master 节点将应用程序实例调度到集群中的具体的节点上。 创建应用程序实例后，Kubernetes Deployment Controller 会持续监控这些实例。如果运行实例的 worker 节点关机或被删除，则 Kubernetes Deployment Controller 将在群集中资源最优的另一个 worker 节点上重新创建一个新的实例。这提供了一种自我修复机制来解决机器故障或维护问题。 在容器编排之前的时代，各种安装脚本通常用于启动应用程序，但是不能够使应用程序从机器故障中恢复。通过创建应用程序实例并确保它们在集群节点中的运行实例个数，Kubernetes Deployment 提供了一种完全不同的方式来管理应用程序。 在 Kubernetes 上部署第一个应用程序 ​ Deployment 处于 master 节点上，通过发布 Deployment，master 节点会选择合适的 worker 节点创建 Container（即图中的正方体），Container 会被包含在 Pod （即蓝色圆圈）里。 部署 nginx Deployment创建 YAML 文件 创建文件 nginx-deployment.yaml，内容如下： apiVersion: apps/v1 #与k8s集群版本有关，使用 kubectl api-versions 即可查看当前集群支持的版本 kind: Deployment #该配置的类型，我们使用的是 Deployment metadata: #译名为元数据，即 Deployment 的一些基本属性和信息 name: nginx-deployment #Deployment 的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组，目前不需要理解 app: nginx #为该Deployment设置key为app，value为nginx的标签 spec: #这是关于该Deployment的描述，可以理解为你期待该Deployment在k8s中如何使用 replicas: 1 #使用该Deployment创建一个应用程序实例 selector: #标签选择器，与上面的标签共同作用，目前不需要理解 matchLabels: #选择包含标签app:nginx的资源 app: nginx template: #这是选择或创建的Pod的模板 metadata: #Pod的元数据 labels: #Pod的标签，上面的selector即选择包含标签app:nginx的Pod app: nginx spec: #期望Pod实现的功能（即在pod中部署） containers: #生成container，与docker中的container是同一种 - name: nginx #container的名称 image: nginx:1.7.9 #使用镜像nginx:1.7.9创建container，该container默认80端口可访问 应用 YAML 文件 kubectl apply -f nginx-deployment.yaml 查看部署结果 # 查看 Deployment kubectl get deployments # 查看 Pod kubectl get pods可分别查看到一个名为 nginx-deployment 的 Deployment 和一个名为 nginx-deployment-xxxxxxx 的 Pod 2.查看Pods/NodesKubernetes Pods在 部署第一个应用程序 中创建 Deployment 后，k8s创建了一个 Pod（容器组） 来放置应用程序实例（container 容器）。 Pods概述 Pod 容器组 是一个k8s中一个抽象的概念，用于存放一组 container（可包含一个或多个 container 容器，即图上正方体)，以及这些 container （容器）的一些共享资源。这些资源包括： 共享存储，称为卷(Volumes)，即图上紫色圆柱 网络，每个 Pod（容器组）在集群中有个唯一的 IP，pod（容器组）中的 container（容器）共享该IP地址 container（容器）的基本信息，例如容器的镜像版本，对外暴露的端口等 例如，Pod可能既包含带有Node.js应用程序的 container 容器，也包含另一个非 Node.js 的 container 容器，用于提供 Node.js webserver 要发布的数据。Pod中的容器共享 IP 地址和端口空间（同一 Pod 中的不同 container 端口不能相互冲突），始终位于同一位置并共同调度，并在同一节点上的共享上下文中运行。（同一个Pod内的容器可以使用 localhost + 端口号互相访问）。 Pod（容器组）是 k8s 集群上的最基本的单元。当我们在 k8s 上创建 Deployment 时，会在集群上创建包含容器的 Pod (而不是直接创建容器)。每个Pod都与运行它的 worker 节点（Node）绑定，并保持在那里直到终止或被删除。如果节点（Node）发生故障，则会在群集中的其他可用节点（Node）上运行相同的 Pod（从同样的镜像创建 Container，使用同样的配置，IP 地址不同，Pod 名字 TIP 重要： Pod 是一组容器（可包含一个或多个应用程序容器），以及共享存储（卷 Volumes）、IP 地址和有关如何运行容器的信息。 如果多个容器紧密耦合并且需要共享磁盘等资源，则他们应该被部署在同一个Pod（容器组）中。 Node（节点）下图显示一个 Node（节点）上含有4个 Pod（容器组） Pod（容器组）总是在 Node（节点） 上运行。Node（节点）是 kubernetes 集群中的计算机，可以是虚拟机或物理机。每个 Node（节点）都由 master 管理。一个 Node（节点）可以有多个Pod（容器组），kubernetes master 会根据每个 Node（节点）上可用资源的情况，自动调度 Pod（容器组）到最佳的 Node（节点）上。 每个 Kubernetes Node（节点）至少运行： Kubelet，负责 master 节点和 worker 节点之间通信的进程；管理 Pod（容器组）和 Pod（容器组）内运行的 Container（容器）。 容器运行环境（如Docker）负责下载镜像、创建和运行容器等。 故障排除在部署第一个应用程序 中，我们使用了 kubectl 命令行界面部署了 nginx 并且查看了 Deployment 和 Pod。kubectl 还有如下四个常用命令，在我们排查问题时可以提供帮助： kubectl get - 显示资源列表 # kubectl get 资源类型 #获取类型为Deployment的资源列表 kubectl get deployments #获取类型为Pod的资源列表 kubectl get pods #获取类型为Node的资源列表 kubectl get nodes 名称空间 在命令后增加 -A 或 --all-namespaces 可查看所有 名称空间中 的对象，使用参数 -n 可查看指定名称空间的对象，例如 # 查看所有名称空间的 Deployment kubectl get deployments -A kubectl get deployments --all-namespaces # 查看 kube-system 名称空间的 Deployment kubectl get deployments -n kube-system 并非所有对象都在名称空间里) kubectl describe - 显示有关资源的详细信息 # kubectl describe 资源类型 资源名称 #查看名称为nginx-XXXXXX的Pod的信息 kubectl describe pod nginx-XXXXXX #查看名称为nginx的Deployment的信息 kubectl describe deployment nginx kubectl logs - 查看pod中的容器的打印日志（和命令docker logs 类似） # kubectl logs Pod名称 #查看名称为nginx-pod-XXXXXXX的Pod内的容器打印的日志 #本案例中的 nginx-pod 没有输出日志，所以您看到的结果是空的 kubectl logs -f nginx-pod-XXXXXXX 尝试在集群中执行一下上述的几个命令，可以了解如何通过 kubectl 操作 kubernetes 集群中的 Node、Pod、Container。 TIP Worker节点是k8s中的工作计算机，可能是VM或物理计算机，具体取决于群集。多个Pod可以在一个节点上运行。 3.公布应用程序Kubernetes Service（服务）概述事实上，Pod（容器组）有自己的 生命周期。当 worker node（节点）故障时，节点上运行的 Pod（容器组）也会消失。然后，Deployment 可以通过创建新的 Pod（容器组）来动态地将群集调整回原来的状态，以使应用程序保持运行。 举个例子，假设有一个图像处理后端程序，具有 3 个运行时副本。这 3 个副本是可以替换的（无状态应用），即使 Pod（容器组）消失并被重新创建，或者副本数由 3 增加到 5，前端系统也无需关注后端副本的变化。由于 Kubernetes 集群中每个 Pod（容器组）都有一个唯一的 IP 地址（即使是同一个 Node 上的不同 Pod），我们需要一种机制，为前端系统屏蔽后端系统的 Pod（容器组）在销毁、创建过程中所带来的 IP 地址的变化。 Kubernetes 中的 Service（服务） 提供了这样的一个抽象层，它选择具备某些特征的 Pod（容器组）并为它们定义一个访问方式。Service（服务）使 Pod（容器组）之间的相互依赖解耦（原本从一个 Pod 中访问另外一个 Pod，需要知道对方的 IP 地址）。一个 Service（服务）选定哪些 Pod（容器组） 通常由 LabelSelector(标签选择器) 来决定。 在创建Service的时候，通过设置配置文件中的 spec.type 字段的值，可以以不同方式向外部暴露应用程序： ClusterIP（默认） 在群集中的内部IP上公布服务，这种方式的 Service（服务）只在集群内部可以访问到 NodePort 使用 NAT 在集群中每个的同一端口上公布服务。这种方式下，可以通过访问集群中任意节点+端口号的方式访问服务 :。此时 ClusterIP 的访问方式仍然可用。 LoadBalancer 在云环境中（需要云供应商可以支持）创建一个集群外部的负载均衡器，并为使用该负载均衡器的 IP 地址作为服务的访问地址。此时 ClusterIP 和 NodePort 的访问方式仍然可用。 TIP Service是一个抽象层，它通过 LabelSelector 选择了一组 Pod（容器组），把这些 Pod 的指定端口公布到到集群外部，并支持负载均衡和服务发现。 公布 Pod 的端口以使其可访问 在多个 Pod 间实现负载均衡 使用 Label 和 LabelSelector 服务和标签下图中有两个服务Service A(黄色虚线)和Service B(蓝色虚线) Service A 将请求转发到 IP 为 10.10.10.1 的Pod上， Service B 将请求转发到 IP 为 10.10.10.2、10.10.10.3、10.10.10.4 的Pod上。 Service 将外部请求路由到一组 Pod 中，它提供了一个抽象层，使得 Kubernetes 可以在不影响服务调用者的情况下，动态调度容器组（在容器组失效后重新创建容器组，增加或者减少同一个 Deployment 对应容器组的数量等）。 Service使用 Labels、LabelSelector(标签和选择器) 匹配一组 Pod。Labels（标签）是附加到 Kubernetes 对象的键/值对，其用途有多种： 将 Kubernetes 对象（Node、Deployment、Pod、Service等）指派用于开发环境、测试环境或生产环境 嵌入版本标签，使用标签区别不同应用软件版本 使用标签对 Kubernetes 对象进行分类 下图体现了 Labels（标签）和 LabelSelector（标签选择器）之间的关联关系 Deployment B 含有 LabelSelector 为 app=B 通过此方式声明含有 app=B 标签的 Pod 与之关联 通过 Deployment B 创建的 Pod 包含标签为 app=B Service B 通过标签选择器 app=B 选择可以路由的 Pod abels（标签）可以在创建 Kubernetes 对象时附加上去，也可以在创建之后再附加上去。任何时候都可以修改一个 Kubernetes 对象的 Labels（标签） 练习：为 nginx Deployment 创建一个 Service 创建nginx的Deployment中定义了Labels，如下： metadata: #译名为元数据，即Deployment的一些基本属性和信息 name: nginx-deployment #Deployment的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组 app: nginx #为该Deployment设置key为app，value为nginx的标签 创建文件 nginx-service.yaml vim nginx-service.yaml 文件内容如下： apiVersion: v1 kind: Service metadata: name: nginx-service #Service 的名称 labels: #Service 自己的标签 app: nginx #为该 Service 设置 key 为 app，value 为 nginx 的标签 spec: #这是关于该 Service 的定义，描述了 Service 如何选择 Pod，如何被访问 selector: #标签选择器 app: nginx #选择包含标签 app:nginx 的 Pod ports: - name: nginx-port #端口的名字 protocol: TCP #协议类型 TCP/UDP port: 80 #集群内的其他容器组可通过 80 端口访问 Service nodePort: 32600 #通过任意节点的 32600 端口访问 Service targetPort: 80 #将请求转发到匹配 Pod 的 80 端口 type: NodePort #Serive的类型，ClusterIP/NodePort/LoaderBalancer 执行命令 kubectl apply -f nginx-service.yaml 检查执行结果 kubectl get services -o wide 访问服务 curl :32600 4.伸缩应用程序Scaling（伸缩）应用程序在前面，我们创建了一个 Deployment，然后通过 服务 提供访问 Pod 的方式。我们发布的 Deployment 只创建了一个 Pod 来运行我们的应用程序。当流量增加时，我们需要对应用程序进行伸缩操作以满足系统性能需求。 伸缩 的实现可以通过更改 nginx-deployment.yaml 文件中部署的 replicas（副本数）来完成 spec: replicas: 2 #使用该Deployment创建两个应用程序实例 Scaling（伸缩）概述下图中，Service A 只将访问流量转发到 IP 为 10.0.0.5 的Pod上 ​ 修改了 Deployment 的 replicas 为 4 后，Kubernetes 又为该 Deployment 创建了 3 新的 Pod，这 4 个 Pod 有相同的标签。因此Service A通过标签选择器与新的 Pod建立了对应关系，将访问流量通过负载均衡在 4 个 Pod 之间进行转发。 TIP 通过更改部署中的 replicas（副本数）来完成扩展 练习：将 nginx Deployment 扩容到 4 个副本 修改 nginx-deployment.yaml 文件 将 replicas 修改为 4 执行命令 kubectl apply -f nginx-deployment.yaml 查看结果 watch kubectl get pods -o wide 5.执行滚动更新更新应用程序户期望应用程序始终可用，为此开发者/运维者在更新应用程序时要分多次完成。在 Kubernetes 中，这是通过 Rolling Update 滚动更新完成的。Rolling Update滚动更新 通过使用新版本的 Pod 逐步替代旧版本的 Pod 来实现 Deployment 的更新，从而实现零停机。新的 Pod 将在具有可用资源的 Node（节点）上进行调度。 Kubernetes 更新多副本的 Deployment 的版本时，会逐步的创建新版本的 Pod，逐步的停止旧版本的 Pod，以便使应用一直处于可用状态。这个过程中，Service 能够监视 Pod 的状态，将流量始终转发到可用的 Pod 上。 在上一个模块中，我们学习了将应用程序 Scale Up（扩容）为多个实例，这是执行更新而不影响应用程序可用性的前提（如果只有 1 个实例那还玩啥）。默认情况下，Rolling Update 滚动更新 过程中，Kubernetes 逐个使用新版本 Pod 替换旧版本 Pod（最大不可用 Pod 数为 1、最大新建 Pod 数也为 1）。这两个参数可以配置为数字或百分比。在Kubernetes 中，更新是版本化的，任何部署更新都可以恢复为以前的（稳定）版本。 滚动更新概述 原本 Service A 将流量负载均衡到 4 个旧版本的 Pod （当中的容器为 绿色）上 更新完 Deployment 部署文件中的镜像版本后，master 节点选择了一个 worker 节点，并根据新的镜像版本创建 Pod（紫色容器）。新 Pod 拥有唯一的新的 IP。同时，master 节点选择一个旧版本的 Pod 将其移除。 此时，Service A 将新 Pod 纳入到负载均衡中，将旧Pod移除 同步骤2，再创建一个新的 Pod 替换一个原有的 Pod 如此 Rolling Update 滚动更新，直到所有旧版本 Pod 均移除，新版本 Pod 也达到 Deployment 部署文件中定义的副本数，则滚动更新完成 滚动更新允许以下操作： 将应用程序从准上线环境升级到生产环境（通过更新容器镜像） 回滚到以前的版本 持续集成和持续交付应用程序，无需停机 练习：更新 nginx Deployment修改 nginx-deployment.yaml 文件 修改文件中 image 镜像的标签，如下所示 执行命令 kubectl apply -f nginx-deployment.yaml 查看过程及结果 执行命令，可观察到 pod 逐个被替换的过程。 watch kubectl get pods -l app=nginx Kubernetes核心概念什么是Kubernetes？Kubernetes（k8s）是自动化容器操作的开源平台，这些操作包括部署，调度和节点集群间扩展。如果你曾经用过Docker容器技术部署容器，那么可以将Docker看成Kubernetes内部使用的低级别组件。Kubernetes不仅仅支持Docker，还支持Rocket，这是另一种容器技术。 使用Kubernetes可以： 自动化容器的部署和复制 随时扩展或收缩容器规模 将容器组织成组，并且提供容器间的负载均衡 很容易地升级应用程序容器的新版本 提供容器弹性，如果容器失效就替换它，等等… 集群集群是一组节点，这些节点可以是物理服务器或者虚拟机，之上安装了Kubernetes平台。下图展示这样的集群。注意该图为了强调核心概念有所简化。这里可以看到一个典型的Kubernetes架构图。 上图可以看到如下组件，使用特别的图标表示Service和Label： PodContainer（容器） Label())（标签） Replication Controller（复制控制器） Service（）（服务） Node（节点） Kubernetes Master（Kubernetes主节点） PodPod（上图绿色方框）安排在节点上，包含一组容器和卷。同一个Pod里的容器共享同一个网络命名空间，可以使用localhost互相通信。Pod是短暂的，不是持续性实体。你可能会有这些问题： 如果Pod是短暂的，那么我怎么才能持久化容器数据使其能够跨重启而存在呢？ 是的，Kubernetes支持 卷 的概念，因此可以使用持久化的卷类型。 是否手动创建Pod，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么？可以手动创建单个Pod，但是也可以使用Replication Controller使用Pod模板创建出多份拷贝，下文会详细介绍。 如果Pod是短暂的，那么重启时IP地址可能会改变，那么怎么才能从前端容器正确可靠地指向后台容器呢？这时可以使用Service，下文会详细介绍。 Label正如图所示，一些Pod有Label（）。一个Label是attach到Pod的一对键/值对，用来传递用户定义的属性。比如，你可能创建了一个”tier”和“app”标签，通过Label（tier=frontend, app=myapp）来标记前端Pod容器，使用Label（tier=backend, app=myapp）标记后台Pod。然后可以使用 Selectors 选择带有特定Label的Pod，并且将Service或者Replication Controller应用到上面。 Replication Controller是否手动创建Pod，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么，能否将Pods划到逻辑组里？ Replication Controller确保任意时间都有指定数量的Pod“副本”在运行。如果为某个Pod创建了Replication Controller并且指定3个副本，它会创建3个Pod，并且持续监控它们。如果某个Pod不响应，那么Replication Controller会替换它，保持总数为3.如下面的动画所示： 如果之前不响应的Pod恢复了，现在就有4个Pod了，那么Replication Controller会将其中一个终止保持总数为3。如果在运行中将副本总数改为5，Replication Controller会立刻启动2个新Pod，保证总数为5。还可以按照这样的方式缩小Pod，这个特性在执行滚动 升级 时很有用。 当创建Replication Controller时，需要指定两个东西： Pod模板：用来创建Pod副本的模板 Label：Replication Controller需要监控的Pod的标签。现在已经创建了Pod的一些副本，那么在这些副本上如何均衡负载呢？我们需要的是Service。 TIP 最新 Kubernetes 版本里，推荐使用 Deployment Service如果Pods是短暂的，那么重启时IP地址可能会改变，怎么才能从前端容器正确可靠地指向后台容器呢？ Service 抽象 现在，假定有2个后台Pod，并且定义后台Service的名称为‘backend-service’，label选择器为(tier=backend, app=myapp) 的Service会完成如下两件重要的事情： 会为Service创建一个本地集群的DNS入口，因此前端Pod只需要DNS查找主机名为 ‘backend-service’，就能够解析出前端应用程序可用的IP地址。 现在前端已经得到了后台服务的IP地址，但是它应该访问2个后台Pod的哪一个呢？Service在这2个后台Pod之间提供透明的负载均衡，会将请求分发给其中的任意一个（如下面的动画所示）。通过每个Node上运行的代理（kube-proxy）完成。 下述动画展示了Service的功能。注意该图作了很多简化。如果不进入网络配置，那么达到透明的负载均衡目标所涉及的底层网络和路由相对先进。如果有兴趣，有更深入的介绍。 每个节点都运行如下Kubernetes关键组件： Kubelet：是主节点代理。 Kube-proxy：Service使用其将链接路由到Pod，如上文所述。 Docker或Rocket：Kubernetes使用的容器技术来创建容器。 Kubernetes Master集群拥有一个Kubernetes Master（紫色方框）。Kubernetes Master提供集群的独特视角，并且拥有一系列组件，比如Kubernetes API Server。API Server提供可以用来和集群交互的REST端点。master节点包括用来创建和复制Pod的Replication Controller。","categories":[{"name":"容器","slug":"容器","permalink":"https://cyylog.github.io/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://cyylog.github.io/tags/Kubernetes/"}]},{"title":"Docker数据共享与持久化","slug":"容器/Docker数据共享与持久化","date":"2020-01-19T10:11:44.000Z","updated":"2020-06-19T10:36:42.162Z","comments":true,"path":"2020/01/19/rong-qi/docker-shu-ju-gong-xiang-yu-chi-jiu-hua/","link":"","permalink":"https://cyylog.github.io/2020/01/19/rong-qi/docker-shu-ju-gong-xiang-yu-chi-jiu-hua/","excerpt":"","text":"本文介绍如何在 Docker 内部以及容器之间管理数据，在容器中管理数据主要有两种方式： 数据卷（Data Volumes） 挂载主机目录 (Bind mounts) 数据卷数据卷是一个可供一个或多个容器使用的特殊目录，它绕过UFS，可以提供很多有用的特性： 数据卷 可以在容器之间共享和重用 对 数据卷 的修改会立马生效 对 数据卷 的更新，不会影响镜像 数据卷 默认会一直存在，即使容器被删除 注意：数据卷 的使用，类似于 Linux 下对目录或文件进行 mount，镜像中的被指定为挂载点的目录中的文件会隐藏掉，能显示看的是挂载的 数据卷。 选择 -v 还是 -–mount 参数： Docker 新用户应该选择--mount参数，经验丰富的 Docker 使用者对-v或者 --volume已经很熟悉了，但是推荐使用--mount参数。 创建一个数据卷： $ docker volume create my-vol 查看所有的 数据卷： $ docker volume ls local my-vol 在主机里使用以下命令可以查看指定 数据卷 的信息 $ docker volume inspect my-vol [ { \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\", \"Name\": \"my-vol\", \"Options\": {}, \"Scope\": \"local\" } ] 启动一个挂载数据卷的容器：在用docker run命令的时候，使用--mount标记来将 数据卷 挂载到容器里。在一次docker run中可以挂载多个 数据卷。下面创建一个名为 web 的容器，并加载一个 数据卷 到容器的 /webapp 目录。 $ docker run -d -P \\ --name web \\ # -v my-vol:/wepapp \\ --mount source=my-vol,target=/webapp \\ training/webapp \\ python app.py 查看数据卷的具体信息：在主机里使用以下命令可以查看 web 容器的信息 $ docker inspect web ... \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"my-vol\", \"Source\": \"/var/lib/docker/volumes/my-vol/_data\", \"Destination\": \"/app\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" } ], ... 删除数据卷： $ docker volume rm my-vol数据卷 是被设计用来持久化数据的，它的生命周期独立于容器，Docker 不会在容器被删除后自动删除 数据卷，并且也不存在垃圾回收这样的机制来处理没有任何容器引用的 数据卷。如果需要在删除容器的同时移除数据卷。可以在删除容器的时候使用docker rm -v这个命令。 无主的数据卷可能会占据很多空间，要清理请使用以下命令 $ docker volume prune挂载主机目录选择 -v 还是 -–mount 参数： Docker 新用户应该选择 –mount 参数，经验丰富的 Docker 使用者对 -v 或者 –volume 已经很熟悉了，但是推荐使用 –mount 参数。 挂载一个主机目录作为数据卷：使用 --mount 标记可以指定挂载一个本地主机的目录到容器中去。 $ docker run -d -P \\ --name web \\ # -v /src/webapp:/opt/webapp \\ --mount type=bind,source=/src/webapp,target=/opt/webapp \\ training/webapp \\ python app.py 上面的命令加载主机的 /src/webapp 目录到容器的 /opt/webapp目录。这个功能在进行测试的时候十分方便，比如用户可以放置一些程序到本地目录中，来查看容器是否正常工作。本地目录的路径必须是绝对路径，以前使用 -v 参数时如果本地目录不存在 Docker 会自动为你创建一个文件夹，现在使用 –mount 参数时如果本地目录不存在，Docker 会报错。 Docker 挂载主机目录的默认权限是 读写，用户也可以通过增加readonly指定为 只读。 $ docker run -d -P \\ --name web \\ # -v /src/webapp:/opt/webapp:ro \\ --mount type=bind,source=/src/webapp,target=/opt/webapp,readonly \\ training/webapp \\ python app.py 加了readonly之后，就挂载为 只读 了。如果你在容器内 /opt/webapp 目录新建文件，会显示如下错误: /opt/webapp # touch new.txt touch: new.txt: Read-only file system 查看数据卷的具体信息：在主机里使用以下命令可以查看 web 容器的信息 $ docker inspect web ... \"Mounts\": [ { \"Type\": \"bind\", \"Source\": \"/src/webapp\", \"Destination\": \"/opt/webapp\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\" } ], 挂载一个本地主机文件作为数据卷：--mount标记也可以从主机挂载单个文件到容器中 $ docker run --rm -it \\ # -v $HOME/.bash_history:/root/.bash_history \\ --mount type=bind,source=$HOME/.bash_history,target=/root/.bash_history \\ ubuntu:17.10 \\ bash root@2affd44b4667:/# history 1 ls 2 diskutil list 这样就可以记录在容器输入过的命令了。","categories":[{"name":"容器","slug":"容器","permalink":"https://cyylog.github.io/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://cyylog.github.io/tags/Docker/"}]},{"title":"zabbix-监控项","slug":"监控/zabbix/Zabbix监控项","date":"2020-01-04T17:55:39.000Z","updated":"2020-10-30T04:11:52.454Z","comments":true,"path":"2020/01/05/jian-kong/zabbix/zabbix-jian-kong-xiang/","link":"","permalink":"https://cyylog.github.io/2020/01/05/jian-kong/zabbix/zabbix-jian-kong-xiang/","excerpt":"","text":"Zabbix监控什么？ 监控项 Zabbix常用监控项zabbix自带的常用监控项 agent.ping 检测客户端可达性、返回nothing表示不可达。1表示可达 system.cpu.load --检测cpu负载。返回浮点数 system.cpu.util -- 检测cpu使用率。返回浮点数 vfs.dev.read -- 检测硬盘读取数据，返回是sps.ops.bps浮点类型，需要定义1024倍 vfs.dev.write -- 检测硬盘写入数据。返回是sps.ops.bps浮点类型，需要定义1024倍 net.if.out[br0] --检测网卡流速、流出方向，时间间隔为60S net-if-in[br0] --检测网卡流速，流入方向（单位：字节） 时间间隔60S proc.num[] 目前系统中的进程总数，时间间隔60s proc.num[,,run] 目前正在运行的进程总数，时间间隔60S处理器信息通过zabbix_get 获取负载值 合理的控制用户态、系统态、IO等待时间剋保证进程高效率的运行 系统态运行时间较高说明进程进行系统调用的次数比较多，一般的程序如果系统态运行时间占用过高就需要优化程序，减少系统调用 io等待时间过高则表明硬盘的io性能差，如果是读写文件比较频繁、读写效率要求比较高，可以考虑更换硬盘，或者使用多磁盘做raid的方案 system.cpu.swtiches --cpu的进程上下文切换，单位sps，表示每秒采样次数，api中参数history需指定为3 system.cpu.intr --cpu中断数量、api中参数history需指定为3 system.cpu.load[percpu,avg1] --cpu每分钟的负载值，按照核数做平均值(Processor load (1 min average per core))，api中参数history需指定为0 system.cpu.load[percpu,avg5] --cpu每5分钟的负载值，按照核数做平均值(Processor load (5 min average per core))，api中参数history需指定为0 system.cpu.load[percpu,avg15] --cpu每5分钟的负载值，按照核数做平均值(Processor load (15 min average per core))，api中参数history需指定为0zabbix的自定义常用项内存相关vim /usr/local/zabbix/etc/zabbix_agentd.conf.d/catcarm.conf UserParameter=ram.info[*],/bin/cat /proc/meminfo |awk &#39;/^$1:{print $2}&#39; ram.info[Cached] --检测内存的缓存使用量、返回整数，需要定义1024倍 ram.info[MemFree] --检测内存的空余量，返回整数，需要定义1024倍 ram.info[Buffers] --检测内存的使用量，返回整数，需要定义1024倍TCP相关的自定义项vim /usr/local/zabbix/share/zabbix/alertscripts/tcp_connection.sh #!/bin/bash function ESTAB { /usr/sbin/ss -ant |awk &#39;{++s[$1]} END {for(k in s) print k,s[k]}&#39; | grep &#39;ESTAB&#39; | awk &#39;{print $2}&#39; } function TIMEWAIT { /usr/sbin/ss -ant | awk &#39;{++s[$1]} END {for(k in s) print k,s[k]}&#39; | grep &#39;TIME-WAIT&#39; | awk &#39;{print $2}&#39; } function LISTEN { /usr/sbin/ss -ant | awk &#39;{++s[$1]} END {for(k in s) print k,s[k]}&#39; | grep &#39;LISTEN&#39; | awk &#39;{print $2}&#39; } $1 vim /usr/local/zabbix/etc/zabbix_agentd.conf.d/cattcp.conf UserParameter=tcp[*],/usr/local/zabbix/share/zabbix/alertscripts/tcp_connection.sh $1 tcp[TIMEWAIT] --检测TCP的驻留数，返回整数 tcp[ESTAB] --检测tcp的连接数、返回整数 tcp[LISTEN] --检测TCP的监听数，返回整数nginx相关的自定义项vim /etc/nginx/conf.d/default.conf location /nginx-status { stub_status on; access_log off; allow 127.0.0.1; deny all; } vim /usr/local/zabbix/etc/zabbix_agentd.conf.d/nginx.conf UserParameter=Nginx.active,/usr/bin/curl -s &quot;http://127.0.0.1:80/nginx-status&quot; | awk &#39;/Active/ {print $NF}&#39; UserParameter=Nginx.read,/usr/bin/curl -s &quot;http://127.0.0.1:80/nginx-status&quot; | grep &#39;Reading&#39; | cut -d&quot; &quot; -f2 UserParameter=Nginx.wrie,/usr/bin/curl -s &quot;http://127.0.0.1:80/nginx-status&quot; | grep &#39;Writing&#39; | cut -d&quot; &quot; -f4 UserParameter=Nginx.wait,/usr/bin/curl -s &quot;http://127.0.0.1:80/nginx-status&quot; | grep &#39;Waiting&#39; | cut -d&quot; &quot; -f6 UserParameter=Nginx.accepted,/usr/bin/curl -s &quot;http://127.0.0.1:80/nginx-status&quot; | awk &#39;/^[ \\t]+[0-9]+[ \\t]+[0-9]+[ \\t]+[0-9]+/ {print $1}&#39; UserParameter=Nginx.handled,/usr/bin/curl -s &quot;http://127.0.0.1:80/nginx-status&quot; | awk &#39;/^[ \\t]+[0-9]+[ \\t]+[0-9]+[ \\t]+[0-9]+/ {print $2}&#39; UserParameter=Nginx.requests,/usr/bin/curl -s &quot;http://127.0.0.1:80/nginx-status&quot; | awk &#39;/^[ \\t]+[0-9]+[ \\t]+[0-9]+[ \\t]+[0-9]+/ {print $3}&#39; PHP.listenqueue --检测PHP队列数，返回整数 PHP.idle --检测PHP空闲进程数，返回整数 PHP.active --检测PHP活动进程数，返回整数 PHP.conn --检测PHP请求数,返回整数 PHP.reached --检测PHP达到限制次数，返回整数 PHP.requets --检测PHP慢请求书，返回整数redis相关的自定义项vim /usr/local/zabbix/etc/zabbix_agentd.conf.d/redis.conf UserParameter=Redis.Status,/usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 6379 ping |grep -c PONG UserParameter=Redis_conn[*],/usr/local/redis/bin/redis-cli -h $1 -p $2 info | grep -w &quot;connected_clients&quot; | awk -F&#39;:&#39; &#39;{print $2}&#39; UserParameter=Redis_rss_mem[*],/usr/local/redis/bin/redis-cli -h $1 -p $2 info | grep -w &quot;used_memory_rss&quot; | awk -F&#39;:&#39; &#39;{print $2}&#39; UserParameter=Redis_lua_mem[*],/usr/local/redis/bin/redis-cli -h $1 -p $2 info | grep -w &quot;used_memory_lua&quot; | awk -F&#39;:&#39; &#39;{print $2}&#39; UserParameter=Redis_cpu_sys[*],/usr/local/redis/bin/redis-cli -h $1 -p $2 info | grep -w &quot;used_cpu_sys&quot; | awk -F&#39;:&#39; &#39;{print $2}&#39; UserParameter=Redis_cpu_user[*],/usr/local/redis/bin/redis-cli -h $1 -p $2 info | grep -w &quot;used_cpu_user&quot; | awk -F&#39;:&#39; &#39;{print $2}&#39; UserParameter=Redis_cpu_sys_cline[*],/usr/local/redis/bin/redis-cli -h $1 -p $2 info | grep -w &quot;used_cpu_sys_children&quot; | awk -F&#39;:&#39; &#39;{print $2}&#39; UserParameter=Redis_cpu_user_cline[*],/usr/local/redis/bin/redis-cli -h $1 -p $2 info | grep -w &quot;used_cpu_user_children&quot; | awk -F&#39;:&#39; &#39;{print $2}&#39; UserParameter=Redis_keys_num[*],/usr/local/redis/bin/redis-cli -h $1 -p $2 info | grep -w &quot;$$1&quot; | grep -w &quot;keys&quot; | grep db$3 | awk -F&#39;=&#39; &#39;{print $2}&#39; | awk -F&#39;,&#39; &#39;{print $1}&#39; UserParameter=Redis_loading[*],/usr/local/redis/bin/redis-cli -h $1 -p $2 info | grep loading | awk -F&#39;:&#39; &#39;{print $$2}&#39; Redis.Status --检测Redis运行状态， 返回整数 Redis_conn --检测Redis成功连接数，返回整数 Redis_rss_mem --检测Redis系统分配内存，返回整数 Redis_lua_mem --检测Redis引擎消耗内存，返回整数 Redis_cpu_sys --检测Redis主程序核心CPU消耗率，返回整数 Redis_cpu_user --检测Redis主程序用户CPU消耗率，返回整数 Redis_cpu_sys_cline --检测Redis后台核心CPU消耗率，返回整数 Redis_cpu_user_cline --检测Redis后台用户CPU消耗率，返回整数 Redis_keys_num --检测库键值数，返回整数 Redis_loding --检测Redis持久化文件状态，返回整数MySQL:version:数据库版本 key_buffer_size:myisam的索引buffer大小 sort_buffer_size:会话的排序空间（每个线程会申请一个） join_buffer_size:这是为链接操作分配的最小缓存大小，这些连接使用普通索引扫描、范围扫描、或者连接不适用索引 max_connections:最大允许同时连接的数量 max_connect_errors：允许一个主机最多的错误链接次数，如果超过了就会拒绝之后链接（默认100）。可以使用flush hosts命令去解除拒绝 open_files_limits:操作系统允许mysql打开的文件数量，可以通过opened_tables状态确定是否需要增大table_open_cache,如果opened_tables比较大且一直还在增大说明需要增大table_open_cache max-heap_tables_size:建立的内存表的最大大小（默认16M）这个参数和tmp_table_size一起限制内部临时表的最大值(取这两个参数的小的一个），如果超过限制，则表会变为innodb或myisam引擎，（5.7.5之前是默认是myisam，5.7.6开始是innodb，可以通过internal_tmp_disk_storage_engine参数调整）。 max_allowed_packet:一个包的最大大小 ##########GET INNODB INFO #INNODB variables innodb_version: innodb_buffer_pool_instances：将innodb缓冲池分为指定的多个（默认为1） innodb_buffer_pool_size:innodb缓冲池大小、5.7.5引入了innodb_buffer_pool_chunk_size, innodb_doublewrite：是否开启doublewrite（默认开启） innodb_read_io_threads:IO读线程的数量 innodb_write_io_threads:IO写线程的数量 ########innodb status innodb_buffer_pool_pages_total:innodb缓冲池页的数量、大小等于innodb_buffer_pool_size/(16*1024) innodb_buffer_pool_pages_data:innodb缓冲池中包含数据的页的数量 ########## GET MYSQL HITRATE 1、查询缓存命中率 如果Qcache_hits+Com_select&lt;&gt;0则为 Qcache_hits/（Qcache_hits+Com_select），否则为0 2、线程缓存命中率 如果Connections&lt;&gt;0,则为1-Threads_created/Connections，否则为0 3、myisam键缓存命中率 如果Key_read_requests&lt;&gt;0,则为1-Key_reads/Key_read_requests，否则为0 4、myisam键缓存写命中率 如果Key_write_requests&lt;&gt;0,则为1-Key_writes/Key_write_requests，否则为0 5、键块使用率 如果Key_blocks_used+Key_blocks_unused&lt;&gt;0，则Key_blocks_used/（Key_blocks_used+Key_blocks_unused），否则为0 6、创建磁盘存储的临时表比率 如果Created_tmp_disk_tables+Created_tmp_tables&lt;&gt;0,则Created_tmp_disk_tables/（Created_tmp_disk_tables+Created_tmp_tables），否则为0 7、连接使用率 如果max_connections&lt;&gt;0，则threads_connected/max_connections，否则为0 8、打开文件比率 如果open_files_limit&lt;&gt;0，则open_files/open_files_limit，否则为0 9、表缓存使用率 如果table_open_cache&lt;&gt;0，则open_tables/table_open_cache，否则为0","categories":[{"name":"监控","slug":"监控","permalink":"https://cyylog.github.io/categories/%E7%9B%91%E6%8E%A7/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://cyylog.github.io/tags/zabbix/"}]},{"title":"Go_学习之Docke容器","slug":"DevOPs/Golang/Go-study","date":"2019-12-04T17:55:39.000Z","updated":"2020-06-19T16:48:06.268Z","comments":true,"path":"2019/12/05/devops/golang/go-study/","link":"","permalink":"https://cyylog.github.io/2019/12/05/devops/golang/go-study/","excerpt":"","text":"mysql容器[mysqld] log-error=/mylog/error.log slow_query_log = on long_query_time=2 slow-query-log-file =/mylog/slow.log docker run -it --rm --entrypoint=&quot;/bin/bash&quot; mysql:5.7 -c &quot;cat /etc/group &quot; 因为容器默认使用的是mysql用户。 因此我们需要把映射的文件夹修改owner docker run --name mysql -d \\ -p 3306:3306 \\ -v /home/cyy/mysql/data:/data \\ -v /home/cyy/mysql/conf/my.cnf:/etc/mysql/my.cnf \\ -v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime \\ -v /home/cyy/mysql/mylog:/mylog \\ -e MYSQL_ROOT_PASSWORD=123456 \\ mysql:5.7 alpine容器docker pull alpine docker run --name goserver -d \\ -v /home/cyy/web:/server \\ -v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime \\ -w /server \\ alpine ./gin nginx容器docker pull nginx:alpine user nginx; worker_processes auto; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; keepalive_timeout 65; upstream gin { server 172.17.0.4:8080; } server{ listen 80; location / { proxy_pass http://gin; #Proxy Settings proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } } docker run -d --name ngx \\ -v /home/cyy/ngx/nginx.conf:/etc/nginx/nginx.conf \\ -v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime \\ -p 80:80 \\ nginx:alpine Redis 容器docker run --name redis-d -v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime -p 6379:6379 redis:5-alpine redis-servver","categories":[{"name":"容器","slug":"容器","permalink":"https://cyylog.github.io/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://cyylog.github.io/tags/Docker/"}]},{"title":"Oracle设置开机自启","slug":"SQL/Oracle设置开机自启","date":"2019-11-27T14:48:07.000Z","updated":"2020-09-30T19:58:58.891Z","comments":true,"path":"2019/11/27/sql/oracle-she-zhi-kai-ji-zi-qi/","link":"","permalink":"https://cyylog.github.io/2019/11/27/sql/oracle-she-zhi-kai-ji-zi-qi/","excerpt":"","text":"步骤： 1：查看ORACLE_HOME是否设置$ echo $ORACLE_HOME /u01/app/oracle/product/11.2.0/db_12：执行dbstart 数据库自带启动脚本[oracle@njdzjkdb ~]$ cd $ORACLE_HOME [oracle@njdzjkdb dbhome_1]$ cd bin/ [oracle@njdzjkdb bin]$ dbstart ORACLE_HOME_LISTNER is not SET, unable to auto-start Oracle Net Listener Usage: /u01/app/oracle/product/11.2.0/db_1/bin/dbstart ORACLE_HOME 错误提示：ORACLE_HOME_LISTNER 没有设置 [oracle@njdzjkdb bin]$ ll | grep dbs -rwxr-x---. 1 oracle oinstall 6088 1月 1 2000 dbshut -rwxr-x---. 1 oracle oinstall 13892 12月 11 16:01 dbstart 编辑 dbstart，将ORACLE_HOME_LISTNER=$1修改成 ORACLE_HOME_LISTNER=$ORACLE_HOME 前提是$ORACLE_HOME环境设置正确 [oracle@njdzjkdb bin]$ vi dbstart ORACLE_HOME_LISTNER=/u01/app/oracle/product/11.2.0/db_13：编辑/etc/oratab文件dbca建库时都会自动创建/etc/oratab文件 将oracle:/u01/app/oracle/product/11.2.0/db_1:N 修改成 oracle:/u01/app/oracle/product/11.2.0/db_1:Y4：编辑/etc/rc.d/rc.local启动文件，添加数据库启动脚本dbstart[root@njdzjkdb ~]# vi /etc/rc.d/rc.local #!/bin/bash # THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES # # It is highly advisable to create own systemd services or udev rules # to run scripts during boot instead of using this file. # # In contrast to previous versions due to parallel execution during boot # this script will NOT be run after all other services. # # Please note that you must run &#39;chmod +x /etc/rc.d/rc.local&#39; to ensure # that this script will be executed during boot. touch /var/lock/subsys/local su oracle -lc &quot;/u01/app/oracle/product/11.2.0/db_1/bin/lsnrctl start&quot; su oracle -lc /u01/app/oracle/product/11.2.0/db_1/bin/dbstart5：重启主机，查看数据库和监听是自启动netstat -tunlp | grep 15216：查看数据库是否处于open状态select status from v$instance","categories":[{"name":"SQL","slug":"SQL","permalink":"https://cyylog.github.io/categories/SQL/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://cyylog.github.io/tags/Oracle/"}]},{"title":"CentOS7静默安装oracle11g","slug":"SQL/CentOS7静默安装oracle11g","date":"2019-11-27T14:43:32.000Z","updated":"2020-09-30T20:01:54.616Z","comments":true,"path":"2019/11/27/sql/centos7-jing-mo-an-zhuang-oracle11g/","link":"","permalink":"https://cyylog.github.io/2019/11/27/sql/centos7-jing-mo-an-zhuang-oracle11g/","excerpt":"","text":"操作系统：[root@cyylog ~]# uname -m x86_64 [root@cyylog ~]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) 安装前的准备：1. 修改主机名#sed -i &quot;s/HOSTNAME=localhost.localdomain/HOSTNAME=oracledb/&quot; /etc/sysconfig/network2.添加主机名与IP对应记录# vim /etc/hosts 192.168.0.9 oracledb3.关闭Selinux# sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/&quot; /etc/selinux/config # setenforce 0 4.检查是否有swap分区.(我的机器是没有这个,所有后面有报错) Linux一切皆文件,没有就自己造一个 1、检查 Swap 空间在设置 Swap 文件之前，有必要先检查一下系统里有没有既存的 Swap 文件。运行以下命令： # swapon -s 如果返回的信息概要是空的，则表示 Swap 文件不存在。 2、检查文件系统在设置 Swap 文件之前，同样有必要检查一下文件系统，看看是否有足够的硬盘空间来设置 Swap 。运行以下命令： # df -hal 3、创建并允许 Swap 文件下面使用 dd 命令来创建 Swap 文件。检查返回的信息，还剩余足够的硬盘空间即可。 # dd if=/dev/zero of=/swapfile bs=1024 count=512k 参数解读：if=文件名：输入文件名，缺省为标准输入。即指定源文件。&lt; if=input file &gt;of=文件名：输出文件名，缺省为标准输出。即指定目的文件。&lt; of=output file &gt;bs=bytes：同时设置读入/输出的块大小为bytes个字节count=blocks：仅拷贝blocks个块，块大小等于bs指定的字节数。 4、格式化并激活 Swap 文件上面已经创建好 Swap 文件，还需要格式化后才能使用。运行命令： # mkswap /swapfile 激活 Swap ，运行命令： # swapon /swapfile 以上步骤做完，再次运行命令： # swapon -s 你会发现返回的信息概要： 1 Filename Type Size Used Priority 2 /swapfile file 524284 0 -1 如果要机器重启的时候自动挂载 Swap ，那么还需要修改 fstab 配置。用 vim 打开 /etc/fstab 文件，在其最后添加如下一行： /swapfile swap swap defaults 0 0 最后，赋予 Swap 文件适当的权限： # chown root:root /swapfile # chmod 0600 /swapfile 5.安装常用工具,配置阿里源(个人习惯,在使用的主机上面配置这些常用工具) # curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo # yum clean all # yum makecache fast # yum install -y wget ntpdate net-tools vim bash-completion ShellCheck # ntpdate -b ntp1.aliyun.com安装软件包参考官方：http://docs.oracle.com/cd/E11882_01/install.112/e24326/toc.htm#BHCCADGD The following or later version of packages for Oracle Linux 7, and Red Hat Enterprise Linux 7 must be installed: binutils-2.23.52.0.1-12.el7.x86_64 compat-libcap1-1.10-3.el7.x86_64 compat-libstdc++-33-3.2.3-71.el7.i686 compat-libstdc++-33-3.2.3-71.el7.x86_64 gcc-4.8.2-3.el7.x86_64 gcc-c++-4.8.2-3.el7.x86_64 glibc-2.17-36.el7.i686 glibc-2.17-36.el7.x86_64 glibc-devel-2.17-36.el7.i686 glibc-devel-2.17-36.el7.x86_64 ksh libaio-0.3.109-9.el7.i686 libaio-0.3.109-9.el7.x86_64 libaio-devel-0.3.109-9.el7.i686 libaio-devel-0.3.109-9.el7.x86_64 libgcc-4.8.2-3.el7.i686 libgcc-4.8.2-3.el7.x86_64 libstdc++-4.8.2-3.el7.i686 libstdc++-4.8.2-3.el7.x86_64 libstdc++-devel-4.8.2-3.el7.i686 libstdc++-devel-4.8.2-3.el7.x86_64 libXi-1.7.2-1.el7.i686 libXi-1.7.2-1.el7.x86_64 libXtst-1.2.2-1.el7.i686 libXtst-1.2.2-1.el7.x86_64 make-3.82-19.el7.x86_64 sysstat-10.1.5-1.el7.x86_64 unixODBC-2.3.1-6.el7.x86_64 or later unixODBC-2.3.1-6.el7.i686 or later unixODBC-devel-2.3.1-6.el7.x86_64 or later unixODBC-devel-2.3.1-6.el7.i686 or laterbinutils-2.23.52.0.1-12.el7.x86_64 compat-libcap1-1.10-3.el7.x86_64 compat-libstdc++-33-3.2.3-71.el7.i686 compat-libstdc++-33-3.2.3-71.el7.x86_64 gcc-4.8.2-3.el7.x86_64 gcc-c++-4.8.2-3.el7.x86_64 glibc-2.17-36.el7.i686 glibc-2.17-36.el7.x86_64 glibc-devel-2.17-36.el7.i686 glibc-devel-2.17-36.el7.x86_64 ksh libaio-0.3.109-9.el7.i686 libaio-0.3.109-9.el7.x86_64 libaio-devel-0.3.109-9.el7.i686 libaio-devel-0.3.109-9.el7.x86_64 libgcc-4.8.2-3.el7.i686 libgcc-4.8.2-3.el7.x86_64 libstdc++-4.8.2-3.el7.i686 libstdc++-4.8.2-3.el7.x86_64 libstdc++-devel-4.8.2-3.el7.i686 libstdc++-devel-4.8.2-3.el7.x86_64 libXi-1.7.2-1.el7.i686 libXi-1.7.2-1.el7.x86_64 libXtst-1.2.2-1.el7.i686 libXtst-1.2.2-1.el7.x86_64 make-3.82-19.el7.x86_64 sysstat-10.1.5-1.el7.x86_64 unixODBC-2.3.1-6.el7.x86_64 or later unixODBC-2.3.1-6.el7.i686 or later unixODBC-devel-2.3.1-6.el7.x86_64 or later unixODBC-devel-2.3.1-6.el7.i686 or later用yum进行安装yum -y install binutils compat-libcap1 compat-libstdc++-33 compat-libstdc++-33*i686 compat-libstdc++-33*.devel compat-libstdc++-33 compat-libstdc++-33*.devel gcc gcc-c++ glibc glibc*.i686 glibc-devel glibc-devel*.i686 ksh libaio libaio*.i686 libaio-devel libaio-devel*.devel libgcc libgcc*.i686 libstdc++ libstdc++*.i686 libstdc++-devel libstdc++-devel*.devel libXi libXi*.i686 libXtst libXtst*.i686 make sysstat unixODBC unixODBC*.i686 unixODBC-devel unixODBC-devel*.i686检测是否31个包都有安装开放源码绿色蓝色按钮样式 [root@cyylog ~]# rpm -q binutils compat-libcap1 compat-libstdc++-33 gcc gcc-c++ glibc glibc-devel ksh libaio libaio-devel libgcc libstdc++ libstdc++-devel libXi libXtst make sysstat unixODBC unixODBC-devel binutils-2.23.52.0.1-55.el7.x86_64 compat-libcap1-1.10-7.el7.x86_64 compat-libstdc++-33-3.2.3-72.el7.x86_64 compat-libstdc++-33-3.2.3-72.el7.i686 gcc-4.8.5-4.el7.x86_64 gcc-c++-4.8.5-4.el7.x86_64 glibc-2.17-106.el7_2.8.x86_64 glibc-2.17-106.el7_2.8.i686 glibc-devel-2.17-106.el7_2.8.x86_64 glibc-devel-2.17-106.el7_2.8.i686 ksh-20120801-22.el7_1.3.x86_64 libaio-0.3.109-13.el7.x86_64 libaio-0.3.109-13.el7.i686 libaio-devel-0.3.109-13.el7.x86_64 libaio-devel-0.3.109-13.el7.i686 libgcc-4.8.5-4.el7.x86_64 libgcc-4.8.5-4.el7.i686 libstdc++-4.8.5-4.el7.x86_64 libstdc++-4.8.5-4.el7.i686 libstdc++-devel-4.8.5-4.el7.x86_64 libstdc++-devel-4.8.5-4.el7.i686 libXi-1.7.2-2.1.el7.x86_64 libXi-1.7.4-2.el7.i686 libXtst-1.2.2-2.1.el7.x86_64 libXtst-1.2.2-2.1.el7.i686 make-3.82-21.el7.x86_64 sysstat-10.1.5-7.el7.x86_64 unixODBC-2.3.1-11.el7.x86_64 unixODBC-2.3.1-11.el7.i686 unixODBC-devel-2.3.1-11.el7.x86_64 unixODBC-devel-2.3.1-11.el7.i686版本号只能大于规定的版本，不能小于。 创建oinstall和dba组 # groupadd oinstall # groupadd dba创建oracle用户 # useradd -g oinstall -G dba oracle设置oracle用户密码 # passwd oracle验证创建是否正确 [root@cyylog ~]# id oracle uid=1000(oracle) gid=1000(oinstall) groups=1000(oinstall),1001(dba)配置内核参数[root@cyylog ~]# vim /etc/sysctl.conf # System default settings live in /usr/lib/sysctl.d/00-system.conf. # To override those settings, enter new settings here, or in an /etc/sysctl.d/&lt;name&gt;.conf file # # For more information, see sysctl.conf(5) and sysctl.d(5). fs.aio-max-nr = 1048576 fs.file-max = 6815744 kernel.shmall = 2097152 kernel.shmmax = 536870912 #最低：536870912，最大值：比物理内存小1个字节的值，建议超过物理内存的一半 kernel.shmmni = 4096 kernel.sem = 250 32000 100 128 net.ipv4.ip_local_port_range = 9000 65500 net.core.rmem_default = 262144 net.core.rmem_max = 4194304 net.core.wmem_default = 262144 net.core.wmem_max = 1048576参数的值不能小于上面的配置，这是oracle官方建议的最小值，生产环境建议调整这些参数，以优化系统性能。 修改后使之生效 # sysctl -p修改用户限制 vim /etc/security/limits.conf #在末尾添加 oracle soft nproc 2047 oracle hard nproc 16384 oracle soft nofile 1024 oracle hard nofile 65536 oracle soft stack 10240 oracle hard stack 10240 在/etc/pam.d/login 文件中，使用文本编辑器或vi命令增加或修改以下内容 session required /lib64/security/pam_limits.so session required pam_limits.so在/etc/profile 文件中，使用文本编辑器或vi命令增加或修改以下内容 if [ $USER = &quot;oracle&quot; ]; then if [ $SHELL = &quot;/bin/ksh&quot; ]; then ulimit -p 16384 ulimit -n 65536 else ulimit -u 16384 -n 65536 fi fi使之生效 # source /etc/profile创建安装目录 # mkdir -p /u01/app/ # chown -R oracle:oinstall /u01/app/ # chmod -R 775 /u01/app/配置环境变量 [oracle@cyylog ~]$ vim ~/.bash_profile export ORACLE_BASE=/u01/app/oracle export ORACLE_SID=dbsrv2使之生效 source ~/.bash_profile解压oracle软件 [root@cyylog src]# unzip linux.x64_11gR2_database_1of2.zip [root@cyylog src]# unzip linux.x64_11gR2_database_2of2.zip复制响应文件模板 [oracle@cyylog ~]$ mkdir etc [oracle@cyylog ~]$ cp /usr/local/src/database/response/* /home/oracle/etc/ [oracle@cyylog ~]$ ls etc dbca.rsp db_install.rsp netca.rsp设置响应文件权限 [oracle@cyylog ~]$ su - root [root@cyylog ~]# chmod 700 /home/oracle/etc/*.rsp静默安装Oracle软件su - oracle 修改安装Oracle软件的响应文件/home/oracle/etc/db_install.rsp oracle.install.option=INSTALL_DB_SWONLY // 安装类型 ORACLE_HOSTNAME=oracledb // 主机名称（hostname查询） UNIX_GROUP_NAME=oinstall // 安装组 INVENTORY_LOCATION=/u01/app/oraInventory //INVENTORY目录（不填就是默认值） SELECTED_LANGUAGES=en,zh_CN,zh_TW // 选择语言 ORACLE_HOME=/u01/app/oracle/product/11.2.0/db_1 //oracle_home ORACLE_BASE=/u01/app/oracle //oracle_base oracle.install.db.InstallEdition=EE // oracle版本 oracle.install.db.isCustomInstall=false //自定义安装，否，使用默认组件 oracle.install.db.DBA_GROUP=dba / / dba用户组 oracle.install.db.OPER_GROUP=oinstall // oper用户组 oracle.install.db.config.starterdb.type=GENERAL_PURPOSE //数据库类型 oracle.install.db.config.starterdb.globalDBName=orcl //globalDBName oracle.install.db.config.starterdb.SID=dbsrv2 //SID oracle.install.db.config.starterdb.memoryLimit=81920 //自动管理内存的内存(M) oracle.install.db.config.starterdb.password.ALL=oracle //设定所有数据库用户使用同一个密码 SECURITY_UPDATES_VIA_MYORACLESUPPORT=false //（手动写了false） DECLINE_SECURITY_UPDATES=true //设置安全更新（貌似是有bug，这个一定要选true，否则会无限提醒邮件地址有问题，终止安装。PS：不管地址对不对）oracle.install.option=INSTALL_DB_SWONLY // 安装类型 ORACLE_HOSTNAME=oracledb // 主机名称（hostname查询） UNIX_GROUP_NAME=oinstall // 安装组 INVENTORY_LOCATION=/u01/app/oraInventory //INVENTORY目录（不填就是默认值） SELECTED_LANGUAGES=en,zh_CN,zh_TW // 选择语言 ORACLE_HOME=/u01/app/oracle/product/11.2.0/db_1 //oracle_home ORACLE_BASE=/u01/app/oracle //oracle_base oracle.install.db.InstallEdition=EE // oracle版本 oracle.install.db.isCustomInstall=false //自定义安装，否，使用默认组件 oracle.install.db.DBA_GROUP=dba / / dba用户组 oracle.install.db.OPER_GROUP=oinstall // oper用户组 oracle.install.db.config.starterdb.type=GENERAL_PURPOSE //数据库类型 oracle.install.db.config.starterdb.globalDBName=orcl //globalDBName oracle.install.db.config.starterdb.SID=dbsrv2 //SID oracle.install.db.config.starterdb.memoryLimit=81920 //自动管理内存的内存(M) oracle.install.db.config.starterdb.password.ALL=oracle //设定所有数据库用户使用同一个密码 SECURITY_UPDATES_VIA_MYORACLESUPPORT=false //（手动写了false） DECLINE_SECURITY_UPDATES=true //设置安全更新（貌似是有bug，这个一定要选true，否则会无限提醒邮件地址有问题，终止安装。PS：不管地址对不对）开始静默安装 [oracle@cyylog database]$ ./runInstaller -silent -responseFile /home/oracle/etc/db_install.rsp新开一个终端 查看安装日志 # tail -f /u01/app/oraInventory/logs/installActions2016-08-31_06-56-29PM.log出现类似如下提示表示安装完成： -———————————————————————– The following configuration scripts need to be executed as the “root” user. #!/bin/sh #Root scripts to run /u01/app/oraInventory/orainstRoot.sh /u01/app/oracle/product/11.2.0/db_1/root.sh To execute the configuration scripts: Open a terminal window Log in as “root” Run the scripts Return to this window and hit “Enter” key to continue Successfully Setup Software. -—————————————————————————- 使用root用户执行脚本 $ su - root # /u01/app/oraInventory/orainstRoot.sh # /u01/app/oracle/product/11.2.0/db_1/root.sh增加或修改oracle的环境变量 # su - oracle # vim ~/.bash_profile#for oracle export ORACLE_BASE=/u01/app/oracle export ORACLE_SID=dbsrv2 export ROACLE_PID=ora11g #export NLS_LANG=AMERICAN_AMERICA.AL32UTF8 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/usr/lib export ORACLE_HOME=/u01/app/oracle/product/11.2.0/db_1 export PATH=$PATH:$ORACLE_HOME/bin export LANG=&quot;zh_CN.UTF-8&quot; export NLS_LANG=&quot;SIMPLIFIED CHINESE_CHINA.AL32UTF8&quot; export NLS_DATE_FORMAT=&#39;yyyy-mm-dd hh24:mi:ss&#39; 刷新环境变量 # source ~/.bash_profile配置监听程序 [oracle@cyylog ~]$ netca /silent /responsefile /home/oracle/etc/netca.rsp Parsing command line arguments: Parameter &quot;silent&quot; = true Parameter &quot;responsefile&quot; = /home/oracle/etc/netca.rsp Done parsing command line arguments. Oracle Net Services Configuration: Profile configuration complete. Oracle Net Listener Startup: Running Listener Control: /u01/app/oracle/product/11.2.0/db_1/bin/lsnrctl start LISTENER Listener Control complete. Listener started successfully. Listener configuration complete. Oracle Net Services configuration successful. The exit code is 0启动监控程序 [oracle@cyylog ~]$ lsnrctl start LSNRCTL for Linux: Version 11.2.0.1.0 - Production on 01-SEP-2016 11:23:31 Copyright (c) 1991, 2009, Oracle. All rights reserved. Starting /u01/app/oracle/product/11.2.0/db_1/bin/tnslsnr: please wait... TNSLSNR for Linux: Version 11.2.0.1.0 - Production System parameter file is /u01/app/oracle/product/11.2.0/db_1/network/admin/listener.ora Log messages written to /u01/app/oracle/diag/tnslsnr/cyylog/listener/alert/log.xml Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521))) Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=cyylog)(PORT=1521))) Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=IPC)(KEY=EXTPROC1521))) STATUS of the LISTENER ------------------------ Alias LISTENER Version TNSLSNR for Linux: Version 11.2.0.1.0 - Production Start Date 01-SEP-2016 11:23:31 Uptime 0 days 0 hr. 0 min. 0 sec Trace Level off Security ON: Local OS Authentication SNMP OFF Listener Parameter File /u01/app/oracle/product/11.2.0/db_1/network/admin/listener.ora Listener Log File /u01/app/oracle/diag/tnslsnr/cyylog/listener/alert/log.xml Listening Endpoints Summary... (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521))) (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=cyylog)(PORT=1521))) The listener supports no services The command completed successfully静默dbca建库 编辑应答文件 [oracle@cyylog ~]$ vi etc/dbca.rsp [GENERAL] RESPONSEFILE_VERSION = &quot;11.2.0&quot; OPERATION_TYPE = &quot;createDatabase&quot; [CREATEDATABASE] GDBNAME = &quot;dbsrv2&quot; SID = &quot;dbsrv2&quot; TEMPLATENAME = &quot;General_Purpose.dbc&quot; CHARACTERSET = &quot;AL32UTF8&quot;建库(我的finashell会闪屏,但是不影响使用，因为这个是因为无图形界面的情况) [oracle@cyylog ~]$ dbca -silent -responseFile etc/dbca.rsp Enter SYS user password: Enter SYSTEM user password: sh: /bin/ksh: No such file or directory sh: /bin/ksh: No such file or directory Copying database files 1% complete 3% complete 11% complete 18% complete 26% complete 37% complete Creating and starting Oracle instance 40% complete 45% complete 50% complete 55% complete 56% complete 57% complete 60% complete 62% complete Completing Database Creation 66% complete 70% complete 73% complete 74% complete 85% complete 96% complete 100% complete Look at the log file Look at the log file &quot;/u01/app/oracle/cfgtoollogs/dbca/orcl11g/orcl11g.log&quot; for further details.[oracle@cyylog ~]$ dbca -silent -responseFile etc/dbca.rsp Enter SYS user password: Enter SYSTEM user password: sh: /bin/ksh: No such file or directory sh: /bin/ksh: No such file or directory Copying database files 1% complete 3% complete 11% complete 18% complete 26% complete 37% complete Creating and starting Oracle instance 40% complete 45% complete 50% complete 55% complete 56% complete 57% complete 60% complete 62% complete Completing Database Creation 66% complete 70% complete 73% complete 74% complete 85% complete 96% complete 100% complete Look at the log file Look at the log file &quot;/u01/app/oracle/cfgtoollogs/dbca/orcl11g/orcl11g.log&quot; for further details.查看输出日志 [oracle@cyylog ~]$ tailf /u01/app/oracle/cfgtoollogs/dbca/silent.log Copying database files DBCA_PROGRESS : 1% DBCA_PROGRESS : 3% DBCA_PROGRESS : 11% DBCA_PROGRESS : 18% DBCA_PROGRESS : 26% DBCA_PROGRESS : 37% Creating and starting Oracle instance DBCA_PROGRESS : 40% DBCA_PROGRESS : 45% DBCA_PROGRESS : 50% DBCA_PROGRESS : 55% DBCA_PROGRESS : 56% DBCA_PROGRESS : 60% DBCA_PROGRESS : 62% Completing Database Creation DBCA_PROGRESS : 66% DBCA_PROGRESS : 70% DBCA_PROGRESS : 73% DBCA_PROGRESS : 85% DBCA_PROGRESS : 96% DBCA_PROGRESS : 100% Database creation complete. For details check the logfiles at: /u01/app/oracle/cfgtoollogs/dbca/orcl11g. Database Information: Global Database Name:orcl11g.us.oracle.com System Identifier(SID):dbsrv2至此完成数据库实例的创建。 -———————————————————————————- -———————————————————————————- 附： 删除实例： [oracle@cyylog ~]$ dbca -silent -deleteDatabase -sourcedb dbsrv2文章来源:https://www.cnblogs.com/zydev/p/5827207.html","categories":[{"name":"SQL","slug":"SQL","permalink":"https://cyylog.github.io/categories/SQL/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://cyylog.github.io/tags/Oracle/"}]},{"title":"Docker_002","slug":"容器/Dockerfilee-002","date":"2019-11-18T17:55:39.000Z","updated":"2020-05-25T14:01:09.188Z","comments":true,"path":"2019/11/19/rong-qi/dockerfilee-002/","link":"","permalink":"https://cyylog.github.io/2019/11/19/rong-qi/dockerfilee-002/","excerpt":"","text":"Dockerfile_redis_5.0FROM debian:buster-slim # add our user and group first to make sure their IDs get assigned consistently, regardless of whatever dependencies get added RUN groupadd -r -g 999 redis && useradd -r -g redis -u 999 redis # grab gosu for easy step-down from root # https://github.com/tianon/gosu/releases ENV GOSU_VERSION 1.11 RUN set -eux; \\ # save list of currently installed packages for later so we can clean up savedAptMark=\"$(apt-mark showmanual)\"; \\ apt-get update; \\ apt-get install -y --no-install-recommends \\ ca-certificates \\ dirmngr \\ gnupg \\ wget \\ ; \\ rm -rf /var/lib/apt/lists/*; \\ \\ dpkgArch=\"$(dpkg --print-architecture | awk -F- '{ print $NF }')\"; \\ wget -O /usr/local/bin/gosu \"https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$dpkgArch\"; \\ wget -O /usr/local/bin/gosu.asc \"https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$dpkgArch.asc\"; \\ \\ # verify the signature export GNUPGHOME=\"$(mktemp -d)\"; \\ gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4; \\ gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu; \\ gpgconf --kill all; \\ rm -rf \"$GNUPGHOME\" /usr/local/bin/gosu.asc; \\ \\ # clean up fetch dependencies apt-mark auto '.*' > /dev/null; \\ [ -z \"$savedAptMark\" ] || apt-mark manual $savedAptMark > /dev/null; \\ apt-get purge -y --auto-remove -o APT::AutoRemove::RecommendsImportant=false; \\ \\ chmod +x /usr/local/bin/gosu; \\ # verify that the binary works gosu --version; \\ gosu nobody true ENV REDIS_VERSION 5.0.8 ENV REDIS_DOWNLOAD_URL http://download.redis.io/releases/redis-5.0.8.tar.gz ENV REDIS_DOWNLOAD_SHA f3c7eac42f433326a8d981b50dba0169fdfaf46abb23fcda2f933a7552ee4ed7 RUN set -eux; \\ \\ savedAptMark=\"$(apt-mark showmanual)\"; \\ apt-get update; \\ apt-get install -y --no-install-recommends \\ ca-certificates \\ wget \\ \\ gcc \\ libc6-dev \\ make \\ ; \\ rm -rf /var/lib/apt/lists/*; \\ \\ wget -O redis.tar.gz \"$REDIS_DOWNLOAD_URL\"; \\ echo \"$REDIS_DOWNLOAD_SHA *redis.tar.gz\" | sha256sum -c -; \\ mkdir -p /usr/src/redis; \\ tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1; \\ rm redis.tar.gz; \\ \\ # disable Redis protected mode [1] as it is unnecessary in context of Docker # (ports are not automatically exposed when running inside Docker, but rather explicitly by specifying -p / -P) # [1]: https://github.com/antirez/redis/commit/edd4d555df57dc84265fdfb4ef59a4678832f6da grep -q '^#define CONFIG_DEFAULT_PROTECTED_MODE 1$' /usr/src/redis/src/server.h; \\ sed -ri 's!^(#define CONFIG_DEFAULT_PROTECTED_MODE) 1$!\\1 0!' /usr/src/redis/src/server.h; \\ grep -q '^#define CONFIG_DEFAULT_PROTECTED_MODE 0$' /usr/src/redis/src/server.h; \\ # for future reference, we modify this directly in the source instead of just supplying a default configuration flag because apparently \"if you specify any argument to redis-server, [it assumes] you are going to specify everything\" # see also https://github.com/docker-library/redis/issues/4#issuecomment-50780840 # (more exactly, this makes sure the default behavior of \"save on SIGTERM\" stays functional by default) \\ make -C /usr/src/redis -j \"$(nproc)\" all; \\ make -C /usr/src/redis install; \\ \\ # TODO https://github.com/antirez/redis/pull/3494 (deduplicate \"redis-server\" copies) serverMd5=\"$(md5sum /usr/local/bin/redis-server | cut -d' ' -f1)\"; export serverMd5; \\ find /usr/local/bin/redis* -maxdepth 0 \\ -type f -not -name redis-server \\ -exec sh -eux -c ' \\ md5=\"$(md5sum \"$1\" | cut -d\" \" -f1)\"; \\ test \"$md5\" = \"$serverMd5\"; \\ ' -- '{}' ';' \\ -exec ln -svfT 'redis-server' '{}' ';' \\ ; \\ \\ rm -r /usr/src/redis; \\ \\ apt-mark auto '.*' > /dev/null; \\ [ -z \"$savedAptMark\" ] || apt-mark manual $savedAptMark > /dev/null; \\ find /usr/local -type f -executable -exec ldd '{}' ';' \\ | awk '/=>/ { print $(NF-1) }' \\ | sort -u \\ | xargs -r dpkg-query --search \\ | cut -d: -f1 \\ | sort -u \\ | xargs -r apt-mark manual \\ ; \\ apt-get purge -y --auto-remove -o APT::AutoRemove::RecommendsImportant=false; \\ \\ redis-cli --version; \\ redis-server --version RUN mkdir /data && chown redis:redis /data VOLUME /data WORKDIR /data COPY docker-entrypoint.sh /usr/local/bin/ ENTRYPOINT [\"docker-entrypoint.sh\"] EXPOSE 6379 CMD [\"redis-server\"] Dockerfile_alpine_httpd_2.4FROM alpine:3.11 # ensure www-data user exists RUN set -x \\ && addgroup -g 82 -S www-data \\ && adduser -u 82 -D -S -G www-data www-data # 82 is the standard uid/gid for \"www-data\" in Alpine # https://git.alpinelinux.org/cgit/aports/tree/main/apache2/apache2.pre-install?h=v3.8.1 # https://git.alpinelinux.org/cgit/aports/tree/main/lighttpd/lighttpd.pre-install?h=v3.8.1 # https://git.alpinelinux.org/cgit/aports/tree/main/nginx/nginx.pre-install?h=v3.8.1 ENV HTTPD_PREFIX /usr/local/apache2 ENV PATH $HTTPD_PREFIX/bin:$PATH RUN mkdir -p \"$HTTPD_PREFIX\" \\ && chown www-data:www-data \"$HTTPD_PREFIX\" WORKDIR $HTTPD_PREFIX ENV HTTPD_VERSION 2.4.43 ENV HTTPD_SHA256 a497652ab3fc81318cdc2a203090a999150d86461acff97c1065dc910fe10f43 # https://httpd.apache.org/security/vulnerabilities_24.html ENV HTTPD_PATCHES=\"\" # see https://httpd.apache.org/docs/2.4/install.html#requirements RUN set -eux; \\ \\ runDeps=' \\ apr-dev \\ apr-util-dbm_db \\ apr-util-dev \\ apr-util-ldap \\ perl \\ '; \\ apk add --no-cache --virtual .build-deps \\ $runDeps \\ ca-certificates \\ coreutils \\ dpkg-dev dpkg \\ gcc \\ gnupg \\ libc-dev \\ # mod_md curl-dev \\ jansson-dev \\ # mod_proxy_html mod_xml2enc libxml2-dev \\ # mod_lua lua-dev \\ make \\ # mod_http2 nghttp2-dev \\ # mod_session_crypto openssl \\ openssl-dev \\ pcre-dev \\ tar \\ # mod_deflate zlib-dev \\ # mod_brotli brotli-dev \\ ; \\ \\ ddist() { \\ local f=\"$1\"; shift; \\ local distFile=\"$1\"; shift; \\ local success=; \\ local distUrl=; \\ for distUrl in \\ # https://issues.apache.org/jira/browse/INFRA-8753?focusedCommentId=14735394#comment-14735394 'https://www.apache.org/dyn/closer.cgi?action=download&filename=' \\ # if the version is outdated (or we're grabbing the .asc file), we might have to pull from the dist/archive :/ https://www-us.apache.org/dist/ \\ https://www.apache.org/dist/ \\ https://archive.apache.org/dist/ \\ ; do \\ if wget -O \"$f\" \"$distUrl$distFile\" && [ -s \"$f\" ]; then \\ success=1; \\ break; \\ fi; \\ done; \\ [ -n \"$success\" ]; \\ }; \\ \\ ddist 'httpd.tar.bz2' \"httpd/httpd-$HTTPD_VERSION.tar.bz2\"; \\ echo \"$HTTPD_SHA256 *httpd.tar.bz2\" | sha256sum -c -; \\ \\ # see https://httpd.apache.org/download.cgi#verify ddist 'httpd.tar.bz2.asc' \"httpd/httpd-$HTTPD_VERSION.tar.bz2.asc\"; \\ export GNUPGHOME=\"$(mktemp -d)\"; \\ for key in \\ # gpg: key 791485A8: public key \"Jim Jagielski (Release Signing Key) \" imported A93D62ECC3C8EA12DB220EC934EA76E6791485A8 \\ # gpg: key 995E35221AD84DFF: public key \"Daniel Ruggeri (https://home.apache.org/~druggeri/) \" imported B9E8213AEFB861AF35A41F2C995E35221AD84DFF \\ ; do \\ gpg --batch --keyserver ha.pool.sks-keyservers.net --recv-keys \"$key\"; \\ done; \\ gpg --batch --verify httpd.tar.bz2.asc httpd.tar.bz2; \\ command -v gpgconf && gpgconf --kill all || :; \\ rm -rf \"$GNUPGHOME\" httpd.tar.bz2.asc; \\ \\ mkdir -p src; \\ tar -xf httpd.tar.bz2 -C src --strip-components=1; \\ rm httpd.tar.bz2; \\ cd src; \\ \\ patches() { \\ while [ \"$#\" -gt 0 ]; do \\ local patchFile=\"$1\"; shift; \\ local patchSha256=\"$1\"; shift; \\ ddist \"$patchFile\" \"httpd/patches/apply_to_$HTTPD_VERSION/$patchFile\"; \\ echo \"$patchSha256 *$patchFile\" | sha256sum -c -; \\ patch -p0 < \"$patchFile\"; \\ rm -f \"$patchFile\"; \\ done; \\ }; \\ patches $HTTPD_PATCHES; \\ \\ gnuArch=\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\"; \\ ./configure \\ --build=\"$gnuArch\" \\ --prefix=\"$HTTPD_PREFIX\" \\ --enable-mods-shared=reallyall \\ --enable-mpms-shared=all \\ # PIE and hardening flags are unnecessary as Alpine enables them automatically (https://alpinelinux.org/about/) ; \\ make -j \"$(nproc)\"; \\ make install; \\ \\ cd ..; \\ rm -r src man manual; \\ \\ sed -ri \\ -e 's!^(\\s*CustomLog)\\s+\\S+!\\1 /proc/self/fd/1!g' \\ -e 's!^(\\s*ErrorLog)\\s+\\S+!\\1 /proc/self/fd/2!g' \\ -e 's!^(\\s*TransferLog)\\s+\\S+!\\1 /proc/self/fd/1!g' \\ \"$HTTPD_PREFIX/conf/httpd.conf\" \\ \"$HTTPD_PREFIX/conf/extra/httpd-ssl.conf\" \\ ; \\ \\ runDeps=\"$runDeps $( \\ scanelf --needed --nobanner --format '%n#p' --recursive /usr/local \\ | tr ',' '\\n' \\ | sort -u \\ | awk 'system(\"[ -e /usr/local/lib/\" $1 \" ]\") == 0 { next } { print \"so:\" $1 }' \\ )\"; \\ apk add --no-network --virtual .httpd-rundeps $runDeps; \\ apk del --no-network .build-deps; \\ \\ # smoke test httpd -v # https://httpd.apache.org/docs/2.4/stopping.html#gracefulstop STOPSIGNAL SIGWINCH COPY httpd-foreground /usr/local/bin/ EXPOSE 80 CMD [\"httpd-foreground\"]","categories":[{"name":"容器","slug":"容器","permalink":"https://cyylog.github.io/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://cyylog.github.io/tags/Docker/"}]},{"title":"Docker_001","slug":"容器/Dockerfilee-001","date":"2019-11-17T14:16:23.000Z","updated":"2020-05-25T14:01:00.834Z","comments":true,"path":"2019/11/17/rong-qi/dockerfilee-001/","link":"","permalink":"https://cyylog.github.io/2019/11/17/rong-qi/dockerfilee-001/","excerpt":"","text":"Docker_MySQL 5.7FROM debian:buster-slim # add our user and group first to make sure their IDs get assigned consistently, regardless of whatever dependencies get added RUN groupadd -r mysql && useradd -r -g mysql mysql RUN apt-get update && apt-get install -y --no-install-recommends gnupg dirmngr && rm -rf /var/lib/apt/lists/* # add gosu for easy step-down from root ENV GOSU_VERSION 1.7 RUN set -x \\ && apt-get update && apt-get install -y --no-install-recommends ca-certificates wget && rm -rf /var/lib/apt/lists/* \\ && wget -O /usr/local/bin/gosu \"https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture)\" \\ && wget -O /usr/local/bin/gosu.asc \"https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture).asc\" \\ && export GNUPGHOME=\"$(mktemp -d)\" \\ && gpg --batch --keyserver ha.pool.sks-keyservers.net --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4 \\ && gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu \\ && gpgconf --kill all \\ && rm -rf \"$GNUPGHOME\" /usr/local/bin/gosu.asc \\ && chmod +x /usr/local/bin/gosu \\ && gosu nobody true \\ && apt-get purge -y --auto-remove ca-certificates wget RUN mkdir /docker-entrypoint-initdb.d RUN apt-get update && apt-get install -y --no-install-recommends \\ # for MYSQL_RANDOM_ROOT_PASSWORD pwgen \\ # for mysql_ssl_rsa_setup openssl \\ # FATAL ERROR: please install the following Perl modules before executing /usr/local/mysql/scripts/mysql_install_db: # File::Basename # File::Copy # Sys::Hostname # Data::Dumper perl \\ # install \"xz-utils\" for .sql.xz docker-entrypoint-initdb.d files xz-utils \\ && rm -rf /var/lib/apt/lists/* RUN set -ex; \\ # gpg: key 5072E1F5: public key \"MySQL Release Engineering \" imported key='A4A9406876FCBD3C456770C88C718D3B5072E1F5'; \\ export GNUPGHOME=\"$(mktemp -d)\"; \\ gpg --batch --keyserver ha.pool.sks-keyservers.net --recv-keys \"$key\"; \\ gpg --batch --export \"$key\" > /etc/apt/trusted.gpg.d/mysql.gpg; \\ gpgconf --kill all; \\ rm -rf \"$GNUPGHOME\"; \\ apt-key list > /dev/null ENV MYSQL_MAJOR 5.7 ENV MYSQL_VERSION 5.7.29-1debian10 RUN echo \"deb http://repo.mysql.com/apt/debian/ buster mysql-${MYSQL_MAJOR}\" > /etc/apt/sources.list.d/mysql.list # the \"/var/lib/mysql\" stuff here is because the mysql-server postinst doesn't have an explicit way to disable the mysql_install_db codepath besides having a database already \"configured\" (ie, stuff in /var/lib/mysql/mysql) # also, we set debconf keys to make APT a little quieter RUN { \\ echo mysql-community-server mysql-community-server/data-dir select ''; \\ echo mysql-community-server mysql-community-server/root-pass password ''; \\ echo mysql-community-server mysql-community-server/re-root-pass password ''; \\ echo mysql-community-server mysql-community-server/remove-test-db select false; \\ } | debconf-set-selections \\ && apt-get update && apt-get install -y mysql-server=\"${MYSQL_VERSION}\" && rm -rf /var/lib/apt/lists/* \\ && rm -rf /var/lib/mysql && mkdir -p /var/lib/mysql /var/run/mysqld \\ && chown -R mysql:mysql /var/lib/mysql /var/run/mysqld \\ # ensure that /var/run/mysqld (used for socket and lock files) is writable regardless of the UID our mysqld instance ends up having at runtime && chmod 777 /var/run/mysqld \\ # comment out a few problematic configuration values && find /etc/mysql/ -name '*.cnf' -print0 \\ | xargs -0 grep -lZE '^(bind-address|log)' \\ | xargs -rt -0 sed -Ei 's/^(bind-address|log)/#&/' \\ # don't reverse lookup hostnames, they are usually another container && echo '[mysqld]\\nskip-host-cache\\nskip-name-resolve' > /etc/mysql/conf.d/docker.cnf VOLUME /var/lib/mysql COPY docker-entrypoint.sh /usr/local/bin/ RUN ln -s usr/local/bin/docker-entrypoint.sh /entrypoint.sh # backwards compat ENTRYPOINT [\"docker-entrypoint.sh\"] EXPOSE 3306 33060 CMD [\"mysqld\"] Docker_NGINX_1.17.9FROM alpine:3.10 LABEL maintainer=\"NGINX Docker Maintainers \" ENV NGINX_VERSION 1.17.9 ENV NJS_VERSION 0.3.9 ENV PKG_RELEASE 1 RUN set -x \\ # create nginx user/group first, to be consistent throughout docker variants && addgroup -g 101 -S nginx \\ && adduser -S -D -H -u 101 -h /var/cache/nginx -s /sbin/nologin -G nginx -g nginx nginx \\ && apkArch=\"$(cat /etc/apk/arch)\" \\ && nginxPackages=\" \\ nginx=${NGINX_VERSION}-r${PKG_RELEASE} \\ nginx-module-xslt=${NGINX_VERSION}-r${PKG_RELEASE} \\ nginx-module-geoip=${NGINX_VERSION}-r${PKG_RELEASE} \\ nginx-module-image-filter=${NGINX_VERSION}-r${PKG_RELEASE} \\ nginx-module-njs=${NGINX_VERSION}.${NJS_VERSION}-r${PKG_RELEASE} \\ \" \\ && case \"$apkArch\" in \\ x86_64) \\ # arches officially built by upstream set -x \\ && KEY_SHA512=\"e7fa8303923d9b95db37a77ad46c68fd4755ff935d0a534d26eba83de193c76166c68bfe7f65471bf8881004ef4aa6df3e34689c305662750c0172fca5d8552a *stdin\" \\ && apk add --no-cache --virtual .cert-deps \\ openssl \\ && wget -O /tmp/nginx_signing.rsa.pub https://nginx.org/keys/nginx_signing.rsa.pub \\ && if [ \"$(openssl rsa -pubin -in /tmp/nginx_signing.rsa.pub -text -noout | openssl sha512 -r)\" = \"$KEY_SHA512\" ]; then \\ echo \"key verification succeeded!\"; \\ mv /tmp/nginx_signing.rsa.pub /etc/apk/keys/; \\ else \\ echo \"key verification failed!\"; \\ exit 1; \\ fi \\ && apk del .cert-deps \\ && apk add -X \"https://nginx.org/packages/mainline/alpine/v$(egrep -o '^[0-9]+\\.[0-9]+' /etc/alpine-release)/main\" --no-cache $nginxPackages \\ ;; \\ *) \\ # we're on an architecture upstream doesn't officially build for # let's build binaries from the published packaging sources set -x \\ && tempDir=\"$(mktemp -d)\" \\ && chown nobody:nobody $tempDir \\ && apk add --no-cache --virtual .build-deps \\ gcc \\ libc-dev \\ make \\ openssl-dev \\ pcre-dev \\ zlib-dev \\ linux-headers \\ libxslt-dev \\ gd-dev \\ geoip-dev \\ perl-dev \\ libedit-dev \\ mercurial \\ bash \\ alpine-sdk \\ findutils \\ && su nobody -s /bin/sh -c \" \\ export HOME=${tempDir} \\ && cd ${tempDir} \\ && hg clone https://hg.nginx.org/pkg-oss \\ && cd pkg-oss \\ && hg up ${NGINX_VERSION}-${PKG_RELEASE} \\ && cd alpine \\ && make all \\ && apk index -o ${tempDir}/packages/alpine/${apkArch}/APKINDEX.tar.gz ${tempDir}/packages/alpine/${apkArch}/*.apk \\ && abuild-sign -k ${tempDir}/.abuild/abuild-key.rsa ${tempDir}/packages/alpine/${apkArch}/APKINDEX.tar.gz \\ \" \\ && cp ${tempDir}/.abuild/abuild-key.rsa.pub /etc/apk/keys/ \\ && apk del .build-deps \\ && apk add -X ${tempDir}/packages/alpine/ --no-cache $nginxPackages \\ ;; \\ esac \\ # if we have leftovers from building, let's purge them (including extra, unnecessary build deps) && if [ -n \"$tempDir\" ]; then rm -rf \"$tempDir\"; fi \\ && if [ -n \"/etc/apk/keys/abuild-key.rsa.pub\" ]; then rm -f /etc/apk/keys/abuild-key.rsa.pub; fi \\ && if [ -n \"/etc/apk/keys/nginx_signing.rsa.pub\" ]; then rm -f /etc/apk/keys/nginx_signing.rsa.pub; fi \\ # Bring in gettext so we can get `envsubst`, then throw # the rest away. To do this, we need to install `gettext` # then move `envsubst` out of the way so `gettext` can # be deleted completely, then move `envsubst` back. && apk add --no-cache --virtual .gettext gettext \\ && mv /usr/bin/envsubst /tmp/ \\ \\ && runDeps=\"$( \\ scanelf --needed --nobanner /tmp/envsubst \\ | awk '{ gsub(/,/, \"\\nso:\", $2); print \"so:\" $2 }' \\ | sort -u \\ | xargs -r apk info --installed \\ | sort -u \\ )\" \\ && apk add --no-cache $runDeps \\ && apk del .gettext \\ && mv /tmp/envsubst /usr/local/bin/ \\ # Bring in tzdata so users could set the timezones through the environment # variables && apk add --no-cache tzdata \\ # forward request and error logs to docker log collector && ln -sf /dev/stdout /var/log/nginx/access.log \\ && ln -sf /dev/stderr /var/log/nginx/error.log EXPOSE 80 STOPSIGNAL SIGTERM CMD [\"nginx\", \"-g\", \"daemon off;\"]","categories":[{"name":"容器","slug":"容器","permalink":"https://cyylog.github.io/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://cyylog.github.io/tags/Docker/"}]},{"title":"Docker_000","slug":"容器/Dockerfilee-000","date":"2019-11-15T14:16:23.000Z","updated":"2020-10-25T23:08:19.674Z","comments":true,"path":"2019/11/15/rong-qi/dockerfilee-000/","link":"","permalink":"https://cyylog.github.io/2019/11/15/rong-qi/dockerfilee-000/","excerpt":"","text":"Dockerfile构建镜像通过Dockerfile创建镜像 虽然可以自己制作 rootfs(见&#39;容器文件系统那些事儿&#39;)，但Docker 提供了一种更便捷的方式，叫作 Dockerfile docker build命令用于根据给定的Dockerfile和上下文以构建Docker镜像。 docker build语法： # docker build [OPTIONS] &lt;PATH | URL | -&gt; 1. 常用选项说明 --build-arg，设置构建时的变量 --no-cache，默认false。设置该选项，将不使用Build Cache构建镜像 --pull，默认false。设置该选项，总是尝试pull镜像的最新版本 --compress，默认false。设置该选项，将使用gzip压缩构建的上下文 --disable-content-trust，默认true。设置该选项，将对镜像进行验证 --file, -f，Dockerfile的完整路径，默认值为‘PATH/Dockerfile’ --isolation，默认--isolation=&quot;default&quot;，即Linux命名空间；其他还有process或hyperv --label，为生成的镜像设置metadata --squash，默认false。设置该选项，将新构建出的多个层压缩为一个新层，但是将无法在多个镜像之间共享新层；设置该选项，实际上是创建了新image，同时保留原有image。 --tag, -t，镜像的名字及tag，通常name:tag或者name格式；可以在一次构建中为一个镜像设置多个tag --network，默认default。设置该选项，Set the networking mode for the RUN instructions during build --quiet, -q ，默认false。设置该选项，Suppress the build output and print image ID on success --force-rm，默认false。设置该选项，总是删除掉中间环节的容器 --rm，默认--rm=true，即整个构建过程成功后删除中间环节的容器 2. PATH | URL | -说明： 给出命令执行的上下文。 上下文可以是构建执行所在的本地路径，也可以是远程URL，如Git库、tarball或文本文件等。 如果是Git库，如https://github.com/docker/rootfs.git#container:docker，则隐含先执行git clone --depth 1 --recursive，到本地临时目录；然后再将该临时目录发送给构建进程。 构建镜像的进程中，可以通过ADD命令将上下文中的任何文件（注意文件必须在上下文中）加入到镜像中。 -表示通过STDIN给出Dockerfile或上下文。 示例： docker build - &lt; Dockerfile 说明：该构建过程只有Dockerfile，没有上下文 docker build - &lt; context.tar.gz 说明：其中Dockerfile位于context.tar.gz的根路径 docker build -t champagne/bbauto:latest -t champagne/bbauto:v2.1 . docker build -f dockerfiles/Dockerfile.debug -t myapp_debug . 2.1、 创建镜像所在的文件夹和Dockerfile文件 命令： 1、mkdir sinatra 2、cd sinatra 3、touch Dockerfile 2.2、 在Dockerfile文件中写入指令，每一条指令都会更新镜像的信息例如： # This is a comment FROM ubuntu:14.04 MAINTAINER tiger tiger@localhost.localdomain RUN apt-get update &amp;&amp; apt-get install -y ruby ruby-dev RUN gem install sinatra 格式说明： 每行命令都是以 INSTRUCTION statement 形式，就是命令+ 清单的模式。命令要大写，&quot;#&quot;是注解。 FROM 命令是告诉docker 我们的镜像什么。 MAINTAINER 是描述 镜像的创建人。 RUN 命令是在镜像内部执行。就是说他后面的命令应该是针对镜像可以运行的命令。 2.3、创建镜像 命令：docker build -t tiger/sinatra:v2 . docker build 是docker创建镜像的命令 -t 是标识新建的镜像属于 ouruser的 sinatra是仓库的名称 ：v2 是tag &quot;.&quot;是用来指明 我们的使用的Dockerfile文件当前目录的 详细执行过程： [root@master sinatra]# docker build -t tiger/sinatra:v2 . Sending build context to Docker daemon 2.048 kB Step 1 : FROM daocloud.io/ubuntu:14.04 Trying to pull repository daocloud.io/ubuntu ... 14.04: Pulling from daocloud.io/ubuntu f3ead5e8856b: Pull complete Digest: sha256:ea2b82924b078d9c8b5d3f0db585297a5cd5b9c2f7b60258cdbf9d3b9181d828 ---&gt; 2ff3b426bbaa Step 2 : MAINTAINER tiger tiger@localhost.localdomain ---&gt; Running in 948396c9edaa ---&gt; 227da301bad8 Removing intermediate container 948396c9edaa Step 3 : RUN apt-get update &amp;&amp; apt-get install -y ruby ruby-dev ... Step 4 : RUN gem install sinatra ---&gt; Running in 89234cb493d9 2.4、创建完成后，从镜像创建容器 #docker run -t -i tiger/sinatra:v2 /bin/bashDockerfile分为四个部分: 基础镜像信息、维护者信息、镜像操作指令和容器启动指令。 即FROM、MAINTAINER、RUN、CMD四个部分 指令说明FROM 指定所创建镜像的基础镜像 MAINTAINER 制定维护者信息 RUN 运行命令 CMD 容器启动是默认执行的命令 LABEL 指定生成镜像的元数据标签信息 EXPOSE 声明镜像内服务所监听的端口 ENV 指定环境变量 ADD 复制指定src路径的内容到容器的dest路径下，如果src为tar文件，则自动解压到dest路径下 copy 复制指定src路径的内容到镜像的dest路径下 ENTERPOINT 指定镜像的默认入口 VOLUME 创建数据卷挂载点 USER 指定运行容器是的用户名或UID WORKDIR 配置工作目录 ARG 指定镜像内使用的参数 ONBUILD 配置当所创建的镜像作为其他镜像的基础镜像时，所执行创建操作指令 STOPSIGAL 容器退出信号值 HEALTHCHECK 如何进行健康检查 SHELL 指定使用shell的默认shell类型nginx-dockerfile示例vim Dockerfile FROM centos:7.2.1511 ENV TZ=Asia/Shanghai RUN yum -y install epel* \\ yum -y install gcc openssl openssl-devel pcre-devel zlib-devel ADD nginx-1.14.2.tar.gz /opt/ WORKDIR /opt/nginx-1.14.2 RUN ./configure --prefix=/opt/nginx --http-log-path=/opt/nginx/logs/access.log --error-log-path=/opt/nginx/logs/error.log --http-client-body-temp-path=/opt/nginx/client/ --http-proxy-temp-path=/opt/nginx/proxy/ --with-http_stub_status_module --with-file-aio --with-http_flv_module --with-http_gzip_static_module --with-stream --with-threads --user=www --group=www RUN make &amp;&amp; make install RUN groupadd www &amp;&amp; useradd -g www www WORKDIR /opt/nginx RUN rm -rf /opt/nginx-1.14.2 ENV NGINX_HOME=/opt/nginx ENV PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/nginx/sbin EXPOSE 80 443 CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]需要先下载nginx-1.14.2.tar.gz在Dockerfile同级目录下，然后执行如下命令 docker build -t nginx_image ./Dockerfile tomcat-dockerfile示例FROM centos:7.4.1708 ADD jdk-8u171-linux-x64.tar.gz /usr/local/ ADD apache-tomcat-7.0.88.tar.gz /usr/local/ WORKDIR /usr/local/ RUN mv jdk1.8.0_171 jdk &amp;&amp; mv apache-tomcat-7.0.88 tomcat ENV JAVA_HOME=/usr/local/jdk ENV CLASS_PATH=$JAVA_HOME/lib:$JAVA_HOME/jre/lib ENV PATH=$JAVA_HOME/bin:$PATH ENV CATALINA_HOME /usr/local/tomcat EXPOSE 8080 CMD /usr/local/tomcat/bin/catalina.sh run需要先下载jdk和tomcat在dockerfile的同级目录下，然后执行如下命令 docker build -t tomcat_image ./Dockerfile 容器网络 小规模docker环境大部分运行在单台主机上，如果公司大规模采用docker，那么多个宿主机上的docker如何互联 Docker默认的内部ip为172.17.42.0网段，所以必须要修改其中一台的默认网段以免ip冲突。 #vim /etc/sysconfig/docker-network DOCKER_NETWORK_OPTIONS= --bip=172.18.42.1/16 #reboot docker 130上： #route add -net 172.18.0.0/16 gw 192.168.18.128 docker 128上： #route add -net 172.17.0.0/16 gw 192.168.18.130 现在两台宿主机里的容器就可以通信了。容器固定IPdocker安装后，默认会创建三种网络类型，bridge、host和none 显示当前网络： # docker network list NETWORK ID NAME DRIVER SCOPE 90b22f633d2f bridge bridge local e0b365da7fd2 host host local da7b7a090837 none null local bridge:网络桥接 默认情况下启动、创建容器都是用该模式，所以每次docker容器重启时会按照顺序获取对应ip地址，这就导致容器每次重启，ip都发生变化 none：无指定网络 启动容器时，可以通过–network=none,docker容器不会分配局域网ip host：主机网络 docker容器的网络会附属在主机上，两者是互通的。 创建固定ip容器 1、创建自定义网络类型，并且指定网段 #docker network create --subnet=192.168.0.0/16 staticnet 通过docker network ls可以查看到网络类型中多了一个staticnet 2、使用新的网络类型创建并启动容器 #docker run -it --name userserver --net staticnet --ip 192.168.0.2 centos:6 /bin/bash 通过docker inspect可以查看容器ip为192.168.0.2，关闭容器并重启，发现容器ip并未发生改变","categories":[{"name":"容器","slug":"容器","permalink":"https://cyylog.github.io/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://cyylog.github.io/tags/Docker/"}]},{"title":"zabbix-微信报警设置","slug":"监控/zabbix/zabbix配置企业微信报警","date":"2019-11-04T17:55:39.000Z","updated":"2020-10-30T04:12:01.798Z","comments":true,"path":"2019/11/05/jian-kong/zabbix/zabbix-pei-zhi-qi-ye-wei-xin-bao-jing/","link":"","permalink":"https://cyylog.github.io/2019/11/05/jian-kong/zabbix/zabbix-pei-zhi-qi-ye-wei-xin-bao-jing/","excerpt":"","text":"zabbix 微信报警设置一、主要获取三个参数:企业ID、用户账号、AgentId,和Secret：1.获取企业ID 2.获取AgentId,和Secret3这里要先点通讯录创建一个部门，然后再点应用小程序创建应用，填写logo、名称、和选择部门就可以了 3.获取用户账号 4.测试gentId,和Secret这个是接口调用测试gentId,和Secret的地址：https://work.weixin.qq.com/api/devtools/devtool.php 这里看到有HTTP/1.1 200 OK 就说明接口有效了，其它的不管。 二、调用的shell脚本方式，脚本如下：这里要注意的是填写正确的通讯录 部门ID，可以点那个下线三个点那里。 [root@cyy alertscripts]# vim wechat.sh #!/usr/bin/env bash #!/usr/bin/env bash # # Author: cyylog # Email: cyylog@aliyun.com # Date: 2019/09/25 # Github: https://github.com/cyylog # Usage: Wechat alert script for zabbix # if [ $# -eq 0 ] || [[ \"$1\" == \"-h\" || \"$1\" == \"--help\" ]];then echo \"Usage of $0:\" echo -e \" --CorpID=string\" echo -e \" --Secret=string\" echo -e \" --AgentID=string\" echo -e \" --UserID=string\" echo -e \" --Msg=string\" exit fi #ops=(-c -s -a -u) #args=(CorpID Secret AgentID UserID) #while [ $# -gt 0 ];do # [ \"$1\" == \"-m\" ] && Msg=\"$2\" && shift 2 # for i in {0..3};do # [ \"$1\" == \"${ops[i]}\" ] && eval ${args[i]}=\"$2\" # done # shift 2 #done for i in \"$@\";do echo $i|grep Msg &> /dev/null && msg=$(echo $i|sed 's/.*=//') && Msg=\"$msg\" && continue eval \"$(echo $i|sed 's/--//')\" done #echo $CorpID #echo $Secret #echo $UserID #echo $AgentID #echo $Msg # GURL=\"https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=$CorpID&corpsecret=$Secret\" Token=$(/usr/bin/curl -s -G $GURL |awk -F \\\" '{print $10}') PURL=\"https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=$Token\" Info(){ printf '{\\n' printf '\\t\"touser\": \"'\"$UserID\"\\\"\",\\n\" printf '\\t\"msgtype\": \"text\",\\n' printf '\\t\"agentid\": \"'\"$AgentID\"\\\"\",\\n\" printf '\\t\"text\": {\\n' printf '\\t\\t\"content\": \"'\"$Msg\"\\\"\"\\n\" printf '\\t},\\n' printf '\\t\"safe\":\"0\"\\n' printf '}\\n' } /usr/bin/curl --data-ascii \"$(Info)\" $PURL echo [root@cyy alertscripts]# chmod +x wechat.sh [root@cyy alertscripts]# ./wechat.sh \"这里一个测试\" //可以这样直接调试，然后登陆到企业微信查看该部门的群成员是否收到此信息 脚本测试通过后就是在zabbix控制台上设置了 三、zabbix 控制台添加新媒体1.点管理 -&gt; 报警媒介类型 -&gt; 创建媒介类型 --AgentID=1000002 --CorpID=ww74c********56c --Secret=-c-3Xw*****************j-Zj6cw --Msg={ALERT.MESSAGE} --UserID={ALERT.SENDTO} 2.然后再设置上用户：点管理 —&gt; 创建用户（微信报警的用户） 3.再点用户旁边的 报警媒介 进行设置（收件人要填写用户的账号）第一步的第3点获取的账号 到这里就基本都设置完成了，可以设置个触发器和动作来测试脚本。","categories":[{"name":"监控","slug":"监控","permalink":"https://cyylog.github.io/categories/%E7%9B%91%E6%8E%A7/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://cyylog.github.io/tags/zabbix/"}]},{"title":"搭建Gitlab","slug":"DevOPs/搭建Gitlab","date":"2019-10-04T17:55:39.000Z","updated":"2020-09-28T05:15:59.689Z","comments":true,"path":"2019/10/05/devops/da-jian-gitlab/","link":"","permalink":"https://cyylog.github.io/2019/10/05/devops/da-jian-gitlab/","excerpt":"","text":"Gitlab Server 部署 1、环境准备1.系统版本：CentOS7.4 2.Gitlab版本：gitlab-ee 11.0.1 3.初始化系统环境 4.关闭防火墙 [root@localhost ~]# systemctl stop iptables firewalld [root@localhost ~]# systemctl disable iptables firewalld 5.开启邮件服务 [root@vm1 ~]# systemctl start postfix [root@vm1 ~]# systemctl enable postfix 6.关闭SELinux [root@localhost ~]# sed -ri &#39;/SELINUX=/cSELINUX=disabled&#39; /etc/selinux/config [root@localhost ~]# setenforce 0 # 临时关闭SELinux [root@localhost ~]# reboot 2、部署Gitlab1.安装Gitlab社区版/企业版 2.安装gitlab依赖包 [root@localhost ~]# yum install -y curl openssh-server openssh-clients postfix cronie policycoreutils-python # gitlab-ce 10.x.x以后的版本需要依赖policycoreutils-python 3.开启postfix，并设置开机自启 [root@localhost ~]# systemctl start postfix;systemctl enable postfix 4.选择添加yum源安装gitlab(根据需求配置源) （1）添加阿里源 # vim /etc/yum.repos.d/gitlab-ce.repo [gitlab-ce] name=gitlab-ce baseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7 Repo_gpgcheck=0 Enabled=1 Gpgkey=https://packages.gitlab.com/gpg.key （2） 添加清华源 # vim gitlab-ce.repo [gitlab-ce] name=Gitlab CE Repository baseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el$releasever/ gpgcheck=0 enabled=1 # vim gitlab-ee.repo [gitlab-ee] name=Gitlab EE Repository baseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ee/yum/el$releasever/ gpgcheck=0 enabled=1 # vim runner_gitlab-ci-multi-runner.repo [runner_gitlab-ci-multi-runner] name=runner_gitlab-ci-multi-runner baseurl=https://packages.gitlab.com/runner/gitlab-ci-multi-runner/el/7/$basearch repo_gpgcheck=1 gpgcheck=0 enabled=1 gpgkey=https://packages.gitlab.com/runner/gitlab-ci-multi-runner/gpgkey sslverify=1 sslcacert=/etc/pki/tls/certs/ca-bundle.crt metadata_expire=300 [runner_gitlab-ci-multi-runner-source] name=runner_gitlab-ci-multi-runner-source baseurl=https://packages.gitlab.com/runner/gitlab-ci-multi-runner/el/7/SRPMS repo_gpgcheck=1 gpgcheck=0 enabled=1 gpgkey=https://packages.gitlab.com/runner/gitlab-ci-multi-runner/gpgkey sslverify=1 sslcacert=/etc/pki/tls/certs/ca-bundle.crt metadata_expire=300 (3) 添加官方源 curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.rpm.sh | sudo bash 5.安装包下载 https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/ https://mirrors.tuna.tsinghua.edu.cn/gitlab-ee/yum/el7/ 6.根据需要选择ce/ee [root@localhost ~]# yum -y install gitlab-ce # 自动安装最新版 [root@localhost ~]# yum -y install gitlab-ce-x.x.x # 安装指定版本Gitlab [root@localhost ~]# yum -y install gitlab-ce warning: gitlab-ce-10.7.2-ce.0.el7.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID f27eab47: NOKEY Preparing... ################################# [100%] Updating / installing... 1:gitlab-ce-10.7.2-ce.0.el7 ################################# [100%] It looks like GitLab has not been configured yet; skipping the upgrade script. *. *. *** *** ***** ***** .****** ******* ******** ******** ,,,,,,,,,***********,,,,,,,,, ,,,,,,,,,,,*********,,,,,,,,,,, .,,,,,,,,,,,*******,,,,,,,,,,,, ,,,,,,,,,*****,,,,,,,,,. ,,,,,,,****,,,,,, .,,,***,,,, ,*,. _______ __ __ __ / ____(_) /_/ / ____ _/ /_ / / __/ / __/ / / __ `/ __ \\ / /_/ / / /_/ /___/ /_/ / /_/ / \\____/_/\\__/_____/\\__,_/_.___/ Thank you for installing GitLab! GitLab was unable to detect a valid hostname for your instance. Please configure a URL for your GitLab instance by setting `external_url` configuration in /etc/gitlab/gitlab.rb file. Then, you can start your GitLab instance by running the following command: sudo gitlab-ctl reconfigure For a comprehensive list of configuration options please see the Omnibus GitLab readme https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/README.md###3、配置 Gitlab 1、查看Gitlab版本[root@localhost ~]# head -1 /opt/gitlab/version-manifest.txt gitlab-ce 10.1.12、Gitlab 配置文登录链接#设置登录链接 [root@localhost ~]# vim /etc/gitlab/gitlab.rb *** ## GitLab URL ##! URL on which GitLab will be reachable. ##! For more details on configuring external_url see: ##! https://docs.gitlab.com/omnibus/settings/configuration.html#configuring-the-external-url-for-gitlab # 没有域名，可以设置为本机IP地址 external_url &#39;http://172.17.0.61&#39; *** [root@localhost ~]# grep &quot;^external_url&quot; /etc/gitlab/gitlab.rb external_url &#39;http://172.17.0.61&#39; #绑定监听的域名或IP3、初始化 Gitlab (第一次使用配置时间较长) [root@localhost ~]# gitlab-ctl reconfigure .....4、启动 Gitlab 服务[root@vm1 ~]# gitlab-ctl start ok: run: gitaly: (pid 22896) 2922s ok: run: gitlab-monitor: (pid 22914) 2921s ok: run: gitlab-workhorse: (pid 22882) 2922s ok: run: logrotate: (pid 22517) 2987s ok: run: nginx: (pid 22500) 2993s ok: run: node-exporter: (pid 22584) 2974s ok: run: postgres-exporter: (pid 22946) 2919s ok: run: postgresql: (pid 22250) 3047s ok: run: prometheus: (pid 22931) 2920s ok: run: redis: (pid 22190) 3053s ok: run: redis-exporter: (pid 22732) 2962s ok: run: sidekiq: (pid 22472) 3005s ok: run: unicorn: (pid 22433) 3011s [root@vm1 ~]# [root@vm1 ~]# lsof -i:80 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME nginx 22500 root 7u IPv4 50923 0t0 TCP *:http (LISTEN) nginx 22501 gitlab-www 7u IPv4 50923 0t0 TCP *:http (LISTEN) [root@vm1 ~]# 5、Gitlab 设置 HTTPS 方式如果想要以上的 https 方式正常生效使用，则需要把 letsencrypt 自动生成证书的配置打开，这样在执行重 新让配置生效命令 (gitlab-ctl reconfigure) 的时候会自动给域名生成免费的证书并自动在 gitlab 自带的 nginx 中加上相关的跳转配置，都是全自动的，非常方便。 letsencrypt[&#39;enable&#39;] = true letsencrypt[&#39;contact_emails&#39;] = [&#39;caryyu@qq.com&#39;] # 这应该是一组要添加为联系人的电子邮件地址6、Gitlab 添加smtp邮件功能[root@vm1 ~]# vim /etc/gitlab/gitlab.rb postfix 并非必须的；根据具体情况配置，以 SMTP 的为例配置邮件服务器来实现通知；参考配置如下： ### Email Settings gitlab_rails[&#39;gitlab_email_enabled&#39;] = true gitlab_rails[&#39;gitlab_email_from&#39;] = &#39;system.notice@qq.com&#39; gitlab_rails[&#39;gitlab_email_display_name&#39;] = &#39;gitlab.notice&#39; gitlab_rails[&#39;gitlab_email_reply_to&#39;] = &#39;system.notice@qq.com&#39; gitlab_rails[&#39;gitlab_email_subject_suffix&#39;] = &#39;gitlab&#39; ### GitLab email server settings ###! Docs: https://docs.gitlab.com/omnibus/settings/smtp.html ###! **Use smtp instead of sendmail/postfix.** gitlab_rails[&#39;smtp_enable&#39;] = true gitlab_rails[&#39;smtp_address&#39;] = &quot;smtp.qq.com&quot; gitlab_rails[&#39;smtp_port&#39;] = 465 gitlab_rails[&#39;smtp_user_name&#39;] = &quot;system.notice@qq.com&quot; gitlab_rails[&#39;smtp_password&#39;] = &quot;xxxxx&quot; gitlab_rails[&#39;smtp_domain&#39;] = &quot;qq.com&quot; gitlab_rails[&#39;smtp_authentication&#39;] = &quot;login&quot; gitlab_rails[&#39;smtp_enable_starttls_auto&#39;] = true gitlab_rails[&#39;smtp_tls&#39;] = true [root@vm1 ~]# grep -P &quot;^[^#].*smtp_|user_email|gitlab_email&quot; /etc/gitlab/gitlab.rb gitlab_rails[&#39;gitlab_email_enabled&#39;] = true gitlab_rails[&#39;gitlab_email_from&#39;] = &#39;username@domain.cn&#39; gitlab_rails[&#39;gitlab_email_display_name&#39;] = &#39;Admin&#39; gitlab_rails[&#39;gitlab_email_reply_to&#39;] = &#39;usernamei@domain.cn&#39; gitlab_rails[&#39;gitlab_email_subject_suffix&#39;] = &#39;[gitlab]&#39; gitlab_rails[&#39;smtp_enable&#39;] = true gitlab_rails[&#39;smtp_address&#39;] = &quot;smtp.exmail.qq.com&quot; gitlab_rails[&#39;smtp_port&#39;] = 25 gitlab_rails[&#39;smtp_user_name&#39;] = &quot;username@domain.cn&quot; gitlab_rails[&#39;smtp_password&#39;] = &quot;password&quot; gitlab_rails[&#39;smtp_domain&#39;] = &quot;domain.cn&quot; gitlab_rails[&#39;smtp_authentication&#39;] = &quot;login&quot; gitlab_rails[&#39;smtp_enable_starttls_auto&#39;] = true gitlab_rails[&#39;smtp_tls&#39;] = false user[&#39;git_user_email&#39;] = &quot;username@domain.cn&quot; [root@vm1 ~]# gitlab-ctl reconfigure #修改配置后需要初始化配置 ...... [root@vm1 ~]# gitlab-ctl stop ok: down: gitaly: 0s, normally up ok: down: gitlab-monitor: 1s, normally up ok: down: gitlab-workhorse: 0s, normally up ok: down: logrotate: 1s, normally up ok: down: nginx: 0s, normally up ok: down: node-exporter: 1s, normally up ok: down: postgres-exporter: 0s, normally up ok: down: postgresql: 0s, normally up ok: down: prometheus: 0s, normally up ok: down: redis: 0s, normally up ok: down: redis-exporter: 1s, normally up ok: down: sidekiq: 0s, normally up ok: down: unicorn: 1s, normally up [root@vm1 ~]# gitlab-ctl start ok: run: gitaly: (pid 37603) 0s ok: run: gitlab-monitor: (pid 37613) 0s ok: run: gitlab-workhorse: (pid 37625) 0s ok: run: logrotate: (pid 37631) 0s ok: run: nginx: (pid 37639) 1s ok: run: node-exporter: (pid 37644) 0s ok: run: postgres-exporter: (pid 37648) 1s ok: run: postgresql: (pid 37652) 0s ok: run: prometheus: (pid 37660) 1s ok: run: redis: (pid 37668) 0s ok: run: redis-exporter: (pid 37746) 0s ok: run: sidekiq: (pid 37750) 1s ok: run: unicorn: (pid 37757) 0s7、Gitlab 发送邮件测试[root@vm1 ~]# gitlab-rails console Loading production environment (Rails 4.2.10) irb(main):001:0&gt; Notify.test_email(&#39;user@destination.com&#39;, &#39;Message Subject&#39;, &#39;Message Body&#39;).deliver_now Notify#test_email: processed outbound mail in 2219.5ms Sent mail to user@destination.com (2469.5ms) Date: Fri, 04 May 2018 15:50:10 +0800 From: Admin &lt;username@domain.cn&gt; Reply-To: Admin &lt;username@domain.cn&gt; To: user@destination.com Message-ID: &lt;5aec10b24cfaa_93933fee282db10c162d@vm1.mail&gt; Subject: Message Subject Mime-Version: 1.0 Content-Type: text/html; charset=UTF-8 Content-Transfer-Encoding: 7bit Auto-Submitted: auto-generated X-Auto-Response-Suppress: All &lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.0 Transitional//EN&quot; &quot;http://www.w3.org/TR/REC-html40/loose.dtd&quot;&gt; &lt;html&gt;&lt;body&gt;&lt;p&gt;Message Body&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; =&gt; #&lt;Mail::Message:70291731344240, Multipart: false, Headers: &lt;Date: Fri, 04 May 2018 15:50:10 +0800&gt;, &lt;From: Admin &lt;username@domain.cn&gt;&gt;, &lt;Reply-To: Admin &lt;username@domain.cn&gt;&gt;, &lt;To: user@destination.com&gt;, &lt;Message-ID: &lt;5aec10b24cfaa_93933fee282db10c162d@vm1.mail&gt;&gt;, &lt;Subject: Message Subject&gt;, &lt;Mime-Version: 1.0&gt;, &lt;Content-Type: text/html; charset=UTF-8&gt;, &lt;Content-Transfer-Encoding: 7bit&gt;, &lt;Auto-Submitted: auto-generated&gt;, &lt;X-Auto-Response-Suppress: All&gt;&gt; irb(main):002:0&gt;quit ###3、gitlab的使用 在浏览器中输入 http://192.168.60.119/ ，然后 change password: ，并使用root用户登录 即可 (后续动作根据提示操作) 1、gitlab 命令行修改密码gitlab-rails console production irb(main):001:0&gt; user = User.where(id: 1).first # id为1的是超级管理员 irb(main):002:0&gt;user.password = &#39;yourpassword&#39; # 密码必须至少8个字符 irb(main):003:0&gt;user.save! # 如没有问题 返回true exit # 退出2、gitlab服务管理gitlab-ctl start # 启动所有 gitlab 组件； gitlab-ctl stop # 停止所有 gitlab 组件； gitlab-ctl restart # 重启所有 gitlab 组件； gitlab-ctl status # 查看服务状态； gitlab-ctl reconfigure # 启动服务； vim /etc/gitlab/gitlab.rb # 修改默认的配置文件； gitlab-ctl tail # 查看日志；3、登陆 Gitlab 如果需要手工修改nginx的port ，可以在gitlab.rb中设置 nginx[‘listen_port’] = 8000 ，然后再次 gitlab-ctl reconfigure即可 登录 gitlab 如下所示(首次登陆设置 root 密码)： 创建项目组 group ，组名为plat-sp ,如下所示: 去掉用户的自动注册功能（安全）： admin are -&gt; settings -&gt; Sign-up Restrictions 去掉钩钩，然后拉到最下面保存，重新登录","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://cyylog.github.io/categories/DevOps/"}],"tags":[{"name":"Gitlab","slug":"Gitlab","permalink":"https://cyylog.github.io/tags/Gitlab/"}]},{"title":"linux服务器被黑排查思路","slug":"Linux/服务器被黑排查思路","date":"2019-10-04T17:55:39.000Z","updated":"2020-05-25T13:52:03.920Z","comments":true,"path":"2019/10/05/linux/fu-wu-qi-bei-hei-pai-cha-si-lu/","link":"","permalink":"https://cyylog.github.io/2019/10/05/linux/fu-wu-qi-bei-hei-pai-cha-si-lu/","excerpt":"","text":"账户和登录安全账户安全是系统安全的第一道屏障，也是系统安全的核心，保障登录账户的安全，在一定程度上可以提高服务器的安全级别，下面重点介绍下 Linux 系统登录账户的安全设置方法。 ①删除特殊的账户和账户组Linux 提供了各种不同角色的系统账号，在系统安装完成后，默认会安装很多不必要的用户和用户组。 如果不需要某些用户或者组，就要立即删除它，因为账户越多，系统就越不安全，很可能被黑客利用，进而威胁到服务器的安全。 Linux系统中可以删除的默认用户和组大致有如下这些： 可删除的用户，如 adm，lp，sync，shutdown，halt，news，uucp，operator，games，gopher 等。 可删除的组，如 adm，lp，news，uucp，games，dip，pppusers，popusers，slipusers 等。 ②关闭系统不需要的服务Linux 在安装完成后，绑定了很多没用的服务，这些服务默认都是自动启动的。 对于服务器来说，运行的服务越多，系统就越不安全，越少服务在运行，安全性就越好，因此关闭一些不需要的服务，对系统安全有很大的帮助。 具体哪些服务可以关闭，要根据服务器的用途而定，一般情况下，只要系统本身用不到的服务都认为是不必要的服务。 例如：某台 Linux 服务器用于 www 应用，那么除了 httpd 服务和系统运行是必须的服务外，其他服务都可以关闭。 下面这些服务一般情况下是不需要的，可以选择关闭： anacron、auditd、autofs、avahi-daemon、avahi-dnsconfd、bluetooth、cpuspeed、firstboot、gpm、haldaemon、hidd、ip6tables、ipsec、isdn、lpd、mcstrans、messagebus、netfs、nfs、nfslock、nscd、pcscd portmap、readahead_early、restorecond、rpcgssd、rpcidmapd、rstatd、sendmail、setroubleshoot、yppasswdd ypserv ③密码安全策略在 Linux 下，远程登录系统有两种认证方式： 密码认证 密钥认证 密码认证方式是传统的安全策略，对于密码的设置，比较普遍的说法是：至少 6 个字符以上，密码要包含数字、字母、下划线、特殊符号等。 设置一个相对复杂的密码，对系统安全能起到一定的防护作用，但是也面临一些其他问题，例如密码暴力破解、密码泄露、密码丢失等，同时过于复杂的密码对运维工作也会造成一定的负担。 密钥认证是一种新型的认证方式，公用密钥存储在远程服务器上，专用密钥保存在本地，当需要登录系统时，通过本地专用密钥和远程服务器的公用密钥进行配对认证，如果认证成功，就成功登录系统。 这种认证方式避免了被暴力破解的危险，同时只要保存在本地的专用密钥不被黑客盗用，攻击者一般无法通过密钥认证的方式进入系统。 因此，在 Linux 下推荐用密钥认证方式登录系统，这样就可以抛弃密码认证登录系统的弊端。 Linux 服务器一般通过 SecureCRT、Putty、Xshell 之类的工具进行远程维护和管理，密钥认证方式的实现就是借助于 SecureCRT 软件和 Linux 系统中的 SSH 服务实现的。 ④合理使用 su、sudo 命令su 命令：是一个切换用户的工具，经常用于将普通用户切换到超级用户下，当然也可以从超级用户切换到普通用户。 为了保证服务器的安全，几乎所有服务器都禁止了超级用户直接登录系统，而是通过普通用户登录系统，然后再通过 su 命令切换到超级用户下，执行一些需要超级权限的工作。 通过 su 命令能够给系统管理带来一定的方便，但是也存在不安全的因素，例如：系统有 10 个普通用户，每个用户都需要执行一些有超级权限的操作，就必须把超级用户的密码交给这 10 个普通用户。 如果这 10 个用户都有超级权限，通过超级权限可以做任何事，那么会在一定程度上对系统的安全造成了威协。 因此 su 命令在很多人都需要参与的系统管理中，并不是最好的选择，超级用户密码应该掌握在少数人手中，此时 sudo 命令就派上用场了。 sudo 命令：允许系统管理员分配给普通用户一些合理的“权利”，并且不需要普通用户知道超级用户密码，就能让他们执行一些只有超级用户或其他特许用户才能完成的任务。 比如：系统服务重启、编辑系统配置文件等，通过这种方式不但能减少超级用户登录次数和管理时间，也提高了系统安全性。 因此，sudo 命令相对于权限无限制性的 su 来说，还是比较安全的，所以 sudo 也被称为受限制的 su，另外 sudo 也是需要事先进行授权认证的，所以也被称为授权认证的 su。 sudo 执行命令的流程是：将当前用户切换到超级用户下，或切换到指定的用户下，然后以超级用户或其指定切换到的用户身份执行命令。 执行完成后，直接退回到当前用户，而这一切的完成要通过 sudo 的配置文件 /etc/sudoers 来进行授权。 sudo 设计的宗旨是：赋予用户尽可能少的权限但仍允许它们完成自己的工作，这种设计兼顾了安全性和易用性。 因此，强烈推荐通过 sudo 来管理系统账号的安全，只允许普通用户登录系统，如果这些用户需要特殊的权限，就通过配置 /etc/sudoers 来完成，这也是多用户系统下账号安全管理的基本方式。 ⑤删减系统登录欢迎信息系统的一些欢迎信息或版本信息，虽然能给系统管理者带来一定的方便，但是这些信息有时候可能被黑客利用，成为攻击服务器的帮凶。 为了保证系统的安全，可以修改或删除某些系统文件，需要修改或删除的文件有四个，分别是： /etc/issue /etc/issue.net /etc/redhat-release /etc/motd /etc/issue 和 /etc/issue.net 文件都记录了操作系统的名称和版本号，当用户通过本地终端或本地虚拟控制台等登录系统时，/etc/issue 的文件内容就会显示。 当用户通过 ssh 或 telnet 等远程登录系统时，/etc/issue.net 文件内容就会在登录后显示。 在默认情况下 /etc/issue.net 文件的内容是不会在 ssh 登录后显示的，要显示这个信息可以修改 /etc/ssh/sshd_config 文件，在此文件中添加如下内容即可：Banner /etc/issue.net。 其实这些登录提示很明显泄漏了系统信息，为了安全起见，建议将此文件中的内容删除或修改。 /etc/redhat-release 文件也记录了操作系统的名称和版本号，为了安全起见，可以将此文件中的内容删除。 /etc/motd 文件是系统的公告信息。每次用户登录后，/etc/motd 文件的内容就会显示在用户的终端。 通过这个文件系统，管理员可以发布一些软件或硬件的升级、系统维护等通告信息，但是此文件的最大作用就是可以发布一些警告信息，当黑客登录系统后，会发现这些警告信息，进而产生一些震慑作用。 看过国外的一个报道，黑客入侵了一个服务器，而这个服务器却给出了欢迎登录的信息，因此法院不做任何裁决。 远程访问和认证安全①远程登录取消 telnet 而采用 SSH 方式telnet 是一种古老的远程登录认证服务，它在网络上用明文传送口令和数据，因此别有用心的人就会非常容易截获这些口令和数据。 而且，telnet 服务程序的安全验证方式也极其脆弱，攻击者可以轻松将虚假信息传送给服务器。 现在远程登录基本抛弃了 telnet 这种方式，而取而代之的是通过 SSH 服务远程登录服务器。 ②合理使用 Shell 历史命令记录功能在 Linux 下可通过 history 命令查看用户所有的历史操作记录，同时 shell 命令操作记录默认保存在用户目录下的 .bash_history 文件中。 通过这个文件可以查询 shell 命令的执行历史，有助于运维人员进行系统审计和问题排查。 同时，在服务器遭受黑客攻击后，也可以通过这个命令或文件查询黑客登录服务器所执行的历史命令操作。 但是有时候黑客在入侵服务器后为了毁灭痕迹，可能会删除 .bash_history 文件，这就需要合理的保护或备份 .bash_history 文件。 ③启用 Tcp_Wrappers 防火墙Tcp_Wrappers 是一个用来分析 TCP/IP 封包的软件，类似的 IP 封包软件还有 iptables。 Linux 默认都安装了 Tcp_Wrappers。作为一个安全的系统，Linux 本身有两层安全防火墙，通过 IP 过滤机制的 iptables 实现第一层防护。 iptables 防火墙通过直观地监视系统的运行状况，阻挡网络中的一些恶意攻击，保护整个系统正常运行，免遭攻击和破坏。 如果通过了第一层防护，那么下一层防护就是 Tcp_Wrappers 了。通过 Tcp_Wrappers 可以实现对系统中提供的某些服务的开放与关闭、允许和禁止，从而更有效地保证系统安全运行。 文件系统安全①锁定系统重要文件系统运维人员有时候可能会遇到通过 Root 用户都不能修改或者删除某个文件的情况，产生这种情况的大部分原因可能是这个文件被锁定了。 在 Linux 下锁定文件的命令是 Chattr，通过这个命令可以修改 ext2、ext3、ext4 文件系统下文件属性，但是这个命令必须有超级用户 Root 来执行。和这个命令对应的命令是 lsattr，这个命令用来查询文件属性。 对重要的文件进行加锁，虽然能够提高服务器的安全性，但是也会带来一些不便。 例如：在软件的安装、升级时可能需要去掉有关目录和文件的 immutable 属性和 append-only 属性，同时，对日志文件设置了 append-only 属性，可能会使日志轮换（logrotate）无法进行。 因此，在使用 Chattr 命令前，需要结合服务器的应用环境来权衡是否需要设置 immutable 属性和 append-only 属性。 另外，虽然通过 Chattr 命令修改文件属性能够提高文件系统的安全性，但是它并不适合所有的目录。Chattr 命令不能保护 /、/dev、/tmp、/var 等目录。 根目录不能有不可修改属性，因为如果根目录具有不可修改属性，那么系统根本无法工作： /dev 在启动时，syslog 需要删除并重新建立 /dev/log 套接字设备，如果设置了不可修改属性，那么可能出问题。 /tmp 目录会有很多应用程序和系统程序需要在这个目录下建立临时文件，也不能设置不可修改属性。 /var 是系统和程序的日志目录，如果设置为不可修改属性，那么系统写日志将无法进行，所以也不能通过 Chattr 命令保护。 ②文件权限检查和修改不正确的权限设置直接威胁着系统的安全，因此运维人员应该能及时发现这些不正确的权限设置，并立刻修正，防患于未然。下面列举几种查找系统不安全权限的方法。 查找系统中任何用户都有写权限的文件或目录： 查找文件：find / -type f -perm -2 -o -perm -20 |xargs ls -al 查找目录：find / -type d -perm -2 -o -perm -20 |xargs ls –ld查找系统中所有含“s”位的程序： find / -type f -perm -4000 -o -perm -2000 -print | xargs ls –al含有“s”位权限的程序对系统安全威胁很大，通过查找系统中所有具有“s”位权限的程序，可以把某些不必要的“s”位程序去掉，这样可以防止用户滥用权限或提升权限的可能性。 检查系统中所有 suid 及 sgid 文件： find / -user root -perm -2000 -print -exec md5sum {} ; find / -user root -perm -4000 -print -exec md5sum {} ;将检查的结果保存到文件中，可在以后的系统检查中作为参考。 检查系统中没有属主的文件： find / -nouser -o –nogroup没有属主的孤儿文件比较危险，往往成为黑客利用的工具，因此找到这些文件后，要么删除掉，要么修改文件的属主，使其处于安全状态。 ③/tmp、/var/tmp、/dev/shm 安全设定在 Linux 系统中，主要有两个目录或分区用来存放临时文件，分别是 /tmp 和 /var/tmp。 存储临时文件的目录或分区有个共同点就是所有用户可读写、可执行，这就为系统留下了安全隐患。 攻击者可以将病毒或者木马脚本放到临时文件的目录下进行信息收集或伪装，严重影响服务器的安全。 此时，如果修改临时目录的读写执行权限，还有可能影响系统上应用程序的正常运行，因此，如果要兼顾两者，就需要对这两个目录或分区进行特殊的设置。 /dev/shm 是 Linux 下的一个共享内存设备，在 Linux 启动的时候系统默认会加载 /dev/shm，被加载的 /dev/shm 使用的是 tmpfs 文件系统，而 tmpfs 是一个内存文件系统，存储到 tmpfs 文件系统的数据会完全驻留在 RAM 中。 这样通过 /dev/shm 就可以直接操控系统内存，这将非常危险，因此如何保证 /dev/shm 安全也至关重要。 对于 /tmp 的安全设置，需要看 /tmp 是一个独立磁盘分区，还是一个根分区下的文件夹。 如果 /tmp 是一个独立的磁盘分区，那么设置非常简单，修改 /etc/fstab 文件中 /tmp 分区对应的挂载属性，加上 nosuid、noexec、nodev 三个选项即可。 修改后的 /tmp 分区挂载属性类似如下： LABEL=/tmp /tmp ext3 rw,nosuid,noexec,nodev 0 0其中，nosuid、noexec、nodev 选项，表示不允许任何 suid 程序，并且在这个分区不能执行任何脚本等程序，并且不存在设备文件。 在挂载属性设置完成后，重新挂载 /tmp 分区，保证设置生效。 对于 /var/tmp，如果是独立分区，安装 /tmp 的设置方法是修改 /etc/fstab 文件即可。 如果是 /var 分区下的一个目录，那么可以将 /var/tmp 目录下所有数据移动到 /tmp 分区下，然后在 /var 下做一个指向 /tmp 的软连接即可。 也就是执行如下操作： [root@server ~]# mv /var/tmp/* /tmp [root@server ~]# ln -s /tmp /var/tmp如果 /tmp 是根目录下的一个目录，那么设置稍微复杂，可以通过创建一个 loopback 文件系统来利用 Linux 内核的 loopback 特性将文件系统挂载到 /tmp 下，然后在挂载时指定限制加载选项即可。 一个简单的操作示例如下： [root@server ~]# dd if=/dev/zero of=/dev/tmpfs bs=1M count=10000 [root@server ~]# mke2fs -j /dev/tmpfs [root@server ~]# cp -av /tmp /tmp.old [root@server ~]# mount -o loop,noexec,nosuid,rw /dev/tmpfs /tmp [root@server ~]# chmod 1777 /tmp [root@server ~]# mv -f /tmp.old/* /tmp/ [root@server ~]# rm -rf /tmp.old最后，编辑 /etc/fstab，添加如下内容，以便系统在启动时自动加载 loopback 文件系统： /dev/tmpfs /tmp ext3 loop,nosuid,noexec,rw 0 0Linux 后门入侵检测工具Rootkit 是 Linux 平台下最常见的一种木马后门工具，它主要通过替换系统文件来达到入侵和和隐蔽的目的，这种木马比普通木马后门更加危险和隐蔽，普通的检测工具和检查手段很难发现这种木马。 Rootkit 攻击能力极强，对系统的危害很大，它通过一套工具来建立后门和隐藏行迹，从而让攻击者保住权限，以使它在任何时候都可以使用 Root 权限登录到系统。 Rootkit 主要有两种类型：文件级别和内核级别，下面分别进行简单介绍。 文件级别的 Rootkit 一般是通过程序漏洞或者系统漏洞进入系统后，通过修改系统的重要文件来达到隐藏自己的目的。 在系统遭受 Rootkit 攻击后，合法的文件被木马程序替代，变成了外壳程序，而其内部是隐藏着的后门程序。 通常容易被 Rootkit 替换的系统程序有 login、ls、ps、ifconfig、du、find、netstat 等，其中 login 程序是最经常被替换的。 因为当访问 Linux 时，无论是通过本地登录还是远程登录，/bin/login 程序都会运行，系统将通过 /bin/login 来收集并核对用户的账号和密码。 而 Rootkit 就是利用这个程序的特点，使用一个带有根权限后门密码的 /bin/login 来替换系统的 /bin/login，这样攻击者通过输入设定好的密码就能轻松进入系统。 此时，即使系统管理员修改 Root 密码或者清除 Root 密码，攻击者还是一样能通过 Root 用户登录系统。 攻击者通常在进入 Linux 系统后，会进行一系列的攻击动作，最常见的是安装嗅探器收集本机或者网络中其他服务器的重要数据。 在默认情况下，Linux 中也有一些系统文件会监控这些工具动作，例如 ifconfig 命令。 所以，攻击者为了避免被发现，会想方设法替换其他系统文件，常见的就是 ls、ps、ifconfig、du、find、netstat 等。 如果这些文件都被替换，那么在系统层面就很难发现 Rootkit 已经在系统中运行了。 这就是文件级别的 Rootkit，对系统维护很大，目前最有效的防御方法是定期对系统重要文件的完整性进行检查。 如果发现文件被修改或者被替换，那么很可能系统已经遭受了 Rootkit 入侵。 检查文件完整性的工具很多，常见的有 Tripwire、 aide 等，可以通过这些工具定期检查文件系统的完整性，以检测系统是否被 Rootkit 入侵。 内核级 Rootkit 是比文件级 Rootkit 更高级的一种入侵方式，它可以使攻击者获得对系统底层的完全控制权。 此时攻击者可以修改系统内核，进而截获运行程序向内核提交的命令，并将其重定向到入侵者所选择的程序并运行此程序。 也就是说，当用户要运行程序 A 时，被入侵者修改过的内核会假装执行 A 程序，而实际上却执行了程序 B。 内核级 Rootkit 主要依附在内核上，它并不对系统文件做任何修改，因此一般的检测工具很难检测到它的存在，这样一旦系统内核被植入 Rootkit，攻击者就可以对系统为所欲为而不被发现。 目前对于内核级的 Rootkit 还没有很好的防御工具，因此，做好系统安全防范就非常重要，将系统维持在最小权限内工作，只要攻击者不能获取 Root 权限，就无法在内核中植入 Rootkit。 ①Rootkit 后门检测工具 ChkrootkitChkrootkit 是一个 Linux 系统下查找并检测 Rootkit 后门的工具，它的官方地址： http://www.chkrootkit.org/Chkrootkit 没有包含在官方的 CentOS 源中，因此要采取手动编译的方法来安装，不过这种安装方法也更加安全。 Chkrootkit 的使用比较简单，直接执行 Chkrootkit 命令即可自动开始检测系统。 下面是某个系统的检测结果： [root@server chkrootkit]# /usr/local/chkrootkit/chkrootkit Checking `ifconfig&#39;... INFECTED Checking `ls&#39;... INFECTED Checking `login&#39;... INFECTED Checking `netstat&#39;... INFECTED Checking `ps&#39;... INFECTED Checking `top&#39;... INFECTED Checking `sshd&#39;... not infected Checking `syslogd&#39;... not tested从输出可以看出，此系统的 ifconfig、ls、login、netstat、ps 和 top 命令已经被感染。 针对被感染 Rootkit 的系统，最安全而有效的方法就是备份数据重新安装系统。 Chkrootkit 在检查 Rootkit 的过程中使用了部分系统命令，因此，如果服务器被黑客入侵，那么依赖的系统命令可能也已经被入侵者替换，此时 Chkrootkit 的检测结果将变得完全不可信。 为了避免 Chkrootkit 的这个问题，可以在服务器对外开放前，事先将 Chkrootkit 使用的系统命令进行备份，在需要的时候使用备份的原始系统命令让 Chkrootkit 对 Rootkit 进行检测。 ②Rootkit 后门检测工具 RKHunterRKHunter 是一款专业的检测系统是否感染 Rootkit 的工具，它通过执行一系列的脚本来确认服务器是否已经感染 Rootkit。 在官方的资料中，RKHunter 可以做的事情有： MD5校验测试，检测文件是否有改动，比较系统命令的md5，从而判断系统命令是否被篡改 检测rootkit使用的二进制和系统工具文件 检测特洛伊木马程序的特征码 检测常用程序的文件属性是否异常 检测系统相关的测试 检测隐藏文件 检测可疑的核心模块LKM 检测系统已启动的监听端口在 Linux 终端使用 RKHunter 来检测，最大的好处在于每项的检测结果都有不同的颜色显示，如果是绿色的表示没有问题，如果是红色的，那就要引起关注了。 另外，在执行检测的过程中，在每个部分检测完成后，需要以 Enter 键来继续。 如果要让程序自动运行，可以执行如下命令： [root@server ~]# /usr/local/bin/rkhunter --check --skip-keypress同时，如果想让检测程序每天定时运行，那么可以在 /etc/crontab 中加入如下内容： 30 09 * * * root /usr/local/bin/rkhunter --check --cronjob 这样，RKHunter 检测程序就会在每天的 9:30 分运行一次。 服务器遭受攻击后的处理过程安全总是相对的，再安全的服务器也有可能遭受到攻击。 作为一个安全运维人员，要把握的原则是：尽量做好系统安全防护，修复所有已知的危险行为，同时，在系统遭受攻击后能够迅速有效地处理攻击行为，最大限度地降低攻击对系统产生的影响。 ①处理服务器遭受攻击的一般思路系统遭受攻击并不可怕，可怕的是面对攻击束手无策，下面就详细介绍下在服务器遭受攻击后的一般处理思路。 切断网络：所有的攻击都来自于网络，因此，在得知系统正遭受黑客的攻击后，首先要做的就是断开服务器的网络连接，这样除了能切断攻击源之外，也能保护服务器所在网络的其他主机。 查找攻击源：可以通过分析系统日志或登录日志文件，查看可疑信息，同时也要查看系统都打开了哪些端口，运行哪些进程，并通过这些进程分析哪些是可疑的程序。 这个过程要根据经验和综合判断能力进行追查和分析。下面会详细介绍这个过程的处理思路。 分析入侵原因和途径：既然系统遭到入侵，那么原因是多方面的，可能是系统漏洞，也可能是程序漏洞。 一定要查清楚是哪个原因导致的，并且还要查清楚遭到攻击的途径，找到攻击源，因为只有知道了遭受攻击的原因和途径，才能删除攻击源同时进行漏洞的修复。 备份用户数据：在服务器遭受攻击后，需要立刻备份服务器上的用户数据，同时也要查看这些数据中是否隐藏着攻击源。 如果攻击源在用户数据中，一定要彻底删除，然后将用户数据备份到一个安全的地方。 重新安装系统：永远不要认为自己能彻底清除攻击源，因为没有人能比黑客更了解攻击程序。 在服务器遭到攻击后，最安全也最简单的方法就是重新安装系统，因为大部分攻击程序都会依附在系统文件或者内核中，所以重新安装系统才能彻底清除攻击源。 修复程序或系统漏洞：在发现系统漏洞或者应用程序漏洞后，首先要做的就是修复系统漏洞或者更改程序 Bug，因为只有将程序的漏洞修复完毕才能正式在服务器上运行。 恢复数据和连接网络：将备份的数据重新复制到新安装的服务器上，然后开启服务，最后将服务器开启网络连接，对外提供服务。 ②检查并锁定可疑用户当发现服务器遭受攻击后，首先要切断网络连接，但是在有些情况下，比如无法马上切断网络连接时，就必须登录系统查看是否有可疑用户。 如果有可疑用户登录了系统，那么需要马上将这个用户锁定，然后中断此用户的远程连接。 ③查看系统日志查看系统日志是查找攻击源最好的方法，可查的系统日志有 /var/log/messages、/var/log/secure 等。 这两个日志文件可以记录软件的运行状态以及远程用户的登录状态，还可以查看每个用户目录下的 .bash_history 文件。 特别是 /root 目录下的 .bash_history 文件，这个文件中记录着用户执行的所有历史命令。 ④检查并关闭系统可疑进程检查可疑进程的命令很多，例如 ps、top 等，但是有时候只知道进程的名称无法得知路径，此时可以通过如下命令查看。 首先通过 pidof 命令可以查找正在运行的进程 PID，例如要查找 sshd 进程的 PID。 执行如下命令： [root@server ~]# pidof sshd 13276 12942 4284然后进入内存目录，查看对应 PID 目录下 exe 文件的信息： [root@server ~]# ls -al /proc/13276/exe lrwxrwxrwx 1 root root 0 Oct 4 22:09 /proc/13276/exe -&gt; /usr/sbin/sshd这样就找到了进程对应的完整执行路径。如果还要查看文件的句柄，可以查看如下目录： [root@server ~]# ls -al /proc/13276/fd通过这种方式基本可以找到任何进程的完整执行信息。 ⑤检查文件系统的完好性检查文件属性是否发生变化是验证文件系统完好性最简单、最直接的方法，例如可以检查被入侵服务器上 /bin/ls 文件的大小是否与正常系统上此文件的大小相同，以验证文件是否被替换，但是这种方法比较低级。 此时可以借助于 Linux 下 rpm 这个工具来完成验证，操作如下： [root@server ~]# rpm -Va ....L... c /etc/pam.d/system-auth S.5..... c /etc/security/limits.conf S.5....T c /etc/sysctl.conf S.5....T /etc/sgml/docbook-simple.cat S.5....T c /etc/login.defs S.5..... c /etc/openldap/ldap.conf S.5....T c /etc/sudoers⑥重新安装系统恢复数据很多情况下，被攻击过的系统已经不再可信任，因此，最好的方法是将服务器上面数据进行备份，然后重新安装系统，最后再恢复数据即可。 数据恢复完成，马上对系统做上面介绍的安全加固策略，保证系统安全。 原文链接如下： https://www.cnblogs.com/MYSQLZOUQI/p/5317916.html","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/tags/Linux/"}]},{"title":"tmux-初探","slug":"Linux/让你的Linux酷起来--小白也能玩转Tmux","date":"2019-09-04T17:55:39.000Z","updated":"2020-05-25T13:52:55.068Z","comments":true,"path":"2019/09/05/linux/rang-ni-de-linux-ku-qi-lai-xiao-bai-ye-neng-wan-zhuan-tmux/","link":"","permalink":"https://cyylog.github.io/2019/09/05/linux/rang-ni-de-linux-ku-qi-lai-xiao-bai-ye-neng-wan-zhuan-tmux/","excerpt":"","text":"Linux终端复用神器-tmux初探Tmux是一个优秀的终端复用软件，类似GNU Screen，但来自于OpenBSD，采用BSD授权。使用它最直观的好处就是，通过一个终端登录远程主机并运行tmux后，在其中可以开启多个控制台而无需再“浪费”多余的终端来连接这台远程主机。是BSD实现的Screen替代品，相对于Screen，它更加先进：支持屏幕切分，而且具备丰富的命令行参数，使其可以灵活、动态的进行各种布局和操作。废话不多说来个效果图 Tmux的使用场景1）可以某个程序在执行时一直是输出状态，需要结合nohup、&amp;来放在后台执行，并且ctrl+c结束。这时可以打开一个Tmux窗口，在该窗口里执行这个程序，用来保证该程序一直在执行中，只要Tmux这个窗口不关闭 2）公司需要备份数据库时，数据量巨大，备份两三天弄不完，这时不小心关闭了终端窗口或误操作就前功尽弃了，使用Tmux会话运行命令或任务，就不用担心这些问题。 3）下班后，你需要断开ssh或关闭电脑，将运行的命令或任务放置后台运行。 4）关闭终端,再次打开时原终端里面的任务进程依然不会中断Tmux功能：- 提供了强劲的、易于使用的命令行界面。 - 可横向和纵向分割窗口。 - 窗格可以自由移动和调整大小，或直接利用四个预设布局之一。 - 支持 UTF-8 编码及 256 色终端。 - 可在多个缓冲区进行复制和粘贴。 - 可通过交互式菜单来选择窗口、会话及客户端。 - 支持跨窗口搜索。 - 支持自动及手动锁定窗口。Tmux安装yum -y install tmuxTmux个性化配置此类配置可以在命令行模式中输入show-options -g查询 tmux加上下列参数,实现个性化设置 set-option -g base-index 1 # 窗口的初始序号；默认为0，这里设置为1 set-option -g display-time 5000 # 提示信息的持续时间；设置足够的时间以避免看不清提示，单位为毫秒 set-option -g repeat-time 1000 # 控制台激活后的持续时间；设置合适的时间以避免每次操作都要先激活控制台，单位为毫秒 set-option -g status-keys vi # 操作状态栏时的默认键盘布局；可以设置为vi或emacs set-option -g status-utf8 on # 开启状态栏的UTF-8支持 --- set-option -g status-bg blue set-option -g status-fg &#39;#bbbbbb&#39; set-option -g status-left-fg green set-option -g status-left-bg blue set-option -g status-right-fg green set-option -g status-right-bg blue set-option -g status-left-length 10 # 状态栏左方的内容长度； set-option -g status-right-length 15 # 状态栏右方的内容长度；建议把更多的空间留给状态栏左方（用于列出当前窗口） set-option -g status-left &#39;[#(whoami)]&#39; # 状态栏左方的内容 set-option -g status-right &#39;[#(date +&quot; %m-%d %H:%M &quot;)]&#39; # 状态栏右方的内容；这里的设置将得到类似23:59的显示 set-option -g status-justify &quot;centre&quot; # 窗口列表居中显示 set-option -g default-terminal &quot;screen-256color&quot; # 支持256色显示 分割窗口边界的颜色 set-option -g pane-active-border-fg &#39;#55ff55&#39; set-option -g pane-border-fg &#39;#555555&#39; --- 此类设置可以在命令行模式中输入show-window-options -g查询 set-window-option -g mode-keys vi # 复制模式中的默认键盘布局；可以设置为vi或emacs set-window-option -g utf8 on # 开启窗口的UTF-8支持 set-window-option -g mode-mouse on # 窗口切换后让人可以用鼠标上下滑动显示历史输出 --- 窗口切分快捷键(没设置成功) bind \\ split-window -h # 使用 \\ 将窗口竖切 bind - split-window -v # 使用 - 将窗口横切 bind K confirm-before -p &quot;kill-window #W? (y/n)&quot; kill-window # 使用大写 K 来关闭窗口 bind &#39;&quot;&#39; choose-window # 双引号选择窗口 --- Pane之间切换的快捷键 bind h select-pane -L # 定位到左边窗口的快捷键 bind j select-pane -D # 定位到上边窗口的快捷键 bind k select-pane -U # 定位到下方窗口的快捷键 bind l select-pane -R # 定位到右边窗口的快捷键 --- 设置window属性 setw -g window-status-current-bg red setw -g window-status-current-fg white setw -g window-status-current-attr bright setw -g window-status-attr bright set-option -g window-status-format &#39;#I #W&#39; set-option -g window-status-current-format &#39; #I #W &#39; setw -g window-status-current-bg blue setw -g window-status-current-fg green 不使用prefix键，使用Ctrl和左右方向键方便切换窗口 bind-key -n &quot;C-Left&quot; select-window -t :- bind-key -n &quot;C-Right&quot; select-window -t :+tmux session 使用介绍运行tmux并开启一个新的会话 tmux 显示所有会话 tmux ls 新建会话并指定会话名称（建议制定会话名称，以便了解该会话用途） tmux new -s &lt;session-name&gt; 新建会话（不指定会话名称） tmux new 接入上一个会话 tmux a 接入指定名称的会话 tmux a -t &lt;session-name&gt; 断开当前会话（还可以使用快捷键：control+b，再按d） tmux detach 关闭指定会话 tmux kill-session -t session-name 关闭除指定会话外的所有会话 tmux kill-session -a -t session-name 在会话中切换 control+b，再按s 显示会话列表，再进行会话切换 销毁所有会话并停止tmux tmux kill-serverG 复制粘贴 Ctrl+b [ //进入复制模式 空格+方向键 //选择 回车 // 确认 Ctrl+b [ //粘贴 需要注意的几点1）进入tmux面板后，一定要先按ctrl+b，然后松开，再按其他的组合键才生效。 2）常用到的几个组合键： ctrl+b ? 显示快捷键帮助 ctrl+b 空格键 采用下一个内置布局，这个很有意思，在多屏时，用这个就会将多有屏幕竖着展示 ctrl+b ! 把当前窗口变为新窗口 ctrl+b &quot; 模向分隔窗口 ctrl+b % 纵向分隔窗口 ctrl+b q 显示分隔窗口的编号 ctrl+b o 跳到下一个分隔窗口。多屏之间的切换 ctrl+b 上下键 上一个及下一个分隔窗口 ctrl+b C-方向键 调整分隔窗口大小 ctrl+b &amp; 确认后退出当前tmux ctrl+b [ 复制模式，即将当前屏幕移到上一个的位置上，其他所有窗口都向前移动一个。 ctrl+b c 创建新窗口 ctrl+b n 选择下一个窗口 ctrl+b l 最后使用的窗口 ctrl+b p 选择前一个窗口 ctrl+b w 以菜单方式显示及选择窗口 ctrl+b s 以菜单方式显示和选择会话。这个常用到，可以选择进入哪个tmux ctrl+b t 显示时钟。然后按enter键后就会恢复到shell终端状态 ctrl+b d 脱离当前会话；这样可以暂时返回Shell界面，输入tmux attach能够重新进入之前的会话tmux的常规运维命令1）安装命令： [root@--- title: tmux-初探 date: 2018-12-07 13:59:25 tags: - tmux - 骚操作 --- ### Linux终端复用神器-tmux初探 ​``` Tmux是一个优秀的终端复用软件，类似GNU Screen，但来自于OpenBSD，采用BSD授权。使用它最直观的好处就是，通过一个终端登录远程主机并运行tmux后，在其中可以开启多个控制台而无需再“浪费”多余的终端来连接这台远程主机。是BSD实现的Screen替代品，相对于Screen，它更加先进：支持屏幕切分，而且具备丰富的命令行参数，使其可以灵活、动态的进行各种布局和操作。 ​``` 废话不多说来个效果图 ![tmux](https://i.loli.net/2018/12/14/5c139f4ab6ad2.jpg) ### Tmux的使用场景 ​``` 1）可以某个程序在执行时一直是输出状态，需要结合nohup、&amp;来放在后台执行，并且ctrl+c结束。这时可以打开一个Tmux窗口，在该窗口里执行这个程序，用来保证该程序一直在执行中，只要Tmux这个窗口不关闭 2）公司需要备份数据库时，数据量巨大，备份两三天弄不完，这时不小心关闭了终端窗口或误操作就前功尽弃了，使用Tmux会话运行命令或任务，就不用担心这些问题。 3）下班后，你需要断开ssh或关闭电脑，将运行的命令或任务放置后台运行。 4）关闭终端,再次打开时原终端里面的任务进程依然不会中断 ​``` &lt;!--more--&gt; ### Tmux功能： ​``` - 提供了强劲的、易于使用的命令行界面。 - 可横向和纵向分割窗口。 - 窗格可以自由移动和调整大小，或直接利用四个预设布局之一。 - 支持 UTF-8 编码及 256 色终端。 - 可在多个缓冲区进行复制和粘贴。 - 可通过交互式菜单来选择窗口、会话及客户端。 - 支持跨窗口搜索。 - 支持自动及手动锁定窗口。 ​``` ### Tmux安装 ​``` yum -y install tmux ​``` ### Tmux个性化配置 ​``` 此类配置可以在命令行模式中输入show-options -g查询 tmux加上下列参数,实现个性化设置 set-option -g base-index 1 # 窗口的初始序号；默认为0，这里设置为1 set-option -g display-time 5000 # 提示信息的持续时间；设置足够的时间以避免看不清提示，单位为毫秒 set-option -g repeat-time 1000 # 控制台激活后的持续时间；设置合适的时间以避免每次操作都要先激活控制台，单位为毫秒 set-option -g status-keys vi # 操作状态栏时的默认键盘布局；可以设置为vi或emacs set-option -g status-utf8 on # 开启状态栏的UTF-8支持 --- set-option -g status-bg blue set-option -g status-fg &#39;#bbbbbb&#39; set-option -g status-left-fg green set-option -g status-left-bg blue set-option -g status-right-fg green set-option -g status-right-bg blue set-option -g status-left-length 10 # 状态栏左方的内容长度； set-option -g status-right-length 15 # 状态栏右方的内容长度；建议把更多的空间留给状态栏左方（用于列出当前窗口） set-option -g status-left &#39;[#(whoami)]&#39; # 状态栏左方的内容 set-option -g status-right &#39;[#(date +&quot; %m-%d %H:%M &quot;)]&#39; # 状态栏右方的内容；这里的设置将得到类似23:59的显示 set-option -g status-justify &quot;centre&quot; # 窗口列表居中显示 set-option -g default-terminal &quot;screen-256color&quot; # 支持256色显示 分割窗口边界的颜色 set-option -g pane-active-border-fg &#39;#55ff55&#39; set-option -g pane-border-fg &#39;#555555&#39; --- 此类设置可以在命令行模式中输入show-window-options -g查询 set-window-option -g mode-keys vi # 复制模式中的默认键盘布局；可以设置为vi或emacs set-window-option -g utf8 on # 开启窗口的UTF-8支持 set-window-option -g mode-mouse on # 窗口切换后让人可以用鼠标上下滑动显示历史输出 --- 窗口切分快捷键(没设置成功) bind \\ split-window -h # 使用 \\ 将窗口竖切 bind - split-window -v # 使用 - 将窗口横切 bind K confirm-before -p &quot;kill-window #W? (y/n)&quot; kill-window # 使用大写 K 来关闭窗口 bind &#39;&quot;&#39; choose-window # 双引号选择窗口 --- Pane之间切换的快捷键 bind h select-pane -L # 定位到左边窗口的快捷键 bind j select-pane -D # 定位到上边窗口的快捷键 bind k select-pane -U # 定位到下方窗口的快捷键 bind l select-pane -R # 定位到右边窗口的快捷键 --- 设置window属性 setw -g window-status-current-bg red setw -g window-status-current-fg white setw -g window-status-current-attr bright setw -g window-status-attr bright set-option -g window-status-format &#39;#I #W&#39; set-option -g window-status-current-format &#39; #I #W &#39; setw -g window-status-current-bg blue setw -g window-status-current-fg green 不使用prefix键，使用Ctrl和左右方向键方便切换窗口 bind-key -n &quot;C-Left&quot; select-window -t :- bind-key -n &quot;C-Right&quot; select-window -t :+ ​``` ### tmux session 使用介绍 ​``` 运行tmux并开启一个新的会话 tmux 显示所有会话 tmux ls 新建会话并指定会话名称（建议制定会话名称，以便了解该会话用途） tmux new -s &lt;session-name&gt; 新建会话（不指定会话名称） tmux new 接入上一个会话 tmux a 接入指定名称的会话 tmux a -t &lt;session-name&gt; 断开当前会话（还可以使用快捷键：control+b，再按d） tmux detach 关闭指定会话 tmux kill-session -t session-name 关闭除指定会话外的所有会话 tmux kill-session -a -t session-name 在会话中切换 control+b，再按s 显示会话列表，再进行会话切换 销毁所有会话并停止tmux tmux kill-serverG 复制粘贴 Ctrl+b [ //进入复制模式 空格+方向键 //选择 回车 // 确认 Ctrl+b [ //粘贴 ​``` ### 需要注意的几点 ​``` 1）进入tmux面板后，一定要先按ctrl+b，然后松开，再按其他的组合键才生效。 2）常用到的几个组合键： ctrl+b ? 显示快捷键帮助 ctrl+b 空格键 采用下一个内置布局，这个很有意思，在多屏时，用这个就会将多有屏幕竖着展示 ctrl+b ! 把当前窗口变为新窗口 ctrl+b &quot; 模向分隔窗口 ctrl+b % 纵向分隔窗口 ctrl+b q 显示分隔窗口的编号 ctrl+b o 跳到下一个分隔窗口。多屏之间的切换 ctrl+b 上下键 上一个及下一个分隔窗口 ctrl+b C-方向键 调整分隔窗口大小 ctrl+b &amp; 确认后退出当前tmux ctrl+b [ 复制模式，即将当前屏幕移到上一个的位置上，其他所有窗口都向前移动一个。 ctrl+b c 创建新窗口 ctrl+b n 选择下一个窗口 ctrl+b l 最后使用的窗口 ctrl+b p 选择前一个窗口 ctrl+b w 以菜单方式显示及选择窗口 ctrl+b s 以菜单方式显示和选择会话。这个常用到，可以选择进入哪个tmux ctrl+b t 显示时钟。然后按enter键后就会恢复到shell终端状态 ctrl+b d 脱离当前会话；这样可以暂时返回Shell界面，输入tmux attach能够重新进入之前的会话 ​``` ### tmux的常规运维命令 ​``` 1）安装命令： [root@Centos6 ~]# yum -y install tmux 2）默认创建一个会话，以数字命名。（不推荐） [root@Centos6 ~]# tmux 3）新建会话，比如新创建一个会话以&quot;ccc&quot;命名 [root@Centos6 ~]# tmux new -s ccc 加上参数-d，表示在后台新建会话 root@bobo:~# tmux new -s shibo -d root@bobo:~# tmux ls shibo: 1 windows (created Tue Oct 2 19:22:32 2018) [135x35] 4）查看创建得所有会话 [root@Centos6 ~]# tmux ls 0: 1 windows (created Wed Aug 30 17:58:20 2017) [112x22](attached) #这里的attached表示该会话是当前会话 aaa: 2 windows (created Wed Aug 30 16:54:33 2017) [112x22] ccc: 1 windows (created Wed Aug 30 17:01:05 2017) [112x22] 5）登录一个已知会话。即从终端环境进入会话。 第一个参数a也可以写成attach。后面的aaa是会话名称。 [root@Centos6 ~]# tmux a -t aaa 6）退出会话不是关闭： 登到某一个会话后，依次按键ctrl-b + d，这样就会退化该会话，但不会关闭会话。 如果直接ctrl + d，就会在退出会话的通话也关闭了该会话！ 7）关闭会话（销毁会话） [root@Centos6 ~]# tmux ls aaa: 2 windows (created Wed Aug 30 16:54:33 2017) [112x22] bbb: 1 windows (created Wed Aug 30 19:02:09 2017) [112x22] [root@Centos6 ~]# tmux kill-session -t bbb [root@Centos6 ~]# tmux ls aaa: 2 windows (created Wed Aug 30 16:54:33 2017) [112x22] 8）重命名会话 [root@Centos6 ~]# tmux ls wangshibo: 1 windows (created Sun Sep 30 10:17:00 2018) [136x29] (attached) [root@Centos6 ~]# tmux rename -t wangshibo kevin [root@Centos6 ~]# tmux ls kevin: 1 windows (created Sun Sep 30 10:17:00 2018) [136x29] (attached) ​``` --- title: tmux-初探 date: 2018-12-07 13:59:25 tags: - tmux - 骚操作 --- ### Linux终端复用神器-tmux初探 ​``` Tmux是一个优秀的终端复用软件，类似GNU Screen，但来自于OpenBSD，采用BSD授权。使用它最直观的好处就是，通过一个终端登录远程主机并运行tmux后，在其中可以开启多个控制台而无需再“浪费”多余的终端来连接这台远程主机。是BSD实现的Screen替代品，相对于Screen，它更加先进：支持屏幕切分，而且具备丰富的命令行参数，使其可以灵活、动态的进行各种布局和操作。 ​``` 废话不多说来个效果图 ![tmux](https://i.loli.net/2018/12/14/5c139f4ab6ad2.jpg) ### Tmux的使用场景 ​``` 1）可以某个程序在执行时一直是输出状态，需要结合nohup、&amp;来放在后台执行，并且ctrl+c结束。这时可以打开一个Tmux窗口，在该窗口里执行这个程序，用来保证该程序一直在执行中，只要Tmux这个窗口不关闭 2）公司需要备份数据库时，数据量巨大，备份两三天弄不完，这时不小心关闭了终端窗口或误操作就前功尽弃了，使用Tmux会话运行命令或任务，就不用担心这些问题。 3）下班后，你需要断开ssh或关闭电脑，将运行的命令或任务放置后台运行。 4）关闭终端,再次打开时原终端里面的任务进程依然不会中断 ​``` &lt;!--more--&gt; ### Tmux功能： ​``` - 提供了强劲的、易于使用的命令行界面。 - 可横向和纵向分割窗口。 - 窗格可以自由移动和调整大小，或直接利用四个预设布局之一。 - 支持 UTF-8 编码及 256 色终端。 - 可在多个缓冲区进行复制和粘贴。 - 可通过交互式菜单来选择窗口、会话及客户端。 - 支持跨窗口搜索。 - 支持自动及手动锁定窗口。 ​``` ### Tmux安装 ​``` yum -y install tmux ​``` ### Tmux个性化配置 ​``` 此类配置可以在命令行模式中输入show-options -g查询 tmux加上下列参数,实现个性化设置 set-option -g base-index 1 # 窗口的初始序号；默认为0，这里设置为1 set-option -g display-time 5000 # 提示信息的持续时间；设置足够的时间以避免看不清提示，单位为毫秒 set-option -g repeat-time 1000 # 控制台激活后的持续时间；设置合适的时间以避免每次操作都要先激活控制台，单位为毫秒 set-option -g status-keys vi # 操作状态栏时的默认键盘布局；可以设置为vi或emacs set-option -g status-utf8 on # 开启状态栏的UTF-8支持 --- set-option -g status-bg blue set-option -g status-fg &#39;#bbbbbb&#39; set-option -g status-left-fg green set-option -g status-left-bg blue set-option -g status-right-fg green set-option -g status-right-bg blue set-option -g status-left-length 10 # 状态栏左方的内容长度； set-option -g status-right-length 15 # 状态栏右方的内容长度；建议把更多的空间留给状态栏左方（用于列出当前窗口） set-option -g status-left &#39;[#(whoami)]&#39; # 状态栏左方的内容 set-option -g status-right &#39;[#(date +&quot; %m-%d %H:%M &quot;)]&#39; # 状态栏右方的内容；这里的设置将得到类似23:59的显示 set-option -g status-justify &quot;centre&quot; # 窗口列表居中显示 set-option -g default-terminal &quot;screen-256color&quot; # 支持256色显示 分割窗口边界的颜色 set-option -g pane-active-border-fg &#39;#55ff55&#39; set-option -g pane-border-fg &#39;#555555&#39; --- 此类设置可以在命令行模式中输入show-window-options -g查询 set-window-option -g mode-keys vi # 复制模式中的默认键盘布局；可以设置为vi或emacs set-window-option -g utf8 on # 开启窗口的UTF-8支持 set-window-option -g mode-mouse on # 窗口切换后让人可以用鼠标上下滑动显示历史输出 --- 窗口切分快捷键(没设置成功) bind \\ split-window -h # 使用 \\ 将窗口竖切 bind - split-window -v # 使用 - 将窗口横切 bind K confirm-before -p &quot;kill-window #W? (y/n)&quot; kill-window # 使用大写 K 来关闭窗口 bind &#39;&quot;&#39; choose-window # 双引号选择窗口 --- Pane之间切换的快捷键 bind h select-pane -L # 定位到左边窗口的快捷键 bind j select-pane -D # 定位到上边窗口的快捷键 bind k select-pane -U # 定位到下方窗口的快捷键 bind l select-pane -R # 定位到右边窗口的快捷键 --- 设置window属性 setw -g window-status-current-bg red setw -g window-status-current-fg white setw -g window-status-current-attr bright setw -g window-status-attr bright set-option -g window-status-format &#39;#I #W&#39; set-option -g window-status-current-format &#39; #I #W &#39; setw -g window-status-current-bg blue setw -g window-status-current-fg green 不使用prefix键，使用Ctrl和左右方向键方便切换窗口 bind-key -n &quot;C-Left&quot; select-window -t :- bind-key -n &quot;C-Right&quot; select-window -t :+ ​``` ### tmux session 使用介绍 ​``` 运行tmux并开启一个新的会话 tmux 显示所有会话 tmux ls 新建会话并指定会话名称（建议制定会话名称，以便了解该会话用途） tmux new -s &lt;session-name&gt; 新建会话（不指定会话名称） tmux new 接入上一个会话 tmux a 接入指定名称的会话 tmux a -t &lt;session-name&gt; 断开当前会话（还可以使用快捷键：control+b，再按d） tmux detach 关闭指定会话 tmux kill-session -t session-name 关闭除指定会话外的所有会话 tmux kill-session -a -t session-name 在会话中切换 control+b，再按s 显示会话列表，再进行会话切换 销毁所有会话并停止tmux tmux kill-serverG 复制粘贴 Ctrl+b [ //进入复制模式 空格+方向键 //选择 回车 // 确认 Ctrl+b [ //粘贴 ​``` ### 需要注意的几点 ​``` 1）进入tmux面板后，一定要先按ctrl+b，然后松开，再按其他的组合键才生效。 2）常用到的几个组合键： ctrl+b ? 显示快捷键帮助 ctrl+b 空格键 采用下一个内置布局，这个很有意思，在多屏时，用这个就会将多有屏幕竖着展示 ctrl+b ! 把当前窗口变为新窗口 ctrl+b &quot; 模向分隔窗口 ctrl+b % 纵向分隔窗口 ctrl+b q 显示分隔窗口的编号 ctrl+b o 跳到下一个分隔窗口。多屏之间的切换 ctrl+b 上下键 上一个及下一个分隔窗口 ctrl+b C-方向键 调整分隔窗口大小 ctrl+b &amp; 确认后退出当前tmux ctrl+b [ 复制模式，即将当前屏幕移到上一个的位置上，其他所有窗口都向前移动一个。 ctrl+b c 创建新窗口 ctrl+b n 选择下一个窗口 ctrl+b l 最后使用的窗口 ctrl+b p 选择前一个窗口 ctrl+b w 以菜单方式显示及选择窗口 ctrl+b s 以菜单方式显示和选择会话。这个常用到，可以选择进入哪个tmux ctrl+b t 显示时钟。然后按enter键后就会恢复到shell终端状态 ctrl+b d 脱离当前会话；这样可以暂时返回Shell界面，输入tmux attach能够重新进入之前的会话 ​``` ### tmux的常规运维命令 ​``` 1）安装命令： [root@--- title: tmux-初探 date: 2018-12-07 13:59:25 tags: - tmux - 骚操作 --- ### Linux终端复用神器-tmux初探 ​``` Tmux是一个优秀的终端复用软件，类似GNU Screen，但来自于OpenBSD，采用BSD授权。使用它最直观的好处就是，通过一个终端登录远程主机并运行tmux后，在其中可以开启多个控制台而无需再“浪费”多余的终端来连接这台远程主机。是BSD实现的Screen替代品，相对于Screen，它更加先进：支持屏幕切分，而且具备丰富的命令行参数，使其可以灵活、动态的进行各种布局和操作。 ​``` 废话不多说来个效果图 ![tmux](https://i.loli.net/2018/12/14/5c139f4ab6ad2.jpg) ### Tmux的使用场景 ​``` 1）可以某个程序在执行时一直是输出状态，需要结合nohup、&amp;来放在后台执行，并且ctrl+c结束。这时可以打开一个Tmux窗口，在该窗口里执行这个程序，用来保证该程序一直在执行中，只要Tmux这个窗口不关闭 2）公司需要备份数据库时，数据量巨大，备份两三天弄不完，这时不小心关闭了终端窗口或误操作就前功尽弃了，使用Tmux会话运行命令或任务，就不用担心这些问题。 3）下班后，你需要断开ssh或关闭电脑，将运行的命令或任务放置后台运行。 4）关闭终端,再次打开时原终端里面的任务进程依然不会中断 ​``` &lt;!--more--&gt; ### Tmux功能： ​``` - 提供了强劲的、易于使用的命令行界面。 - 可横向和纵向分割窗口。 - 窗格可以自由移动和调整大小，或直接利用四个预设布局之一。 - 支持 UTF-8 编码及 256 色终端。 - 可在多个缓冲区进行复制和粘贴。 - 可通过交互式菜单来选择窗口、会话及客户端。 - 支持跨窗口搜索。 - 支持自动及手动锁定窗口。 ​``` ### Tmux安装 ​``` yum -y install tmux ​``` ### Tmux个性化配置 ​``` 此类配置可以在命令行模式中输入show-options -g查询 tmux加上下列参数,实现个性化设置 set-option -g base-index 1 # 窗口的初始序号；默认为0，这里设置为1 set-option -g display-time 5000 # 提示信息的持续时间；设置足够的时间以避免看不清提示，单位为毫秒 set-option -g repeat-time 1000 # 控制台激活后的持续时间；设置合适的时间以避免每次操作都要先激活控制台，单位为毫秒 set-option -g status-keys vi # 操作状态栏时的默认键盘布局；可以设置为vi或emacs set-option -g status-utf8 on # 开启状态栏的UTF-8支持 --- set-option -g status-bg blue set-option -g status-fg &#39;#bbbbbb&#39; set-option -g status-left-fg green set-option -g status-left-bg blue set-option -g status-right-fg green set-option -g status-right-bg blue set-option -g status-left-length 10 # 状态栏左方的内容长度； set-option -g status-right-length 15 # 状态栏右方的内容长度；建议把更多的空间留给状态栏左方（用于列出当前窗口） set-option -g status-left &#39;[#(whoami)]&#39; # 状态栏左方的内容 set-option -g status-right &#39;[#(date +&quot; %m-%d %H:%M &quot;)]&#39; # 状态栏右方的内容；这里的设置将得到类似23:59的显示 set-option -g status-justify &quot;centre&quot; # 窗口列表居中显示 set-option -g default-terminal &quot;screen-256color&quot; # 支持256色显示 分割窗口边界的颜色 set-option -g pane-active-border-fg &#39;#55ff55&#39; set-option -g pane-border-fg &#39;#555555&#39; --- 此类设置可以在命令行模式中输入show-window-options -g查询 set-window-option -g mode-keys vi # 复制模式中的默认键盘布局；可以设置为vi或emacs set-window-option -g utf8 on # 开启窗口的UTF-8支持 set-window-option -g mode-mouse on # 窗口切换后让人可以用鼠标上下滑动显示历史输出 --- 窗口切分快捷键(没设置成功) bind \\ split-window -h # 使用 \\ 将窗口竖切 bind - split-window -v # 使用 - 将窗口横切 bind K confirm-before -p &quot;kill-window #W? (y/n)&quot; kill-window # 使用大写 K 来关闭窗口 bind &#39;&quot;&#39; choose-window # 双引号选择窗口 --- Pane之间切换的快捷键 bind h select-pane -L # 定位到左边窗口的快捷键 bind j select-pane -D # 定位到上边窗口的快捷键 bind k select-pane -U # 定位到下方窗口的快捷键 bind l select-pane -R # 定位到右边窗口的快捷键 --- 设置window属性 setw -g window-status-current-bg red setw -g window-status-current-fg white setw -g window-status-current-attr bright setw -g window-status-attr bright set-option -g window-status-format &#39;#I #W&#39; set-option -g window-status-current-format &#39; #I #W &#39; setw -g window-status-current-bg blue setw -g window-status-current-fg green 不使用prefix键，使用Ctrl和左右方向键方便切换窗口 bind-key -n &quot;C-Left&quot; select-window -t :- bind-key -n &quot;C-Right&quot; select-window -t :+ ​``` ### tmux session 使用介绍 ​``` 运行tmux并开启一个新的会话 tmux 显示所有会话 tmux ls 新建会话并指定会话名称（建议制定会话名称，以便了解该会话用途） tmux new -s &lt;session-name&gt; 新建会话（不指定会话名称） tmux new 接入上一个会话 tmux a 接入指定名称的会话 tmux a -t &lt;session-name&gt; 断开当前会话（还可以使用快捷键：control+b，再按d） tmux detach 关闭指定会话 tmux kill-session -t session-name 关闭除指定会话外的所有会话 tmux kill-session -a -t session-name 在会话中切换 control+b，再按s 显示会话列表，再进行会话切换 销毁所有会话并停止tmux tmux kill-serverG 复制粘贴 Ctrl+b [ //进入复制模式 空格+方向键 //选择 回车 // 确认 Ctrl+b [ //粘贴 ​``` ### 需要注意的几点 ​``` 1）进入tmux面板后，一定要先按ctrl+b，然后松开，再按其他的组合键才生效。 2）常用到的几个组合键： ctrl+b ? 显示快捷键帮助 ctrl+b 空格键 采用下一个内置布局，这个很有意思，在多屏时，用这个就会将多有屏幕竖着展示 ctrl+b ! 把当前窗口变为新窗口 ctrl+b &quot; 模向分隔窗口 ctrl+b % 纵向分隔窗口 ctrl+b q 显示分隔窗口的编号 ctrl+b o 跳到下一个分隔窗口。多屏之间的切换 ctrl+b 上下键 上一个及下一个分隔窗口 ctrl+b C-方向键 调整分隔窗口大小 ctrl+b &amp; 确认后退出当前tmux ctrl+b [ 复制模式，即将当前屏幕移到上一个的位置上，其他所有窗口都向前移动一个。 ctrl+b c 创建新窗口 ctrl+b n 选择下一个窗口 ctrl+b l 最后使用的窗口 ctrl+b p 选择前一个窗口 ctrl+b w 以菜单方式显示及选择窗口 ctrl+b s 以菜单方式显示和选择会话。这个常用到，可以选择进入哪个tmux ctrl+b t 显示时钟。然后按enter键后就会恢复到shell终端状态 ctrl+b d 脱离当前会话；这样可以暂时返回Shell界面，输入tmux attach能够重新进入之前的会话 ​``` ### tmux的常规运维命令 ​``` 1）安装命令： [root@1000phone ~]# yum -y install tmux 2）默认创建一个会话，以数字命名。（不推荐） [root@1000phone ~]# tmux 3）新建会话，比如新创建一个会话以&quot;ccc&quot;命名 [root@1000phone ~]# tmux new -s ccc 加上参数-d，表示在后台新建会话 root@1000phone:~# tmux new -s 1000phone -d root@1000phone:~# tmux ls 1000phone: 1 windows (created Tue Oct 2 19:22:32 2018) [135x35] 4）查看创建得所有会话 [root@1000phone ~]# tmux ls 0: 1 windows (created Wed Aug 30 17:58:20 2017) [112x22](attached) #这里的attached表示该会话是当前会话 1000phone: 2 windows (created Wed Aug 30 16:54:33 2017) [112x22] ccc: 1 windows (created Wed Aug 30 17:01:05 2017) [112x22] 5）登录一个已知会话。即从终端环境进入会话。 第一个参数a也可以写成attach。后面的aaa是会话名称。 [root@1000phone ~]# tmux a -t 1000phone 6）退出会话不是关闭： 登到某一个会话后，依次按键ctrl-b + d，这样就会退化该会话，但不会关闭会话。 如果直接ctrl + d，就会在退出会话的通话也关闭了该会话！ 7）关闭会话（销毁会话） [root@1000phone ~]# tmux ls 1000phone: 2 windows (created Wed Aug 30 16:54:33 2017) [112x22] bbb: 1 windows (created Wed Aug 30 19:02:09 2017) [112x22] [root@1000phone ~]# tmux kill-session -t bbb [root@1000phone ~]# tmux ls 1000phone: 2 windows (created Wed Aug 30 16:54:33 2017) [112x22] 8）重命名会话 [root@1000phone ~]# tmux ls tigerfive: 1 windows (created Sun Sep 30 10:17:00 2018) [136x29] (attached) [root@1000phone ~]# tmux rename -t tigerfive 1000phone [root@Centos6 ~]# tmux ls 1000phone: 1 windows (created Sun Sep 30 10:17:00 2018) [136x29] (attached) ​``` ~]# yum -y install tmux 2）默认创建一个会话，以数字命名。（不推荐） [root@1000phone ~]# tmux 3）新建会话，比如新创建一个会话以&quot;ccc&quot;命名 [root@1000phone ~]# tmux new -s ccc 加上参数-d，表示在后台新建会话 root@1000phone:~# tmux new -s 1000phone -d root@1000phone:~# tmux ls 1000phone: 1 windows (created Tue Oct 2 19:22:32 2018) [135x35] 4）查看创建得所有会话 [root@1000phone ~]# tmux ls 0: 1 windows (created Wed Aug 30 17:58:20 2017) [112x22](attached) #这里的attached表示该会话是当前会话 aaa: 2 windows (created Wed Aug 30 16:54:33 2017) [112x22] ccc: 1 windows (created Wed Aug 30 17:01:05 2017) [112x22] 5）登录一个已知会话。即从终端环境进入会话。 第一个参数a也可以写成attach。后面的aaa是会话名称。 [root@1000phone ~]# tmux a -t aaa 6）退出会话不是关闭： 登到某一个会话后，依次按键ctrl-b + d，这样就会退化该会话，但不会关闭会话。 如果直接ctrl + d，就会在退出会话的通话也关闭了该会话！ 7）关闭会话（销毁会话） [root@1000phone ~]# tmux ls aaa: 2 windows (created Wed Aug 30 16:54:33 2017) [112x22] bbb: 1 windows (created Wed Aug 30 19:02:09 2017) [112x22] [root@1000phone ~]# tmux kill-session -t bbb [root@1000phone ~]# tmux ls aaa: 2 windows (created Wed Aug 30 16:54:33 2017) [112x22] 8）重命名会话 [root@1000phone ~]# tmux ls wangshibo: 1 windows (created Sun Sep 30 10:17:00 2018) [136x29] (attached) [root@1000phone ~]# tmux rename -t wangshibo kevin [root@1000phone ~]# tmux ls kevin: 1 windows (created Sun Sep 30 10:17:00 2018) [136x29] (attached) ​```","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Tools","slug":"Tools","permalink":"https://cyylog.github.io/tags/Tools/"}]},{"title":"Nginx性能优化","slug":"Linux/Nginx/Nginx性能优化","date":"2019-08-04T17:55:39.000Z","updated":"2020-05-25T13:57:06.488Z","comments":true,"path":"2019/08/05/linux/nginx/nginx-xing-neng-you-hua/","link":"","permalink":"https://cyylog.github.io/2019/08/05/linux/nginx/nginx-xing-neng-you-hua/","excerpt":"","text":"当我需要进行性能优化时，说明我们服务器无法满足日益增长的业务。性能优化是一个比较大的课题，需要从以下几个方面进行探讨 当前系统结构瓶颈 了解业务模式 性能与安全 1、当前系统结构瓶颈首先需要了解的是当前系统瓶颈，用的是什么，跑的是什么业务。里面的服务是什么样子，每个服务最大支持多少并发。比如针对nginx而言，我们处理静态资源效率最高的瓶颈是多大？能支持多少qps访问请求？怎么得出系统当前的结构瓶颈？ 可以通过查看当前cpu负荷，内存使用率，进程使用率来做简单判断。还可以通过操作系统的一些工具来判断当前系统性能瓶颈，如分析对应的日志，查看请求数量。也可以通过nginx http_stub_status_module模块来查看对应的连接数，总握手次数，总请求数。也可以对线上进行压力测试，来了解当前的系统能性能，并发数，做好性能评估。 2、了解业务模式虽然我们是在做性能优化，但还是要熟悉业务，最终目的都是为业务服务的。我们要了解每一个接口业务类型是什么样的业务，比如电子商务抢购模式，这种情况平时流量会很小，但是到了抢购时间，流量一下子就会猛涨。也要了解系统层级结构，每一层在中间层做的是代理还是动静分离，还是后台进行直接服务。需要我们对业务接入层和系统层次要有一个梳理 3、性能与安全性能与安全也是一个需要考虑的因素，往往大家注重性能忽略安全或注重安全又忽略性能。比如说我们在设计防火墙时，如果规则过于全面肯定会对性能方面有影响。如果对性能过于注重在安全方面肯定会留下很大隐患。所以大家要评估好两者的关系，把握好两者的孰重孰轻，以及整体的相关性。权衡好对应的点。 4、系统与nginx性能优化大家对相关的系统瓶颈及现状有了一定的了解之后，就可以根据影响性能方面做一个全体的评估和优化。 网络（网络流量、是否有丢包，网络的稳定性都会影响用户请求） 系统（系统负载、饱和、内存使用率、系统的稳定性、硬件磁盘是否有损坏） 服务（连接优化、内核性能优化、http服务请求优化都可以在nginx中根据业务来进行设置） 程序（接口性能、处理请求速度、每个程序的执行效率） 数据库、底层服务 上面列举出来每一级都会有关联，也会影响整体性能，这里主要关注的是nginx服务这一层。 1、文件句柄在linux/unix操作系统中一切皆文件，我们的设备是文件，文件是文件，文件夹也是文件。当我们用户每发起一次请求，就会产生一个文件句柄。文件句柄可以简单的理解为文件句柄就是一个索引。文件句柄就会随着请求量的增多,进程调用频繁增加，那么产生的文件句柄也就会越多。 系统默认对文件句柄是有限制的，不可能会让一个进程无限制的调用句柄。因为系统资源是有限的，所以我们需要限制每一个服务能够使用多大的文件句柄。操作系统默认使用的文件句柄是1024个句柄。 2、设置方式 系统全局性修改 用户局部性修改 进程局部性修改 3、系统全局性修该和用户局部性修改[root@server ~]#vim /etc/security/limits.conf在文件最下面找到 #* soft core 0 #* hard rss 10000 #@student hard nproc 20 #@faculty soft nproc 20 #@faculty hard nproc 50 #ftp hard nproc 0 #@student - maxlogins 4 #root只是针对root这个用户来限制，soft只是发提醒，操作系统不会强制限制,一般的站点设置为一万左右就ok了 root soft nofile 65535 root hard nofile 65535 # *代表通配符 所有的用户 * soft nofile 25535 * hard nofile 25535可以看到root和，root代表是root用户，代表的是所有用户，后面的数字就是文件句柄大小。大家可以根据个人业务来进行设置。 4、进程局部性修改[root@server ~]#vim /etc/nginx/nginx.conf user nginx; worker_processes 1; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; worker_rlimit_nofile 65535; #进程限制 events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main &#39;$http_user_agent&#39; &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39; &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39; &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot; &#39; &#39;&quot;$args&quot; &quot;$request_uri&quot;&#39;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; }worker_rlimit_nofile 是在进程上面进行限制。 5、cpu的亲和配置cpu的亲和能够使nginx对于不同的work工作进程绑定到不同的cpu上面去。就能够减少在work间不断切换cpu，把进程通常不会在处理器之间频繁迁移，进程迁移的频率小，来减少性能损耗。nginx 亲和配置 查看物理cpu [root@server ~]#cat /proc/cpuinfo|grep &quot;physical id&quot;|sort |uniq|wc -l查看cpu核心数 [root@server ~]#cat /proc/cpuinfo|grep &quot;cpu cores&quot;|uniq查看cpu使用率 [root@server ~]#top 回车后按 16、配置worker_processes[root@server ~]#vim /etc/nginx/nginx.conf将刚才查看到自己cpu * cpu核心就是worker_processes worker_processes 2; #根据自己cpu核心数配置7、cpu亲和配置假如小菜的配置是2cpu，每个cpu是8核。配置如下 worker_processes 16; worker_cpu_affinity 1010101010101010 0101010101010101;配置完成后可以通过下面命令查看nginx进程配置在哪个核上 [root@server ~]#ps -eo pid,args,psr |grep [n]ginx在nginx 1.9版本之后，就帮我们自动绑定了cpu; worker_cpu_affinity auto;5、nginx通用配置优化[root@server ~]#vim /etc/nginx/nginx.conf #将nginx进程设置为普通用户，为了安全考虑 user nginx; #当前启动的worker进程，官方建议是与系统核心数一直 worker_processes 2; #方式一， 第一个work进程绑定第一个cpu核心，第二个work进程绑定到第二个cpu核心，依次内推 直到弟16个 #wokrer_cpu_affinity 0000000000000000 0000000000000001 0000000000000010 0000000000000100 ... 1000000000000000 #方式二，当 worker_processes 2 时，表明 第一work进程可以绑定第 2 4 6 8 10 12 14 16 核心，那么第二work进程就绑定 奇数核心 #worker_cpu_affinity 1010101010101010 0101010101010101; #方式三，就是自动分配绑定 worker_cpu_affinity auto; #日志配置成warn error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; #针对 nginx 句柄的文件限制 worker_rlimit_nofile 35535; #事件模型 events { #使用epoll内核模型 user epoll; #每一个进程可以处理多少个连接，如果是多核可以将连接数调高 worker_processes * 1024 worker_connections 10240; } http { include /etc/nginx/mime.types; default_type application/octet-stream; charset utf-8; #设置字符集 #设置日志输出格式，根据自己的情况设置 log_format main &#39;$http_user_agent&#39; &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39; &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39; &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot; &#39; &#39;&quot;$args&quot; &quot;$request_uri&quot;&#39;; access_log /var/log/nginx/access.log main; sendfile on; #对静态资源的处理比较有效 #tcp_nopush on; #如果做静态资源服务器可以打开 #tcp_nodeny on; #当nginx做动态的服务时可以选择打开 keepalive_timeout 65; ######## #Gzip module gzip on; #文件压缩默认可以打开 gzip_disable &quot;MSIE [1-6]\\.&quot;; #对于有些浏览器不能识别压缩，需要过滤如ie6 gzip_http_version 1.1; include /etc/nginx/conf.d/*.conf; } #查看 核心绑定的nginx work进程 [root@server ~]#ps -eo pid,args,psr | grep [n]ginx6、ab接口压力测试工具ab是Apache超文本传输协议(HTTP)的性能测试工具。其设计意图是描绘当前所安装的Apache的执行性能，主要是显示你安装的Apache每秒可以处理多少个请求。 # 安装工具 [root@server ~]#yum install httpd-tools # 使用 [root@server ~]#ab -n 2000 -c 2 http://127.0.0.1/index.html -n 总的请求数 -c 并发数 -k 是否开启长连接1、参数选项-n：即requests，用于指定压力测试总共的执行次数 -c：即concurrency，用于指定的并发数 -t：即timelimit，等待响应的最大时间(单位：秒) -b：即windowsize，TCP发送/接收的缓冲大小(单位：字节) -p：即postfile，发送POST请求时需要上传的文件，此外还必须设置-T参数 -u：即putfile，发送PUT请求时需要上传的文件，此外还必须设置-T参数 -T：即content-type，用于设置Content-Type请求头信息，例如：application/x-www-form-urlencoded，默认值为text/plain -v：即verbosity，指定打印帮助信息的冗余级别 -w：以HTML表格形式打印结果 -i：使用HEAD请求代替GET请求 -x：插入字符串作为table标签的属性 -y：插入字符串作为tr标签的属性 -z：插入字符串作为td标签的属性 -C：添加cookie信息，例如：&quot;Apache=1234&quot;(可以重复该参数选项以添加多个) -H：添加任意的请求头，例如：&quot;Accept-Encoding: gzip&quot;，请求头将会添加在现有的多个请求头之后(可以重复该参数选项以添加多个) -A：添加一个基本的网络认证信息，用户名和密码之间用英文冒号隔开 -P：添加一个基本的代理认证信息，用户名和密码之间用英文冒号隔开 -X：指定使用的和端口号，例如:&quot;126.10.10.3:88&quot; -V：打印版本号并退出 -k：使用HTTP的KeepAlive特性 -d：不显示百分比 -S：不显示预估和警告信息 -g：输出结果信息到gnuplot格式的文件中 -e：输出结果信息到CSV格式的文件中 -r：指定接收到错误信息时不退出程序 -H：显示用法信息，其实就是ab -help2、内容解释Server Software: nginx/1.10.2 (服务器软件名称及版本信息) Server Hostname: 192.168.1.106(服务器主机名) Server Port: 80 (服务器端口) Document Path: /index1.html. (供测试的URL路径) Document Length: 3721 bytes (供测试的URL返回的文档大小) Concurrency Level: 1000 (并发数) Time taken for tests: 2.327 seconds (压力测试消耗的总时间) Complete requests: 5000 (的总次数) Failed requests: 688 (失败的请求数) Write errors: 0 (网络连接写入错误数) Total transferred: 17402975 bytes (传输的总数据量) HTML transferred: 16275725 bytes (HTML文档的总数据量) Requests per second: 2148.98 [#/sec] (mean) (平均每秒的请求数) 这个是非常重要的参数数值，服务器的吞吐量 Time per request: 465.338 [ms] (mean) (所有并发用户(这里是1000)都请求一次的平均时间) Time request: 0.247 [ms] (mean, across all concurrent requests) (单个用户请求一次的平均时间) Transfer rate: 7304.41 [Kbytes/sec] received 每秒获取的数据长度 (传输速率，单位：KB/s) ... Percentage of the requests served within a certain time (ms) 50% 347 ## 50%的请求在347ms内返回 66% 401 ## 60%的请求在401ms内返回 75% 431 80% 516 90% 600 95% 846 98% 1571 99% 1593 100% 1619 (longest request)3、示例演示[root@server ~]#ab -n 50 -c 20 http://walidream.com/sub_module输出内容 Server Software: nginx/1.14.1 Server Hostname: walidream.com Server Port: 80 Document Path: /sub_module Document Length: 169 bytes Concurrency Level: 20 Time taken for tests: 0.005 seconds Complete requests: 50 Failed requests: 0 Write errors: 0 Non-2xx responses: 50 Total transferred: 14900 bytes HTML transferred: 8450 bytes Requests per second: 9746.59 [#/sec] (mean) Time per request: 2.052 [ms] (mean) Time per request: 0.103 [ms] (mean, across all concurrent requests) Transfer rate: 2836.41 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 1 Processing: 1 1 0.3 1 2 Waiting: 0 1 0.2 1 1 Total: 1 2 0.3 2 2 Percentage of the requests served within a certain time (ms) 50% 2 66% 2 75% 2 80% 2 90% 2 95% 2 98% 2 99% 2 100% 2 (longest request)5、注意事项● 测试机与被测试机要分开 ● 不要对线上的服务器做压力测试 ● 观察测试工具ab所在机器，以及被测试的前端机的CPU、内存、网络等都不超过最高限度的75% 6、ab性能指标1、吞吐率（Requests per second）服务器并发处理能力的量化描述，单位是reqs/s，指的是在某个并发用户数下单位时间内处理的请求数。某个并发用户数下单位时间内能处理的最大请求数，称之为最大吞吐率。记住：吞吐率是基于并发用户数的。这句话代表了两个含义： ● 吞吐率和并发用户数相关 ● 不同的并发用户数下，吞吐率一般是不同的计算公式：总请求数/处理完成这些请求数所花费的时间，即 Request per second=Complete requests/Time taken for tests必须要说明的是，这个数值表示当前机器的整体性能，值越大越好 2、并发连接数（The number of concurrent connections）并发连接数指的是某个时刻服务器所接受的请求数目，简单的讲，就是一个会话。 3、并发用户数（Concurrency Level）要注意区分这个概念和并发连接数之间的区别，一个用户可能同时会产生多个会话，也即连接数。在HTTP/1.1下，IE7支持两个并发连接，IE8支持6个并发连接，FireFox3支持4个并发连接，所以相应的，我们的并发用户数就得除以这个基数。 4.用户平均请求等待时间（Time per request）计算公式：处理完成所有请求数所花费的时间/（总请求数/并发用户数），即： Time per request=Time taken for tests/（Complete requests/Concurrency Level）5.服务器平均请求等待时间（Time per request:across all concurrent requests）计算公式：处理完成所有请求数所花费的时间/总请求数，即： Time taken for/testsComplete requests可以看到，它是吞吐率的倒数。同时，它也等于用户平均请求等待时间/并发用户数，即 Time per request/Concurrency Level","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://cyylog.github.io/tags/Nginx/"}]},{"title":"Zabbix-源码安装","slug":"监控/zabbix/Zabbix源码安装","date":"2019-08-04T17:55:39.000Z","updated":"2020-10-30T04:11:56.488Z","comments":true,"path":"2019/08/05/jian-kong/zabbix/zabbix-yuan-ma-an-zhuang/","link":"","permalink":"https://cyylog.github.io/2019/08/05/jian-kong/zabbix/zabbix-yuan-ma-an-zhuang/","excerpt":"","text":"Zabbix源码安装1:前期准备 注意安装zabbix需要lnmp环境可以使用脚本安装lnmp 这里我进行源码安装一步步的操作 建议使用脚本进行 用源码安装比较慢 (1) 关闭防火墙和selinux 建议可以实行放行策略(2)创建安装目录 mkdir -pv /cyylog/{mysql-5.7,nginx-1.16,php-7.2,zabbix-4.4} mkdir -pv /cyylog/mysql-5.7/data ln -s /cyylog/mysql-5.7 /cyylog/mysql ln -s /cyylog/nginx-1.16 /cyylog/nginx ln -s /cyylog/php-7.2 /cyylog/php ln -s /cyylog/zabbix-4.4 /cyylog/zabbix (3)创建用户 useradd -s /sbin/nologin mysql useradd -s /sbin/nologin nginx useradd -s /sbin/nologin zabbix 也可执行脚本 2:安装mysql(1)下载mysql源码包wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.29.tar.gz wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-boost-5.7.29.tar.gz (2) 使用yum安装依赖包yum install -y cmake gcc gcc-c++ openssl-devel ncurses-devel (3) 解压并进入进行安装tar xvf mysql-5.7.29.tar.gz tar xvf mysql-boost-5.7.29.tar.gz -C /cyylog/ 配置 cmake \\ -DCMAKE_INSTALL_PREFIX=/cyylog/mysql-5.7 \\ -DMYSQL_DATADIR=/cyylog/mysql-5.7/data \\ -DDEFAULT_CHARSET=utf8 \\ -DDEFAULT_COLLATION=utf8_unicode_ci \\ -DWITH_READLINE=1 \\ -DWITH_SSL=system \\ -DWITH_EMBEDDED_SERVER=1 \\ -DENABLED_LOCAL_INFILE=1 \\ -DDEFAULT_COLLATION=utf8_general_ci \\ -DWITH_MYISAM_STORAGE_ENGINE=1 \\ -DWITH_INNOBASE_STORAGE_ENGINE=1 \\ -DWITH_DEBUG=0 \\ -DWITH_BOOST=/cyylog/mysql-5.7.29/boost/boost_1_59_0 编译且安装 make & make install (4) 创建需要的文件及更改属主和属组mkdir -pv /cyylog/mysql/log touch /cyylog/mysql/log/mariadb.log touch /cyylog/mysql/log/mariadb.pid chown -R /cyylog/{mysql-5.7,mysql-5.7.29,mysql} (5) 初始化数据修改配置文件 vim /etc/my.cnf [mysqld] datadir=/cyylog/mysql/data #数据存储的地方 socket=/cyylog/mysql/mysql.sock #sock文件的路径 skip-grant-tables #跳过登录认证 user=mysql explicit_defaults_for_timestamp=true [mysqld_safe] log-error=/cyylog/mysql/log/mariadb.log #错误日志存放的地方 pid-file=/cyylog/mysql/log/mariadb.pid (6) 添加至环境变量vim /etc/profile 修改末尾添加两行 export PATH=$PATH:/cyylog/mysql/support-files export PATH=$PATH:/cyylog/mysql/bin 保存退出刷新环境变量 source /etc/profile/ (7) 初始化启动mysqlmysqld --initialize --user=mysql --basedir=/cyylog/mysql --datadir=/cyylog/mysql/data mysql.server start ln -s /cyylog/mysql/mysql.sock /tmp/ (8) 下载zabbix源码包并进行解压wget https://sourceforge.net/projects/zabbix/files/ZABBIX%20Latest%20Stable/4.4.5/zabbix-4.4.5.tar.gz tar xvf zabbix-4.4.5.tar.gz cd zabbix-4.4.5/database/mysql 登录mysql 命令为 mysql -u root 进入后执行以下命令 use mysql; create database zabbix default character set utf8; update mysql.user set authentication_string=password('修改的密码') where user='root';use zabbix; source schema.sql; source images.sql; source data.sql; quit; 最后恢复密码登录mysql 修改文件 vim /etc/my.cnf 去掉 skip-grant-tables 保存退出重启mysql服务 mysql.sercer restart 添加lib文件 echo “/cyylog/mysql/lib” > /etc/ld.so.conf.d/mysql.conf ldconfig -v 3:安装nginx(1) 下载 nginx 并解压wget http://nginx.org/download/nginx-1.16.1.tar.gz tar xvf nginx-1.16.1.tar.gz (2) 编译安装并添加环境变量cd nginx-1.16.1 ./configure --prefix=/cyylog/nginx-1.16 --user=nginx --group=nginx --without-select_module --without-poll_module --with-http_ssl_module --with-pcre --with-debug make make install 添加变量 vim /etc/profile 追加一行 export PATH=$PATH://cyylog/nginx/sbin 保存退出刷新变量 source /etc/profile (3)更改 nginx 的属主和属组以及修改配置文件chown nginx:nginx -R /cyylog/nginx-1.16 修改配置文件 vim /cyylog/nginx/conf/nginx.conf 修改启动用户 user nginx; 启动nginx nginx 4:安装php(1) 下载php源码并井进行解压wget https://www.php.net/distributions/php-7.2.27.tar.gz tar xvf php-7.2.27.tar.gz (2) 安装及解决依赖 yum install -y libxml2-devel openssl-devel net-snmp net-snmp-devel libcurl-devel libjpeg-devel libpng-devel libicu-devel openldap-devel bzip2 bzip2-devel freetype-devel gmp-devel readline-devel libxslt-devel fontconfig cd php-7.2.27 ./configure --prefix=/cyylog/php-7.2 --with-mysqli=/cyylog/mysql/bin/mysql_config --enable-inline-optimization --enable-fpm --enable-soap --enable-pcntl --enable-xml --with-libxml-dir --with-xmlrpc --with-openssl --with-mhash --with-pcre-regex --with-sqlite3 --with-zlib --enable-bcmath --with-iconv --with-bz2 --enable-calendar --with-curl --with-cdb --enable-dom --enable-exif --enable-fileinfo --enable-filter --with-pcre-dir --enable-ftp --with-gd --with-openssl-dir --with-jpeg-dir --with-png-dir --with-freetype-dir --with-gettext --with-gmp --with-mhash --enable-json --enable-mbstring --disable-mbregex --disable-mbregex-backtrack --with-libmbfl --with-onig --enable-pdo --with-pdo-mysql --with-zlib-dir --with-pdo-sqlite --with-readline --enable-session --enable-shmop --enable-simplexml --enable-sockets --enable-sysvmsg --enable-sysvsem --enable-sysvshm --enable-wddx --with-libxml-dir --with-xsl --enable-zip --enable-mysqlnd-compression-support --with-pear --without-pear make make install (3) 拷贝服务和配置文件及属主和属组cp /root/php-7.2.27/sapi/fpm/php-fpm.service /usr/lib/systemd/system/php-fpm.service cp /cyylog/php-7.2/etc/{php-fpm.conf.default,php-fpm.conf} cp /cyylog/php-7.2/etc/php-fpm.d/www.conf{.default,} cp php.ini-production /cyylog/php-7.2/lib/php.ini chown nginx:nginx -R /cyylog/php-7.2 (4) 修改配置文件并启动 #### 修改php.ini配置文件 vim /cyylog/php/lib/php.ini 修改四行 post_max_size = 16M max_execution_time = 300 max_input_time = 300 date.timezone = PRC #### 启动php服务 systemctl start php-fpm.service && systemctl enable php-fpm.service #### 修改nginx.conf文件是nginx支持php vim /cyylog/nginx/conf/nginx.conf 修改如下 location ~ \\.php$ { root /cyylog/nginx/html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /cyylog/nginx/html$fastcgi_script_name; include fastcgi_params; } #### 编写测试php文件 vim /cyylog/nginx/html/index.php #### 重启nginx服务 nginx -s reload 重启nginx服务 nginx -s reload 5:安装zabbix(1) 安装依赖以及编译安装yum localinstall -y libevent-devel-2.0.21-4.el7.x86_64.rpm yum install unixODBC-devel mysql-devel net-snmp-devel libxml2-devel libcurl-devel libevent-devel -y 配置cd zabbix-4.4.5 ./configure --prefix=/cyylog/zabbix-4.4 --enable-server --enable-agent --with-mysql=/cyylog/mysql/bin/mysql_config --enable-ipv6 --with-netsnmp --with-libcurl --with-libxml2 make make install (2) 配置环境变量 vim /etc/profile 追加一行 export PATH=$PATH://cyylog/zabbix/sbin #### 保存退出 刷新 source /etc/profile (3) 修改配置文件 vim /cyylog/zabbix/etc/zabbix_server.conf ##修改如下 DBUser=root DBPassword=beimi123 拷贝zabbix至nginx的目录下 cp -R frontends/php/* /cyylog/nginx/html/ 重启nginx服务 nginx -s reload 访问页面ok就行 注意连接数据库那个步骤需要将服务器ip改为127.0.0.1 不能使用localhost 否则会报错 接下会有个配置文件无法安装需手动下载下来传到ngin目录下 最后完成 登录账户为 admin 密码zabbix 登录后界面为","categories":[{"name":"监控","slug":"监控","permalink":"https://cyylog.github.io/categories/%E7%9B%91%E6%8E%A7/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://cyylog.github.io/tags/zabbix/"}]},{"title":"zabbix-邮箱报警设置","slug":"监控/zabbix/Zabbix邮件报警","date":"2019-07-04T17:55:39.000Z","updated":"2020-10-30T04:12:05.884Z","comments":true,"path":"2019/07/05/jian-kong/zabbix/zabbix-you-jian-bao-jing/","link":"","permalink":"https://cyylog.github.io/2019/07/05/jian-kong/zabbix/zabbix-you-jian-bao-jing/","excerpt":"","text":"Zabbix 邮件报警前期准备工作：电脑登录网易邮箱配置，把自己的授权码看一下，并写入配置文件 server端安装配置邮件服务器[root@master ~]# yum -y install mailx [root@master ~]# mailx -V 12.5 7/5/10 配置公网邮箱信息：发邮件：[root@master ~]# vim /etc/mail.rc #追加以下内容 set from=cyylog@163.com #（邮箱地址） set smtp=smtp.163.com #smtp服务器） 发邮件服务器 ---163默认 set smtp-auth-user=cyylog@163.com #(用户名) set smtp-auth-password=Password #（邮箱密码）授权之后的密码 set smtp-auth=login #默认 ###### 测试 [root@master ~]# echo \"test mail from zabbix.server.com\" |mail -s \"test mail\" cyylog@163.com 然后163邮箱就会收到信息 报警媒体的配置:首先需要配置 Zabbix 的邮件功能。点击 管理-&gt;报警媒介类型-&gt;创建媒体类型 然后在页面中填入你的报警媒介类型信息,例如下图所示:注：脚本名称任意，存放于/usr/lib/zabbix/alertscripts (生产上的测试服放这：s /usr/local/zabbix/share/zabbix/alertscripts） 名称：sendmail //名称任意类型：脚本脚本名称：sendmail.sh脚本参数： //一定要写，否则可能发送不成功{ALERT.SENDTO} //照填，收件人变量{ALERT.SUBJECT} //照填，邮件主题变量，变量值来源于‘动作’中的‘默认接收人’{ALERT.MESSAGE} //照填，邮件正文变量，变量值来源于‘动作’中的‘默认信息’ 配置完成后,不要忘记点击存档,保存你的配置。 修改zabbix服务端配置文件＆编写脚本：# 查看指定脚本的存储路径: [root@master ~]# vim /etc/zabbix/zabbix_server.conf AlertScriptsPath=/usr/lib/zabbix/alertscripts 编写邮件脚本: [root@master alertscripts]# cd /usr/lib/zabbix/alertscripts [root@master alertscripts]# vim sendmail.sh #!/bin/sh #export.UTF-8 -----字符集可以删除掉 #send mail messages=echo $3 | tr '\\r\\n' '\\n' subject=echo $2 | tr '\\r\\n' '\\n' echo \"${messages}\" | mail -s \"${subject}\" $1 >>/tmp/mailx.log 2>&1 修改权限： [root@master alertscripts]# chmod u+x sendmail.sh && chown zabbix.zabbix sendmail.sh 创建的脚本名称要和定义的脚本名称一样 修改admin用户的报警媒介：用户默认是没有设置报警媒介的，设置后就可以接收报警消息了。 触发器的配置:接下来,点击配置-&gt;主机 我们给 agent-19 这台主机增加一个触发器。点击 agent-19 这一行中的“触发器”,然后再点击创建触发器。该页各配置项含义如下:名称:填入触发器的名字表达式:用于配置触发器的触发条件,点击添加按钮有条件选项。 —-键值多重事件产生:如果选中,则问题如果持续多重的发生则每次都触发,否则只触发一次点击表达式右侧的添加按钮: 再点击项目右侧的选择,选择我们之前配置过的“web.server.online.monitor”,并设置触发的阀值,如下图所示 Zabbix 会自动生成表达式。接下来根据情况选择事件的严重性。配置完毕后,点击存档保存。 动作的配置:点击:配置-&gt;动作-&gt;事件源下拉菜单中选择触发器-&gt;创建动作可以在内容中使用 Zabbix 内置宏,邮件发出时会自动将宏替换成对应的值。 名称：任意写 默认接收人： 故障级别：{TRIGGER.STATUS}。服务器：【{HOSTNAME1} 】 发生：{TRIGGER.NAME} 故障！ 注：默认接收人：相当于邮件的主题 默认信息：邮件的主题 告警主机：{HOSTNAME1} 告警时间：{EVENT.DATE} {EVENT.TIME} 告警等级：{TRIGGER.SEVERITY} 告警信息：{TRIGGER.NAME} 告警项目：{TRIGGER.KEY1} 问题详情：{ITEM.NAME}：{ITEM.VALUE} 当前状态：{TRIGGER.STATUS}：{ITEM.VALUE1} 事件ID：{EVENT.ID}恢复邮件：恢复主题： 服务器：【{HOSTNAME1}】故障已恢复。故障原因：{TRIGGER.NAME} 恢复信息：恢复邮件的正文。当故障恢复正常后也发邮件通知一下。 点击:操作-&gt;编辑： 发送间隔：60秒步骤：发送10次发送到：admin用户仅使用：sendmail方式发送 —-脚本。 方式可以自行设置，根据实际工作要求 需要特别解释一下的是“步骤”部分的配置。所谓步骤是指报警可以有多个步骤,做不同的报警。例如,自从 1 到 3,就是指报警的步骤有三个。步骤持续时间就是一定时间后如果监控人员仍未响应报警就进入下一个报警步骤。例如,发邮件给你报警,如果60 秒后你没响应,那就发 jabber 信息提醒你。如果 60 秒后还没响应,那就发短信给你。要是还没响应,就没有然后了。你可以形象的把它理解为 Zabbix 的一哭二闹三上吊。到此,一个邮件报警功能就配置完毕了。如果你想立即看到结果,可以修改触发器的条件,将条件的阀值设置为 N&gt;0.0003。你马上就会收到 Zabbix 发来的报警邮件了。 补充：邮件美化（修改默认信息） &lt;table border=\"1\" bordercolor=\"black\" cellspacing=\"0px\" cellpadding=\"4px\"> &lt;tr > &lt;td>告警主机&lt;/td> &lt;td bgcolor=\"#FF3333\">{HOSTNAME1}&lt;/td> &lt;/tr> &lt;tr> &lt;td>告警时间&lt;/td> &lt;td>{EVENT.DATE} {EVENT.TIME}&lt;/td> &lt;/tr> &lt;tr> &lt;td>告警等级&lt;/td> &lt;td>{TRIGGER.SEVERITY}&lt;/td> &lt;/tr> &lt;tr> &lt;td>告警信息&lt;/td> &lt;td>{TRIGGER.NAME}&lt;/td> &lt;/tr> &lt;tr> &lt;td>告警项目&lt;/td> &lt;td>{TRIGGER.KEY1}&lt;/td> &lt;/tr> &lt;tr > &lt;td>问题详情&lt;/td> &lt;td bgcolor=\"#FF3333\">{ITEM.NAME}:&amp;nbsp;{ITEM.VALUE}&lt;/td> &lt;/tr> &lt;tr> &lt;td>当前状态&lt;/td> &lt;td>{TRIGGER.STATUS}:&amp;nbsp;{ITEM.VALUE1}&lt;/td> &lt;/tr> &lt;tr> &lt;td>事件ID&lt;/td> &lt;td>{EVENT.ID}&lt;/td> &lt;/tr> &lt;/table>","categories":[{"name":"监控","slug":"监控","permalink":"https://cyylog.github.io/categories/%E7%9B%91%E6%8E%A7/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://cyylog.github.io/tags/zabbix/"}]},{"title":"MySQL部署之源码安装","slug":"SQL/MySQL部署之源码安装","date":"2019-05-27T14:53:26.000Z","updated":"2020-05-25T13:56:34.046Z","comments":true,"path":"2019/05/27/sql/mysql-bu-shu-zhi-yuan-ma-an-zhuang/","link":"","permalink":"https://cyylog.github.io/2019/05/27/sql/mysql-bu-shu-zhi-yuan-ma-an-zhuang/","excerpt":"","text":"所需要的依赖及安装MySQL的包# yum -y update # yum -y groupinstall \"Development Tools\" # yum -y install gcc gcc-c++ ncurses ncurses-devel bison libgcrypt perl make cmake # wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-boost-5.7.24.tar.gz 在系统中添加运行mysqld进程的用户mysql[root@mysql_source ~]# groupadd mysql [root@mysql_source ~]# useradd -M -g mysql -s /sbin/nologin mysql 在系统中添加自定义MySQL数据库目录及其他必要目录[root@mysql_source ~]# mkdir -p /usr/local/mysqld/{data,mysql,log,tmp} [root@mysql_source ~]# chown -R mysql:mysql /usr/local/mysqld/* 将mysql-boost-5.7.24.tar.gz解压到当前目录,并执行部署操作[root@mysql_source ~]# tar xf mysql-boost-5.7.24.tar.gz [root@mysql_source ~]# cd mysql-5.7.24 [root@mysql_source mysql-5.7.24]# $ cmake . -DCMAKE_INSTALL_PREFIX=/usr/local/mysqld/mysql \\ -DMYSQL_DATADIR=/usr/local/mysqld/data \\ -DWITH_BOOST=/root/mysql-5.7.24/boost \\ -DDEFAULT_CHARSET=utf8 ...... -- Configuring done -- Generating done -- Build files have been written to: /root/mysql-5.7.24 [root@mysql_source mysql-5.7.24]# echo $? 0 [root@mysql_source mysql-5.7.24]# make -j `lscpu | awk 'NR==4{ print $2 }'` ...... [100%] Built target udf_example [root@mysql_source mysql-5.7.24]# echo $? 0 [root@mysql_source mysql-5.7.24]# make install ...... -- Installing: /usr/local/mysqld/mysql/support-files/mysql.server [root@mysql_source mysql-5.7.24]# echo $? 0 [root@mysql_source mysql-5.7.24]# Congratulations Complete! 初始化MySQL安装配置1.提升MySQL命令为系统级别命令[root@mysql_source ~]# echo \"export PATH=$PATH:/usr/local/mysqld/mysql/bin\" >>/etc/profile [root@mysql_source ~]# source /etc/profile 2.拷贝默认配置文件至/etc/my.cnf中[root@mysql_source mysql]# chown -R mysql.mysql /usr/local/mysqld/* [root@mysql_source ~]# cd /usr/local/mysqld/mysql/mysql-test/include [root@mysql_source include]# cp /etc/{my.cnf,my.cnf.bak} [root@mysql_source include]# cp default_mysqld.cnf /etc/my.cnf cp：是否覆盖\"/etc/my.cnf\"？ y [root@mysql_source include]# vim /etc/my.cnf [mysqld] basedir = /usr/local/mysqld/mysql datadir = /usr/local/mysqld/data tmpdir = /usr/local/mysqld/tmp socket = /usr/local/mysqld/tmp/mysql.sock pid_file = /usr/local/mysqld/tmp/mysqld.pid log_error = /usr/local/mysqld/log/mysql_error.log slow_query_log_file = /usr/local/mysqld/log/slow_warn.log server_id = 11 user = mysql port = 3306 bind-address = 0.0.0.0 character-set-server = utf8 default_storage_engine = InnoDB 3.执行数据库服务初始化操作[root@mysql_source mysql]# mysqld --defaults-file=/etc/my.cnf --initialize --user='mysql' [root@mysql_source mysql]# 4.启动mysqld服务[root@mysql_source mysql]# mysqld_safe --defaults-file=/etc/my.cnf & [1] 25705 2018-12-28T09:19:35.334751Z mysqld_safe Logging to '/usr/local/mysqld/log/mysql_error.log'. 2018-12-28T09:19:35.379829Z mysqld_safe Starting mysqld daemon with databases from /usr/local/mysqld/data 5.设置mysql.socket软链接到mysql命令指定的目录中[root@mysql_source ～]# ln -s /usr/local/mysqld/tmp/mysql.sock /tmp/mysql.sock 6.配置mysqld服务的管理工具 [root@mysql_source support-files]# cd /usr/local/mysqld/mysql/support-files [root@mysql_source support-files]# cp mysql.server /etc/init.d/mysqld [root@mysql_source support-files]# chkconfig --add mysqld [root@mysql_source support-files]# chkconfig mysqld on 登录数据库并进行更改密码[root@mysql_source mysql]# grep \"password\" /usr/local/mysqld/log/mysql_error.log 2018-12-28T09:18:34.214401Z 1 [Note] A temporary password is generated for root@localhost: ejhszb2:m3wJ [root@mysql_source tmp]# mysql -uroot -p\"ejhszb2:m3wJ\" mysql: [Warning] Using a password on the command line interface can be insecure. Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 2 Server version: 5.7.24-log Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> alter user 'root'@'localhost' identified by \"(Cyylog..1228)\"; 平常中常用的MySQL部署参数: -DCMAKE_INSTALL_PREFIX=/usr/local/mysqld/mysql \\ -DMYSQL_DATADIR=/usr/local/mysqld/data \\ -DDOWNLOAD_BOOST=1 \\ -DWITH_BOOST=/root/mysql-5.7.24/boost \\ -DSYSCONFDIR=/etc \\ -DWITH_INNOBASE_STORAGE_ENGINE=1 \\ -DWITH_PARTITION_STORAGE_ENGINE=1 \\ -DWITH_FEDERATED_STORAGE_ENGINE=1 \\ -DWITH_BLACKHOLE_STORAGE_ENGINE=1 \\ -DWITH_MYISAM_STORAGE_ENGINE=1 \\ -DENABLED_LOCAL_INFILE=1 \\ -DENABLE_DTRACE=0 \\ -DDEFAULT_CHARSET=utf8 \\ -DDEFAULT_COLLATION=utf8_general_ci \\ -DWITH_EMBEDDED_SERVER=1 绕过验证密码登录 修改密码[root@mysql ~]# vim /etc/my.cnf [mysqld] skip-grant-tables=1 [root@mysql ~]# systemctl restart mysqld [root@mysql ~]# mysql -uroot Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 2 Server version: 5.7.24 MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> alter user 'root'@'localhost' identified by \"(Cyylog..1229)\"; ERROR 1290 (HY000): The MySQL server is running with the --skip-grant-tables option so it cannot execute this statement mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 4 rows in set (0.00 sec) mysql> use mysql; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> show tables; ...... ...... ...... | user | +---------------------------+ 31 rows in set (0.00 sec) mysql> select User,Host,authentication_string from user; +---------------+-----------+-------------------------------------------+ | User | Host | authentication_string | +---------------+-----------+-------------------------------------------+ | root | localhost | *C4571A0C807D96143700250EC4BA41780025A97F | | mysql.session | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | | mysql.sys | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | +---------------+-----------+-------------------------------------------+ 3 rows in set (0.00 sec) mysql> update user set authentication_string=password('(Cyylog@@1229)') where user='root'; Query OK, 1 row affected, 1 warning (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 1 [root@mysql ~]# vim /etc/my.cnf [mysqld] #skip-grant-tables=1 [root@mysql ~]# systemctl restart mysqld [root@mysql ~]# mysql -uroot -p\"(Cyylog@@1229)\" mysql: [Warning] Using a password on the command line interface can be insecure. Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 2 Server version: 5.7.24 MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql>","categories":[{"name":"SQL","slug":"SQL","permalink":"https://cyylog.github.io/categories/SQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://cyylog.github.io/tags/MySQL/"}]},{"title":"Nginx平滑升级","slug":"Linux/Nginx/Nginx平滑升级","date":"2019-04-27T15:18:34.000Z","updated":"2020-05-25T13:56:54.849Z","comments":true,"path":"2019/04/27/linux/nginx/nginx-ping-hua-sheng-ji/","link":"","permalink":"https://cyylog.github.io/2019/04/27/linux/nginx/nginx-ping-hua-sheng-ji/","excerpt":"","text":"Nginx 平滑升级1、查看现有的 nginx 编译参数[root@web ~]#/usr/local/nginx/sbin/nginx -V 按照原来的编译参数安装 nginx 的方法进行安装，只需要到 make，千万不要 make install 2、编译新的 nginx 源码包编译新Nginx源码，安装路径需与旧版一致 (详细过程可参见：Nginx编译安装与配置使用) [root@web ~]#./configure --prefix=/usr/local/nginx-1.14.0 --user=www --group=www --with-http_ssl_module --with-openssl=/path/to/openssl_src [root@web ~]#make 3、备份原 nginx 二进制文件备份二进制文件和 nginx 的配置文件（期间nginx不会停止服务） [root@web ~]#mv /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_$(date +%F) 4、复制新的nginx二进制文件，进入新的nginx源码包 [root@web ~]#cp /usr/local/nginx-1.14.0/objs/nginx /usr/local/nginx/sbin/ 5、测试新版本的nginx是否正常 [root@web ~]#/usr/local/nginx/sbin/nginx -t 6、给nginx发送平滑迁移信号（若不清楚pid路径，请查看nginx配置文件） [root@web ~]#kill -USR2 cat /var/run/nginx.pid 7、查看nginx pid，会出现一个nginx.pid.oldbin [root@web ~]#ll /var/run/nginx.pid* 8、从容关闭旧的Nginx进程 [root@web ~]#kill -WINCH cat /var/run/nginx.pid.oldbin 9、此时不重载配置启动旧的工作进程 [root@web ~]#kill -HUP cat /var/run/nginx.pid.oldbin 10、结束工作进程，完成此次升级 [root@web ~]#kill -QUIT cat /var/run/nginx.pid.oldbin 11、验证Nginx是否升级成功 [root@web ~]#usr/local/nginx/sbin/nginx -V 升级实战1、安装配置1.6版本的 nginx[root@web ~]# yum install -y gcc gcc-c++ pcre-devel openssl-devel zlib-devel [root@web ~]# tar zxvf nginx-1.6.0.tar.gz -C /usr/src/ [root@web ~]# cd /usr/src/nginx-1.6.0/ [root@web nginx-1.6.0]# ./configure --prefix=/usr/local/nginx --user=nginx --group=nginx --with-http_stub_status_module [root@web nginx-1.6.0]# make [root@web nginx-1.6.0]# make install [root@web nginx-1.6.0]# ln -s /usr/local/nginx/sbin/* /usr/sbin/ [root@web nginx-1.6.0]# useradd -M -s /sbin/nologin nginx [root@web nginx-1.6.0]# nginx [root@web nginx-1.6.0]# netstat -anput | grep nginx tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 19008/nginx: master 2、查看 nginx 版本[root@web nginx-1.6.0]# nginx -v nginx version: nginx/1.6.0 3、查看 nginx 现有安装的模块[root@web nginx-1.6.0]# nginx -V nginx version: nginx/1.6.0 built by gcc 4.8.5 20150623 (Red Hat 4.8.5-11) (GCC) configure arguments: --prefix=/usr/local/nginx --user=nginx --group=nginx --with-http_stub_status_module 4、访问验证[root@web nginx-1.6.0]# echo \"nginx1.6.0\" > /usr/local/nginx/html/index.html [root@web nginx-1.6.0]# elinks 192.168.20.167 5、升级 nginx将 nginx 版本进行升级 并在不影响业务的情况下添加 SSL 和 pcre 模块 [root@web ~]# tar zxvf nginx-1.11.2.tar.gz -C /usr/src/ [root@web ~]# cd /usr/src/nginx-1.11.2/ [root@web nginx-1.11.2]# ./configure --prefix=/usr/local/nginx --user=nginx --group=ngiinx --with-http_stub_status_module --with-http_ssl_module --with-pcre [root@web nginx-1.11.2]# make [root@web nginx-1.11.2]# cd [root@web ~]# mv /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_old [root@web ~]# cp /usr/src/nginx-1.11.2/objs/nginx /usr/local/nginx/sbin/ [root@web ~]# mv /usr/local/nginx/conf/nginx.conf /usr/local/nginx/conf/nginx.conf.old [root@Centos ~]# cp /usr/src/nginx-1.11.2/conf/nginx.conf /usr/local/nginx/conf/nginx.conf [root@web ~]# kill -USR2 `cat /usr/local/nginx/logs/nginx.pid` [root@web ~]# ls /usr/local/nginx/logs/ access.log error.log nginx.pid [root@web ~]# ps aux | grep nginx root 19008 0.0 0.0 24324 944 ? Ss 14:07 0:00 nginx: master process nginx nginx 19009 0.0 0.1 26832 1744 ? S 14:07 0:00 nginx: worker process root 53194 0.0 0.0 112660 976 pts/0 R+ 14:36 0:00 grep --color=auto ngin 6、验证 nginx 是否升级成功","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://cyylog.github.io/tags/Nginx/"}]},{"title":"kafka入门","slug":"SQL/kafka入门","date":"2019-04-25T17:11:26.000Z","updated":"2020-05-25T13:55:36.811Z","comments":true,"path":"2019/04/26/sql/kafka-ru-men/","link":"","permalink":"https://cyylog.github.io/2019/04/26/sql/kafka-ru-men/","excerpt":"","text":"KAFKA 消息中间件1、认识kafka1.1 kafka简介Kafka 是一个分布式流媒体平台 kafka官网：http://kafka.apache.org/ （1）流媒体平台有三个关键功能： 发布和订阅记录流，类似于消息队列或企业消息传递系统。 以容错的持久方式存储记录流。 记录发生时处理流。 （2）Kafka通常用于两大类应用： 构建可在系统或应用程序之间可靠获取数据的实时流数据管道 构建转换或响应数据流的实时流应用程序 要了解Kafka如何做这些事情，让我们深入探讨Kafka的能力。 （3）首先是几个概念： Kafka作为一个集群运行在一个或多个可跨多个数据中心的服务器上。 Kafka集群以称为 topics主题 的类别存储记录流。 每条记录都包含一个键，一个值和一个时间戳。 （4）Kafka有四个核心API： Producer API（生产者API）允许应用程序发布记录流至一个或多个kafka的topics（主题）。 Consumer API（消费者API）允许应用程序订阅一个或多个topics（主题），并处理所产生的对他们记录的数据流。 Streams API（流API）允许应用程序充当流处理器，从一个或多个topics（主题）消耗的输入流，并产生一个输出流至一个或多个输出的topics（主题），有效地变换所述输入流，以输出流。 Connector API（连接器API）允许构建和运行kafka topics（主题）连接到现有的应用程序或数据系统中重用生产者或消费者。例如，关系数据库的连接器可能捕获对表的每个更改。 在Kafka中，客户端和服务器之间的通信是通过简单，高性能，语言无关的TCP协议完成的。此协议已版本化并保持与旧版本的向后兼容性。Kafka提供Java客户端，但客户端有多种语言版本。 1.2 Topics主题 和 partitions分区我们首先深入了解 Kafka 为记录流提供的核心抽象 - 主题topics 一个Topic可以认为是一类消息，每个topic将被分成多个partition(区),每个partition在存储层面是append log文件 主题是发布记录的类别或订阅源名称。Kafka的主题总是多用户; 也就是说，一个主题可以有零个，一个或多个消费者订阅写入它的数据。 对于每个主题，Kafka群集都维护一个如下所示的分区日志： 每个分区都是一个有序的，不可变的记录序列，不断附加到结构化的提交日志中。分区中的记录每个都分配了一个称为偏移的顺序ID号，它唯一地标识分区中的每个记录。 Kafka集群持久保存所有已发布的记录 - 无论是否已使用 - 使用可配置的保留期。例如，如果保留策略设置为两天，则在发布记录后的两天内，它可供使用，之后将被丢弃以释放空间。Kafka的性能在数据大小方面实际上是恒定的，因此长时间存储数据不是问题。 实际上，基于每个消费者保留的唯一元数据是该消费者在日志中的偏移或位置。这种偏移由消费者控制：通常消费者在读取记录时会线性地提高其偏移量，但事实上，由于该位置由消费者控制，因此它可以按照自己喜欢的任何顺序消费记录。例如，消费者可以重置为较旧的偏移量来重新处理过去的数据，或者跳到最近的记录并从“现在”开始消费。 这些功能组合意味着Kafka 消费者consumers 非常cheap - 他们可以来来往往对集群或其他消费者没有太大影响。例如，您可以使用我们的命令行工具“tail”任何主题的内容，而无需更改任何现有使用者所消耗的内容。 日志中的分区有多种用途。首先，它们允许日志扩展到超出适合单个服务器的大小。每个单独的分区必须适合托管它的服务器，但主题可能有许多分区，因此它可以处理任意数量的数据。其次，它们充当了并行性的单位 - 更多的是它。 1.3 Distribution 分配 一个Topic的多个partitions,被分布在kafka集群中的多个server上;每个server(kafka实例)负责partitions中消息的读写操作;此外kafka还可以配置partitions需要备份的个数(replicas),每个partition将会被备份到多台机器上,以提高可用性. 基于replicated方案,那么就意味着需要对多个备份进行调度;每个partition都有一个server为”leader”;leader负责所有的读写操作,如果leader失效,那么将会有其他follower来接管(成为新的leader);follower只是单调的和leader跟进,同步消息即可..由此可见作为leader的server承载了全部的请求压力,因此从集群的整体考虑,有多少个partitions就意味着有多少个”leader”,kafka会将”leader”均衡的分散在每个实例上,来确保整体的性能稳定。 1.4 Producers生产者 和 Consumers消费者1.4.1 Producers生产者 Producers 将数据发布到指定的topics 主题。同时Producer 也能决定将此消息归属于哪个partition;比如基于”round-robin”方式或者通过其他的一些算法等。 1.4.2 Consumers 本质上kafka只支持Topic.每个consumer属于一个consumer group;反过来说,每个group中可以有多个consumer.发送到Topic的消息,只会被订阅此Topic的每个group中的一个consumer消费。 如果所有使用者实例具有相同的使用者组，则记录将有效地在使用者实例上进行负载平衡。 如果所有消费者实例具有不同的消费者组，则每个记录将广播到所有消费者进程。 分析：两个服务器Kafka群集，托管四个分区（P0-P3），包含两个使用者组。消费者组A有两个消费者实例，B组有四个消费者实例。 在Kafka中实现消费consumption 的方式是通过在消费者实例上划分日志中的分区，以便每个实例在任何时间点都是分配的“公平份额”的独占消费者。维护组中成员资格的过程由Kafka协议动态处理。如果新实例加入该组，他们将从该组的其他成员接管一些分区; 如果实例死亡，其分区将分发给其余实例。 Kafka仅提供分区内记录的总订单，而不是主题中不同分区之间的记录。对于大多数应用程序而言，按分区排序与按键分区数据的能力相结合就足够了。但是，如果您需要对记录进行总订单，则可以使用仅包含一个分区的主题来实现，但这将意味着每个使用者组只有一个使用者进程。 1.5 Consumers kafka确保 发送到partitions中的消息将会按照它接收的顺序追加到日志中。也就是说，如果记录M1由与记录M2相同的生成者发送，并且首先发送M1，则M1将具有比M2更低的偏移并且在日志中更早出现。 消费者实例按照它们存储在日志中的顺序查看记录。对于消费者而言,它们消费消息的顺序和日志中消息顺序一致。 如果Topic的”replicationfactor”为N,那么允许N-1个kafka实例失效，我们将容忍最多N-1个服务器故障，而不会丢失任何提交到日志的记录。 1.6 kafka**作为消息系统**Kafka的流概念与传统的企业邮件系统相比如何？ （1）传统消息系统 消息传统上有两种模型：queuing排队 and publish-subscribe发布 - 订阅。在队列中，消费者池可以从服务器读取并且每个记录转到其中一个; 在发布 - 订阅中，记录被广播给所有消费者。这两种模型中的每一种都有优点和缺点。排队的优势在于它允许您在多个消费者实例上划分数据处理，从而可以扩展您的处理。不幸的是，一旦一个进程读取它已经消失的数据，队列就不是多用户。发布 - 订阅允许您将数据广播到多个进程，但由于每条消息都发送给每个订阅者，因此无法进行扩展处理。 kafka的消费者群体概念概括了这两个概念。与队列一样，使用者组允许您将处理划分为一组进程（使用者组的成员）。与发布 - 订阅一样，Kafka允许您向多个消费者组广播消息。 （2）kafka 的优势 Kafka模型的优势在于每个主题都具有这些属性 - 它可以扩展处理并且也是多用户 - 不需要选择其中一个。 与传统的消息系统相比，Kafka具有更强的订购保证。 传统队列在服务器上按顺序保留记录，如果多个消费者从队列中消耗，则服务器按照存储顺序分发记录。但是，虽然服务器按顺序分发记录，但是记录是异步传递给消费者的，因此它们可能会在不同的消费者处出现故障。这实际上意味着在存在并行消耗的情况下丢失记录的顺序。消息传递系统通常通过具有“独占消费者”概念来解决这个问题，该概念只允许一个进程从队列中消耗，但当然这意味着处理中没有并行性。 kafka做得更好。通过在主题中具有并行性概念 - 分区 - ，Kafka能够在消费者流程池中提供订购保证和负载平衡。这是通过将主题中的分区分配给使用者组中的使用者来实现的，以便每个分区仅由该组中的一个使用者使用。通过这样做，我们确保使用者是该分区的唯一读者并按顺序使用数据。由于有许多分区，这仍然可以平衡许多消费者实例的负载。但请注意，消费者组中的消费者实例不能超过分区。 1.7 kafka作为存储系统 任何允许发布与消费消息分离的消息的消息队列实际上充当了正在进行的消息的存储系统。Kafka的不同之处在于它是一个非常好的存储系统。 写入Kafka的数据将写入磁盘并进行复制以实现容错。Kafka允许生产者等待确认，以便在完全复制之前写入不被认为是完整的，并且即使写入的服务器失败也保证写入仍然存在。 磁盘结构Kafka很好地使用了规模 - 无论服务器上有50 KB还是50 TB的持久数据，Kafka都会执行相同的操作。 由于认真对待存储并允许客户端控制其读取位置，您可以将Kafka视为一种专用于高性能，低延迟提交日志存储，复制和传播的专用分布式文件系统。 有关Kafka的提交日志存储和复制设计的详细信息，请阅读此页面。 1.8 kafka用于流处理 仅仅读取，写入和存储数据流是不够的，目的是实现流的实时处理。 在Kafka中，流处理器是指从输入主题获取连续数据流，对此输入执行某些处理以及生成连续数据流以输出主题的任何内容。 例如，零售应用程序可能会接收销售和发货的输入流，并输出重新排序流和根据此数据计算的价格调整。 可以使用生产者和消费者API直接进行简单处理。但是，对于更复杂的转换，Kafka提供了完全集成的Streams API。这允许构建执行非平凡处理的应用程序，这些应用程序可以计算流的聚合或将流连接在一起。 此工具有助于解决此类应用程序面临的难题：处理无序数据，在代码更改时重新处理输入，执行有状态计算等。 流API构建在Kafka提供的核心原语上：它使用生产者和消费者API进行输入，使用Kafka进行有状态存储，并在流处理器实例之间使用相同的组机制来实现容错。 2、kafka使用场景2.1 消息Messaging Kafka可以替代更传统的消息代理。消息代理的使用有多种原因（将处理与数据生成器分离，缓冲未处理的消息等）。与大多数消息传递系统相比，Kafka具有更好的吞吐量，内置分区，复制和容错功能，这使其成为大规模消息处理应用程序的理想解决方案。 根据经验，消息传递的使用通常相对较低，但可能需要较低的端到端延迟，并且通常取决于Kafka提供的强大的耐用性保证。 在这个领域，Kafka可与传统的消息传递系统（如ActiveMQ或 RabbitMQ）相媲美。 2.2 网站活动跟踪 Kafka的原始用例是能够将用户活动跟踪管道重建为一组实时发布 - 订阅源。这意味着站点活动（页面查看，搜索或用户可能采取的其他操作）将发布到中心主题，每个活动类型包含一个主题。这些源可用于订购一系列用例，包括实时处理，实时监控以及加载到Hadoop或离线数据仓库系统以进行脱机处理和报告。 活动跟踪通常非常高，因为为每个用户页面视图生成了许多活动消息。 2.3 度量Metrics Kafka通常用于运营监控数据。这涉及从分布式应用程序聚合统计信息以生成操作数据的集中式提要。 2.4 日志聚合 许多人使用Kafka作为日志聚合解决方案的替代品。日志聚合通常从服务器收集物理日志文件，并将它们放在中央位置（可能是文件服务器或HDFS）进行处理。Kafka抽象出文件的细节，并将日志或事件数据作为消息流更清晰地抽象出来。这允许更低延迟的处理并更容易支持多个数据源和分布式数据消耗。与Scribe或Flume等以日志为中心的系统相比，Kafka提供了同样出色的性能，由于复制而具有更强的耐用性保证，以及更低的端到端延迟。 2.5 流处理 许多Kafka用户在处理由多个阶段组成的管道时处理数据，其中原始输入数据从Kafka主题中消费，然后聚合，丰富或以其他方式转换为新主题以供进一步消费或后续处理。 例如，用于推荐新闻文章的处理管道可以从RSS订阅源抓取文章内容并将其发布到“文章”主题; 进一步处理可能会对此内容进行规范化或重复数据删除，并将已清理的文章内容发布到新主题; 最终处理阶段可能会尝试向用户推荐此内容。此类处理管道基于各个主题创建实时数据流的图形。从0.10.0.0开始，这是一个轻量级但功能强大的流处理库，名为Kafka Streams 在Apache Kafka中可用于执行如上所述的此类数据处理。除了Kafka Streams之外，其他开源流处理工具包括Apache Storm和 Apache Samza。 2.6 Event Sourcing Event Sourcing是一种应用程序设计风格，其中状态更改记录为按时间排序的记录序列。Kafka对非常大的存储日志数据的支持使其成为以这种风格构建的应用程序的出色后端。 2.7 提交日志 Kafka可以作为分布式系统的一种外部提交日志。该日志有助于在节点之间复制数据，并充当故障节点恢复其数据的重新同步机制。Kafka中的日志压缩功能有助于支持此用法。在这种用法中，Kafka类似于Apache BookKeeper项目。 3、kafka安装3.1 下载安装到官网http://kafka.apache.org/downloads.html下载想要的版本；我这里下载的最新稳定版2.1.0 注：由于Kafka控制台脚本对于基于Unix和Windows的平台是不同的，因此在Windows平台上使用bin\\windows\\ 而不是bin/ 将脚本扩展名更改为.bat。 [root@along ~]# wget http://mirrors.shu.edu.cn/apache/kafka/2.1.0/kafka_2.11-2.1.0.tgz [root@along ~]# tar -C /data/ -xvf kafka_2.11-2.1.0.tgz [root@along ~]# cd /data/kafka_2.11-2.1.0/ 3.2 配置启动zookeeper kafka正常运行，必须配置zookeeper，否则无论是kafka集群还是客户端的生存者和消费者都无法正常的工作的；所以需要配置启动zookeeper服务。 （1）zookeeper需要java环境 [root@along ~]# yum -y install java-1.8.0（2）这里kafka下载包已经包括zookeeper服务，所以只需修改配置文件，启动即可。 如果需要下载指定zookeeper版本；可以单独去zookeeper官网http://mirrors.shu.edu.cn/apache/zookeeper/下载指定版本。 [root@along ~]# cd /data/kafka_2.11-2.1.0/ [root@along kafka_2.11-2.1.0]# grep &quot;^[^#]&quot; config/zookeeper.properties dataDir=/tmp/zookeeper #数据存储目录 clientPort=2181 #zookeeper端口 maxClientCnxns=0注：可自行添加修改zookeeper配置 3.3 配置kafka（1）修改配置文件 [root@along kafka_2.11-2.1.0]# grep &quot;^[^#]&quot; config/server.properties broker.id=0 listeners=PLAINTEXT://localhost:9092 num.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 log.dirs=/tmp/kafka-logs num.partitions=1 num.recovery.threads.per.data.dir=1 offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connect=localhost:2181 zookeeper.connection.timeout.ms=6000 group.initial.rebalance.delay.ms=0注：可根据自己需求修改配置文件 broker.id：唯一标识ID listeners=PLAINTEXT://localhost:9092：kafka服务监听地址和端口 log.dirs：日志存储目录 zookeeper.connect：指定zookeeper服务 （2）配置环境变量 [root@along ~]# vim /etc/profile.d/kafka.sh export KAFKA_HOME=&quot;/data/kafka_2.11-2.1.0&quot; export PATH=&quot;${KAFKA_HOME}/bin:$PATH&quot; [root@along ~]# source /etc/profile.d/kafka.sh （3）配置服务启动脚本 [root@along ~]# vim /etc/init.d/kafka #!/bin/sh # # chkconfig: 345 99 01 # description: Kafka # # File : Kafka # # Description: Starts and stops the Kafka server # source /etc/rc.d/init.d/functions KAFKA_HOME=/data/kafka_2.11-2.1.0 KAFKA_USER=root export LOG_DIR=/tmp/kafka-logs [ -e /etc/sysconfig/kafka ] && . /etc/sysconfig/kafka # See how we were called. case \"$1\" in start) echo -n \"Starting Kafka:\" /sbin/runuser -s /bin/sh $KAFKA_USER -c \"nohup $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties > $LOG_DIR/server.out 2> $LOG_DIR/server.err &\" echo \" done.\" exit 0 ;; stop) echo -n \"Stopping Kafka: \" /sbin/runuser -s /bin/sh $KAFKA_USER -c \"ps -ef | grep kafka.Kafka | grep -v grep | awk '{print \\$2}' | xargs kill\" echo \" done.\" exit 0 ;; hardstop) echo -n \"Stopping (hard) Kafka: \" /sbin/runuser -s /bin/sh $KAFKA_USER -c \"ps -ef | grep kafka.Kafka | grep -v grep | awk '{print \\$2}' | xargs kill -9\" echo \" done.\" exit 0 ;; status) c_pid=`ps -ef | grep kafka.Kafka | grep -v grep | awk '{print $2}'` if [ \"$c_pid\" = \"\" ] ; then echo \"Stopped\" exit 3 else echo \"Running $c_pid\" exit 0 fi ;; restart) stop start ;; *) echo \"Usage: kafka {start|stop|hardstop|status|restart}\" exit 1 ;; esac chmod +x kafka ​ chkconfig –add kafka 添加到服务器中 ​ chkconfig kafka on 设置开机自动启动 3.4 启动kafka服务（1）后台启动zookeeper服务 [root@along ~]# nohup zookeeper-server-start.sh /data/kafka_2.11-2.1.0/config/zookeeper.properties & （2）启动kafka服务 [root@along ~]# service kafka start Starting kafka (via systemctl): [ OK ][root@along ~]# service kafka status Running 86018 [root@along ~]# ss -nutl Netid State Recv-Q Send-Q Local Address:Port Peer Address:Port tcp LISTEN 0 50 :::9092 :::* tcp LISTEN 0 50 :::2181 :::* 4、kafka使用简单入门4.1 创建主题topics创建一个名为“along”的主题，它只包含一个分区，只有一个副本： [root@along ~]# kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic along Created topic \"along\". 如果我们运行list topic命令，我们现在可以看到该主题： [root@along ~]# kafka-topics.sh --list --zookeeper localhost:2181 along 4.2 发送一些消息Kafka附带一个命令行客户端，它将从文件或标准输入中获取输入，并将其作为消息发送到Kafka集群。默认情况下，每行将作为单独的消息发送。 运行生产者，然后在控制台中键入一些消息以发送到服务器。 [root@along ~]# kafka-console-producer.sh --broker-list localhost:9092 --topic along &gt;This is a message &gt;This is another message 4.3 启动消费者Kafka还有一个命令行使用者，它会将消息转储到标准输出。 [root@along ~]# kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic along --from-beginning This is a message This is another message所有命令行工具都有其他选项; 运行不带参数的命令将显示更详细地记录它们的使用信息。 5、设置多代理kafka群集 到目前为止，我们一直在与一个broker运行，但这并不好玩。对于Kafka，单个代理只是一个大小为1的集群，因此除了启动一些代理实例之外没有太多变化。但是为了感受它，让我们将我们的集群扩展到三个节点（仍然在我们的本地机器上）。 5.1 准备配置文件[root@along kafka_2.11-2.1.0]# cd /data/kafka_2.11-2.1.0/ [root@along kafka_2.11-2.1.0]# cp config/server.properties config/server-1.properties [root@along kafka_2.11-2.1.0]# cp config/server.properties config/server-2.properties [root@along kafka_2.11-2.1.0]# vim config/server-1.properties broker.id=1 listeners=PLAINTEXT://:9093 log.dirs=/tmp/kafka-logs-1 [root@along kafka_2.11-2.1.0]# vim config/server-2.properties broker.id=2 listeners=PLAINTEXT://:9094 log.dirs=/tmp/kafka-logs-2 注：该broker.id 属性是群集中每个节点的唯一且永久的名称。我们必须覆盖端口和日志目录，因为我们在同一台机器上运行这些，并且我们希望让所有代理尝试在同一端口上注册或覆盖彼此的数据。 5.2 开启集群另2个kafka服务[root@along ~]# nohup kafka-server-start.sh /data/kafka_2.11-2.1.0/config/server-1.properties &amp; [root@along ~]# nohup kafka-server-start.sh /data/kafka_2.11-2.1.0/config/server-2.properties &amp; [root@along ~]# ss -nutl Netid State Recv-Q Send-Q Local Address:Port Peer Address:Port tcp LISTEN 0 50 ::ffff:127.0.0.1:9092 :::* tcp LISTEN 0 50 ::ffff:127.0.0.1:9093 :::* tcp LISTEN 0 50 ::ffff:127.0.0.1:9094 :::* 5.3 在集群中进行操作（1）现在创建一个复制因子为3的新主题my-replicated-topic [root@along ~]# kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic Created topic &quot;my-replicated-topic&quot;. （2）在一个集群中，运行“describe topics”命令查看哪个broker正在做什么 [root@along ~]# kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic Topic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 2 Replicas: 2,0,1 Isr: 2,0,1注释：第一行给出了所有分区的摘要，每个附加行提供有关一个分区的信息。由于我们只有一个分区用于此主题，因此只有一行。 “leader”是负责给定分区的所有读取和写入的节点。每个节点将成为随机选择的分区部分的领导者。 “replicas”是复制此分区日志的节点列表，无论它们是否为领导者，或者即使它们当前处于活动状态。 “isr”是“同步”复制品的集合。这是副本列表的子集，该列表当前处于活跃状态并且已经被领导者捕获。 请注意，Leader: 2，在我的示例中，节点2 是该主题的唯一分区的Leader。 （3）可以在我们创建的原始主题上运行相同的命令，以查看它的位置 [root@along ~]# kafka-topics.sh --describe --zookeeper localhost:2181 --topic along Topic:along PartitionCount:1 ReplicationFactor:1 Configs: Topic: along Partition: 0 Leader: 0 Replicas: 0 Isr: 0 （4）向我们的新主题发布一些消息： [root@along ~]# kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic &gt;my test message 1 &gt;my test message 2 &gt;^C （5）现在让我们使用这些消息： [root@along ~]# kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic my test message 1 my test message 2 5.4 测试集群的容错性（1）现在让我们测试一下容错性。Broker 2 充当leader 所以让我们杀了它： [root@along ~]# ps aux | grep server-2.properties |awk &#39;{print $2}&#39; 106737 [root@along ~]# kill -9 106737 [root@along ~]# ss -nutl tcp LISTEN 0 50 ::ffff:127.0.0.1:9092 :::* tcp LISTEN 0 50 ::ffff:127.0.0.1:9093 :::* （2）leader 已切换到其中一个从属节点，节点2不再位于同步副本集中： [root@along ~]# kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic Topic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 0 Replicas: 2,0,1 Isr: 0,1 （3）即使最初接受写入的leader 已经失败，这些消息仍可供消费： [root@along ~]# kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic my test message 1 my test message 2 6、**使用Kafka Connect导入/导出数据** 从控制台写入数据并将其写回控制台是一个方便的起点，但有时候可能希望使用其他来源的数据或将数据从Kafka导出到其他系统。对于许多系统，您可以使用Kafka Connect导入或导出数据，而不是编写自定义集成代码。 Kafka Connect是Kafka附带的工具，用于向Kafka导入和导出数据。它是一个可扩展的工具，运行连接器，实现与外部系统交互的自定义逻辑。在本快速入门中，我们将了解如何使用简单的连接器运行Kafka Connect，这些连接器将数据从文件导入Kafka主题并将数据从Kafka主题导出到文件。 （1）首先创建一些种子数据进行测试： [root@along ~]# echo -e &quot;foo\\nbar&quot; &gt; test.txt或者在Windows上： &gt; echo foo&gt; test.txt&gt; echo bar&gt;&gt; test.txt （2）接下来，启动两个以独立模式运行的连接器，这意味着它们在单个本地专用进程中运行。提供三个配置文件作为参数。 第一个始终是Kafka Connect流程的配置，包含常见配置，例如要连接的Kafka代理和数据的序列化格式。 其余配置文件均指定要创建的连接器。这些文件包括唯一的连接器名称，要实例化的连接器类以及连接器所需的任何其他配置。 [root@along ~]# connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties [2019-01-16 16:16:31,884] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:67) [2019-01-16 16:16:31,903] INFO WorkerInfo values: ... ... 注：Kafka附带的这些示例配置文件使用您之前启动的默认本地群集配置并创建两个连接器：第一个是源连接器，它从输入文件读取行并生成每个Kafka主题，第二个是宿连接器从Kafka主题读取消息并将每个消息生成为输出文件中的一行。 （3）验证是否导入成功（另起终端） 在启动过程中，您将看到许多日志消息，包括一些指示正在实例化连接器的日志消息。 ① 一旦Kafka Connect进程启动，源连接器应该开始从test.txt主题读取行并将其生成到主题connect-test，并且接收器连接器应该开始从主题读取消息connect-test 并将它们写入文件test.sink.txt。我们可以通过检查输出文件的内容来验证数据是否已通过整个管道传递： [root@along ~]# cat test.sink.txt foo bar ② 请注意，数据存储在Kafka主题中connect-test，因此我们还可以运行控制台使用者来查看主题中的数据（或使用自定义使用者代码来处理它）： [root@along ~]# kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning {&quot;schema&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false},&quot;payload&quot;:&quot;foo&quot;} {&quot;schema&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false},&quot;payload&quot;:&quot;bar&quot;} （4）继续追加数据，验证 [root@along ~]# echo Another line&gt;&gt; test.txt [root@along ~]# cat test.sink.txt foo bar Another line [root@along ~]# kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning {&quot;schema&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false},&quot;payload&quot;:&quot;foo&quot;} {&quot;schema&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false},&quot;payload&quot;:&quot;bar&quot;} {&quot;schema&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false},&quot;payload&quot;:&quot;Another line&quot;}","categories":[{"name":"SQL","slug":"SQL","permalink":"https://cyylog.github.io/categories/SQL/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://cyylog.github.io/tags/kafka/"}]},{"title":"kafka集群搭建","slug":"SQL/kafka集群搭建","date":"2019-04-25T16:09:22.000Z","updated":"2020-05-25T13:55:30.117Z","comments":true,"path":"2019/04/26/sql/kafka-ji-qun-da-jian/","link":"","permalink":"https://cyylog.github.io/2019/04/26/sql/kafka-ji-qun-da-jian/","excerpt":"","text":"常用Message Queue对比RabbitMQ RabbitMQ是使用Erlang编写的一个开源的消息队列，本身支持很多的协议：AMQP，XMPP, SMTP, STOMP，也正因如此，它非常重量级，更适合于企业级的开发。同时实现了Broker构架，这意味着消息在发送给客户端时先在中心队列排队。对路由，负载均衡或者数据持久化都有很好的支持。Redis Redis是一个基于Key-Value对的NoSQL数据库，开发维护很活跃。虽然它是一个Key-Value数据库存储系统，但它本身支持MQ功能，所以完全可以当做一个轻量级的队列服务来使用。对于RabbitMQ和Redis的入队和出队操作，各执行100万次，每10万次记录一次执行时间。测试数据分为128Bytes、512Bytes、1K和10K四个不同大小的数据。实验表明：入队时，当数据比较小时Redis的性能要高于RabbitMQ，而如果数据大小超过了10K，Redis则慢的无法忍受；出队时，无论数据大小，Redis都表现出非常好的性能，而RabbitMQ的出队性能则远低于Redis。ZeroMQ ZeroMQ号称最快的消息队列系统，尤其针对大吞吐量的需求场景。ZeroMQ能够实现RabbitMQ不擅长的高级/复杂的队列，但是开发人员需要自己组合多种技术框架，技术上的复杂度是对这MQ能够应用成功的挑战。ZeroMQ具有一个独特的非中间件的模式，你不需要安装和运行一个消息服务器或中间件，因为你的应用程序将扮演这个服务器角色。你只需要简单的引用ZeroMQ程序库，可以使用NuGet安装，然后你就可以愉快的在应用程序之间发送消息了。但是ZeroMQ仅提供非持久性的队列，也就是说如果宕机，数据将会丢失。其中，Twitter的Storm 0.9.0以前的版本中默认使用ZeroMQ作为数据流的传输（Storm从0.9版本开始同时支持ZeroMQ和Netty作为传输模块）。ActiveMQ ActiveMQ是Apache下的一个子项目。 类似于ZeroMQ，它能够以代理人和点对点的技术实现队列。同时类似于RabbitMQ，它少量代码就可以高效地实现高级应用场景。Kafka/Jafka Kafka是Apache下的一个子项目，是一个高性能跨语言分布式发布/订阅消息队列系统，而Jafka是在Kafka之上孵化而来的，即Kafka的一个升级版。具有以下特性：快速持久化，可以在O(1)的系统开销下进行消息持久化；高吞吐，在一台普通的服务器上既可以达到10W/s的吞吐速率；完全的分布式系统，Broker、Producer、Consumer都原生自动支持分布式，自动实现负载均衡；支持Hadoop数据并行加载，对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka通过Hadoop的并行加载机制统一了在线和离线的消息处理。Apache Kafka相对于ActiveMQ是一个非常轻量级的消息系统，除了性能非常好之外，还是一个工作良好的分布式系统相关概念producer： 消息生产者，发布消息到 kafka 集群的终端或服务。broker： kafka 集群中包含的服务器。topic：每条发布到 kafka 集群的消息属于的类别，即 kafka 是面向 topic 的。partition： partition 是物理上的概念，每个 topic 包含一个或多个 partition。kafka 分配的单位是 partition。consumer： 从 kafka 集群中消费消息的终端或服务。Consumer group： high-level consumer API 中，每个 consumer 都属于一个 consumer group，每条消息只能被 consumer group 中的一个 Consumer 消费，但可以被多个 consumer group 消费。replica： partition 的副本，保障 partition 的高可用。leader： replica 中的一个角色， producer 和 consumer 只跟 leader 交互。follower： replica 中的一个角色，从 leader 中复制数据。controller： kafka 集群中的其中一个服务器，用来进行 leader election 以及 各种 failover。zookeeper： kafka 通过 zookeeper 来存储集群的 meta 信息 单实例Kafka前提条件：安装JDK、设置JAVA_HOME、PATH环境变量。 wget http://mirrors.hust.edu.cn/apache/kafka/1.1.0/kafka_2.12-1.1.0.tgz (1) Terminal A[root@sdopenswan-jp ~]# ls configserver.sh kafka_2.12-0.10.2.1.tgz [root@sdopenswan-jp ~]# tar xfz kafka_2.12-0.10.2.1.tgz [root@sdopenswan-jp ~]# ls configserver.sh kafka_2.12-0.10.2.1 kafka_2.12-0.10.2.1.tgz [root@sdopenswan-jp ~]# cd kafka_2.12-0.10.2.1 [root@sdopenswan-jp kafka_2.12-0.10.2.1]# vim config/zookeeper.properties [root@sdopenswan-jp kafka_2.12-0.10.2.1]# grep -Pv \"^#\" config/zookeeper.properties dataDir=zkdata1 clientPort=2181 maxClientCnxns=0 [root@sdopenswan-jp kafka_2.12-0.10.2.1]# bin/zookeeper-server-start.sh config/zookeeper.properties & #启动zookeeper [root@sdopenswan-jp kafka_2.12-0.10.2.1]# ls bin config libs LICENSE logs NOTICE site-docs zkdata1 [root@sdopenswan-jp kafka_2.12-0.10.2.1]# ls zkdata1/ version-2 [root@sdopenswan-jp kafka_2.12-0.10.2.1]# vim config/server.properties [root@sdopenswan-jp kafka_2.12-0.10.2.1]# grep -Pv \"^($|#)\" config/server.properties broker.id=0 listeners=PLAINTEXT://:9092 num.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 log.dirs=kfkdata1 num.partitions=1 num.recovery.threads.per.data.dir=1 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connect=localhost:2181 zookeeper.connection.timeout.ms=6000 [root@sdopenswan-jp kafka_2.12-0.10.2.1]# bin/kafka-server-start.sh config/server.properties [root@sdopenswan-jp kafka_2.12-0.10.2.1]# ls bin config kfkdata1 libs LICENSE logs NOTICE site-docs zkdata1 [root@sdopenswan-jp kafka_2.12-0.10.2.1]# ls kfkdata1/ cleaner-offset-checkpoint meta.properties recovery-point-offset-checkpoint replication-offset-checkpoint [root@sdopenswan-jp kafka_2.12-0.10.2.1]# jps 3538 Jps 2964 QuorumPeerMain 3214 Kafka [root@sdopenswan-jp kafka_2.12-0.10.2.1]# Kafka 占tcp 9092 端口，而zookeeper占 tcp 2181端口 (2) Terminal B 创建 一个topic[root@sdopenswan-jp kafka_2.12-0.10.2.1]# bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partition 1 --topic myfirst-topic WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both. Created topic \"myfirst_topic\". [root@sdopenswan-jp kafka_2.12-0.10.2.1]# ls kfkdata1/ cleaner-offset-checkpoint meta.properties myfirst_topic-0 recovery-point-offset-checkpoint replication-offset-checkpoint [root@sdopenswan-jp kafka_2.12-0.10.2.1]# ls kfkdata1/myfirst-topic-0/ 00000000000000000000.index 00000000000000000000.log 00000000000000000000.timeindex [root@sdopenswan-jp kafka_2.12-0.10.2.1]# [root@sdopenswan-jp kafka_2.12-0.10.2.1]# bin/kafka-topics.sh --list --zookeeper localhost:2181 myfirst-topic [root@sdopenswan-jp kafka_2.12-0.10.2.1]# bin/kafka-console-producer.sh --topic myfirst-topic --broker-list localhost:9092 hello world ! hello kafka myfirst_topic (3) Terminal C # consumer端连接到zookeeper 读取信息[root@sdopenswan-jp kafka_2.12-0.10.2.1]# bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic myfirst_topic --from-beginning Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper]. hello world ! hello kafka myfirst_topic 配置文件说明Zookeeper配置1) 在zoo.cfg中追加以下内容： server.n=ip:portA:portB #n是服务器标识号（1~255） #ip是服务器ip地址 #portA是与leader进行信息交换的端口 #portB是在leader宕机后，进行leader选举所用的端口 例： server.1=200.31.157.116:20881:30881 server.2=200.31.157.116:20882:30882 server.3=200.31.157.117:20881:30881 tickTime：毫秒级的基本时间单位，其他时间如心跳/超时等都为该单位时间的整数倍。 initLimit：tickTime的倍数，表示leader选举结束后，followers与leader同步需要的时间，leader的数据非常多或followers比较多时，该值应适当大一些。 syncLimit：tickTime的倍数，表示follower和observer与leader交互时的最大等待时间，是在与leader同步完毕之后，正常请求转发或ping等消息交互时的超时时间。 clientPort：监听客户端连接的服务端口，若一台服务器上安装多个ZooKeeper server，则需要设置不同的端口号。 dataDir：内存数据库快照地址，事务日志地址（除非由dataLogDir另行指定）。 2) 在$dataDir下新建文件myid，并写入服务器标识号 #/tmp/zookeeper为dataDir cd /tmp/zookeeper/ vim myid #在myid中添加服务器标识号 Kafka配置在配置文件server.properties修改如下内容： #broker.id是broker的标识，具有唯一性 broker.id=0 #端口号默认为9092 port=9092 #host.name位kafka所在机器的ip host.name=10.18.42.251 #设置zookeeper，可连接多个zookeeper服务器 zookeeper.connect=200.31.157.116:2182,200.31.157.116:2183,200.31.157.117:2182 多实例Kafka(1)配置并启动zookeeper[root@sdopenswan-jp kafka_2.12-0.10.2.1]# vim config/zookeeper.properties [root@sdopenswan-jp kafka_2.12-0.10.2.1]# grep -Pv \"^#\" config/zookeeper.properties dataDir=zkdata1 clientPort=2181 maxClientCnxns=0 tickTime=2000 initLimit=5 syncLimit=2 server.0=172.16.1.16:2888:3888 server.1=172.16.2.59:2888:3888 server.2=172.16.0.198:2888:3888 [root@sdopenswan-jp kafka_2.12-0.10.2.1]# echo 0 > zkdata1/myid [root@sdredis01-jp kafka_2.12-0.10.2.1]# vim config/zookeeper.properties [root@sdredis01-jp kafka_2.12-0.10.2.1]# grep -Pv \"^#\" config/zookeeper.properties dataDir=zkdata1 clientPort=2181 maxClientCnxns=0 tickTime=2000 initLimit=5 syncLimit=2 server.0=172.16.1.16:2888:3888 server.1=172.16.2.59:2888:3888 server.2=172.16.0.198:2888:3888 [root@sdredis01-jp kafka_2.12-0.10.2.1]# ls bin config libs LICENSE NOTICE site-docs [root@sdredis01-jp kafka_2.12-0.10.2.1]# mkdir zkdata1 [root@sdredis01-jp kafka_2.12-0.10.2.1]# echo 1 > zkdata1/myid [root@sdredis01-jp kafka_2.12-0.10.2.1]# [root@sdredis02-jp kafka_2.12-0.10.2.1]# vim config/zookeeper.properties [root@sdredis02-jp kafka_2.12-0.10.2.1]# grep -Pv \"^#\" config/zookeeper.properties dataDir=zkdata1 clientPort=2181 maxClientCnxns=0 tickTime=2000 initLimit=5 syncLimit=2 server.0=172.16.1.16:2888:3888 server.1=172.16.2.59:2888:3888 server.2=172.16.0.198:2888:3888 [root@sdredis02-jp kafka_2.12-0.10.2.1]# mkdir zkdata1 [root@sdredis02-jp kafka_2.12-0.10.2.1]# echo 2 > zkdata1/myid [root@sdredis02-jp kafka_2.12-0.10.2.1]# [root@sdopenswan-jp kafka_2.12-0.10.2.1]# bin/zookeeper-server-start.sh config/zookeeper.properties & [root@sdredis01-jp kafka_2.12-0.10.2.1]# bin/zookeeper-server-start.sh config/zookeeper.properties & [root@sdredis02-jp kafka_2.12-0.10.2.1]# bin/zookeeper-server-start.sh config/zookeeper.properties & (2)配置并启动kafka[root@sdopenswan-jp kafka_2.12-0.10.2.1]# vim config/server.properties [root@sdopenswan-jp kafka_2.12-0.10.2.1]# grep -Pv \"^($|#)\" config/server.properties broker.id=0 listeners=PLAINTEXT://:9092 num.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 log.dirs=kfklog num.partitions=3 num.recovery.threads.per.data.dir=1 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connect=172.16.1.16:2181,172.16.2.59:2181,172.16.0.198:2181 zookeeper.connection.timeout.ms=6000 [root@sdopenswan-jp kafka_2.12-0.10.2.1]# vim config/consumer.properties [root@sdopenswan-jp kafka_2.12-0.10.2.1]# grep -Pv \"^($|#)\" config/consumer.properties zookeeper.connect=172.16.1.16:2181,172.16.2.59:2181,172.16.0.198:2181 zookeeper.connection.timeout.ms=6000 group.id=test-consumer-group [root@sdopenswan-jp kafka_2.12-0.10.2.1]# vim config/producer.properties [root@sdopenswan-jp kafka_2.12-0.10.2.1]# grep -Pv \"^($|#)\" config/producer.properties bootstrap.servers=172.16.1.16:9092,172.16.2.59:9092,172.16.0.198:9092 compression.type=none [root@sdopenswan-jp kafka_2.12-0.10.2.1]# [root@sdredis01-jp kafka_2.12-0.10.2.1]# vim config/server.properties [root@sdredis01-jp kafka_2.12-0.10.2.1]# grep -Pv \"^($|#)\" config/server.properties broker.id=1 listeners=PLAINTEXT://:9092 num.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 log.dirs=kfklog num.partitions=3 num.recovery.threads.per.data.dir=1 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connect=172.16.1.16:2181,172.16.2.59:2181,172.16.0.198:2181 zookeeper.connection.timeout.ms=6000 [root@sdredis01-jp kafka_2.12-0.10.2.1]# vim config/consumer.properties [root@sdredis01-jp kafka_2.12-0.10.2.1]# grep -Pv \"^($|#)\" config/consumer.properties zookeeper.connect=172.16.1.16:2181,172.16.2.59:2181,172.16.0.198:2181 zookeeper.connection.timeout.ms=6000 group.id=test-consumer-group [root@sdredis01-jp kafka_2.12-0.10.2.1]# vim config/producer.properties [root@sdredis01-jp kafka_2.12-0.10.2.1]# grep -Pv \"^($|#)\" config/producer.properties bootstrap.servers=172.16.1.16:9092,172.16.2.59:9092,172.16.0.198:9092 compression.type=none [root@sdredis01-jp kafka_2.12-0.10.2.1]# [root@sdredis02-jp kafka_2.12-0.10.2.1]# vim config/server.properties [root@sdredis02-jp kafka_2.12-0.10.2.1]# grep -Pv \"^($|#)\" config/server.properties broker.id=2 listeners=PLAINTEXT://:9092 num.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 log.dirs=kfklog num.partitions=3 num.recovery.threads.per.data.dir=1 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connect=172.16.1.16:2181,172.16.2.59:2181,172.16.0.198:2181 zookeeper.connection.timeout.ms=6000 [root@sdredis02-jp kafka_2.12-0.10.2.1]# vim config/consumer.properties [root@sdredis02-jp kafka_2.12-0.10.2.1]# grep -Pv \"^($|#)\" config/consumer.properties zookeeper.connect=172.16.1.16:2181,172.16.2.59:2181,172.16.0.198:2181 zookeeper.connection.timeout.ms=6000 group.id=test-consumer-group [root@sdredis02-jp kafka_2.12-0.10.2.1]# vim config/producer.properties [root@sdredis02-jp kafka_2.12-0.10.2.1]# grep -Pv \"^($|#)\" config/producer.properties bootstrap.servers=172.16.1.16:9092,172.16.2.59:9092,172.16.0.198:9092 compression.type=none [root@sdredis02-jp kafka_2.12-0.10.2.1]# 三台机器分别启动 bin/kafka-server-start.sh config/server.properties &amp; (3)测试[root@sdredis02-jp kafka_2.12-0.10.2.1]# bin/kafka-topics.sh --create --zookeeper 172.16.0.198:2181 --replication-factor 3 --partitions 1 --topic yc01_topic WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both. Created topic \"yc01_topic\". [root@sdredis02-jp kafka_2.12-0.10.2.1]# bin/kafka-topics.sh --list --zookeeper 172.16.0.198:2181 myfirst_topic yc01_topic [root@sdredis02-jp kafka_2.12-0.10.2.1]# bin/kafka-topics.sh --describe --zookeeper 172.16.0.198:2181 Topic:myfirst_topic PartitionCount:1 ReplicationFactor:1 Configs: Topic: myfirst_topic Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Topic:yc01_topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: yc01_topic Partition: 0 Leader: 1 Replicas: 1,2,0 Isr: 1,2,0 [root@sdredis02-jp kafka_2.12-0.10.2.1]# [root@sdredis01-jp kafka_2.12-0.10.2.1]# bin/kafka-console-producer.sh --broker-list 172.16.1.16:9092,172.16.2.59:9092,172.16.0.198:9092 --topic yc01_topic hello multi instance kafka [root@sdredis02-jp kafka_2.12-0.10.2.1]# bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic yc01_topic --from-beginning Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper]. hello multi instance kafka (4)查看zookeeper状态[root@sdopenswan-jp kafka_2.12-0.10.2.1]# echo stat|nc 127.0.0.1 2181 Zookeeper version: 3.4.9-1757313, built on 08/23/2016 06:50 GMT Clients: /127.0.0.1:46010[0](queued=0,recved=1,sent=0) /172.16.0.198:53940[1](queued=0,recved=875,sent=875) Latency min/avg/max: 0/0/4 Received: 890 Sent: 889 Connections: 2 Outstanding: 0 Zxid: 0x100000060 Mode: leader Node count: 33 [root@sdopenswan-jp kafka_2.12-0.10.2.1]# [root@sdredis01-jp kafka_2.12-0.10.2.1]# echo stat|nc 127.0.0.1 2181 Zookeeper version: 3.4.9-1757313, built on 08/23/2016 06:50 GMT Clients: /127.0.0.1:53466[0](queued=0,recved=1,sent=0) Latency min/avg/max: 0/0/0 Received: 2 Sent: 1 Connections: 1 Outstanding: 0 Zxid: 0x10000005d Mode: follower Node count: 33 [root@sdredis02-jp kafka_2.12-0.10.2.1]# echo stat|nc 127.0.0.1 2181 Zookeeper version: 3.4.9-1757313, built on 08/23/2016 06:50 GMT Clients: /172.16.2.59:53354[1](queued=0,recved=952,sent=952) /172.16.1.16:53038[1](queued=0,recved=1018,sent=1023) /127.0.0.1:49478[0](queued=0,recved=1,sent=0) Latency min/avg/max: 0/0/15 Received: 2566 Sent: 2572 Connections: 3 Outstanding: 0 Zxid: 0x100000060 Mode: follower Node count: 33 [root@sdredis02-jp kafka_2.12-0.10.2.1]# (5)常用命令总结bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 2 --partitions 4 --topic test bin/kafka-topics.sh --describe --zookeeper bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test --producer.config config/producer.properties bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --new-consumer --from-beginning --consumer.config config/consumer.properties bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --list bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zkconnect localhost:2181 --group test bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --describe --group test-consumer-group bin/kafka-preferred-replica-election.sh --zookeeper zk_host:port/chroot bin/kafka-producer-perf-test.sh --topic test --num-records 100 --record-size 1 --throughput 100 --producer-props bootstrap.servers=localhost:9092 用户在客户端可以通过 telnet 或 nc 向 ZooKeeper 提交相应的命令 可以通过命令：echo stat|nc 127.0.0.1 2181 来查看哪个节点被选择作为follower或者leader 使用echo ruok|nc 127.0.0.1 2181 测试是否启动了该Server，若回复imok表示已经启动。 echo dump| nc 127.0.0.1 2181 ,列出未经处理的会话和临时节点。 echo kill | nc 127.0.0.1 2181 ,关掉server echo conf | nc 127.0.0.1 2181 ,输出相关服务配置的详细信息。 echo cons | nc 127.0.0.1 2181 ,列出所有连接到服务器的客户端的完全的连接 / 会话的详细信息。 echo envi |nc 127.0.0.1 2181 ,输出关于服务环境的详细信息（区别于 conf 命令）。 echo reqs | nc 127.0.0.1 2181 ,列出未经处理的请求。 echo wchs | nc 127.0.0.1 2181 ,列出服务器 watch 的详细信息。 echo wchc | nc 127.0.0.1 2181 ,通过 session 列出服务器 watch 的详细信息，它的输出是一个与 watch 相关的会话的列表。 echo wchp | nc 127.0.0.1 2181 ,通过路径列出服务器 watch 的详细信息。它输出一个与 session 相关的路径 补充新实验过程(参考)teacher配置如下： [root@teacher ~]# vim /etc/profile [root@teacher ~]# source /etc/profile [root@teacher ~]# echo $JAVA_HOME /usr/java/latest [root@teacher ~]# which java /usr/java/latest/bin/java [root@teacher ~]# java -version java version \"1.8.0_162\" Java(TM) SE Runtime Environment (build 1.8.0_162-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.162-b12, mixed mode) [root@teacher ~]# [root@teacher ~]# scp /etc/profile node2:/etc/ profile 100% 1859 549.7KB/s 00:00 [root@teacher ~]# scp /etc/profile node3:/etc/ profile 100% 1859 448.7KB/s 00:00 [root@teacher ~]# [root@teacher ~]# tar xf kafka_2.12-1.1.0.tgz -C /usr/local/ [root@teacher opt]# cd /usr/local/ [root@teacher local]# ln -s kafka_2.12-1.1.0/ kafka [root@teacher local]# ls bin etc games include kafka kafka_2.12-1.1.0 lib lib64 libexec logstash php sbin share src zabbix [root@teacher local]# cd kafka [root@teacher kafka]# mkdir zkdata [root@teacher kafka]# mkdir kfklog [root@teacher kafka]# echo 0 > zkdata/myid [root@teacher kafka]# vim config/zookeeper.properties [root@teacher kafka]# grep -Pv \"^(#|$)\" config/zookeeper.properties dataDir=zkdata clientPort=2181 maxClientCnxns=0 tickTime=2000 initLimit=5 syncLimit=2 server.0=192.168.233.102:2888:3888 server.1=192.168.233.103:2888:3888 server.2=192.168.233.104:2888:3888 [root@teacher kafka]# vim config/server.properties [root@teacher kafka]# grep -Pv \"^(#|$)\" config/server.properties broker.id=0 listeners=PLAINTEXT://:9092 advertised.listeners=PLAINTEXT://192.168.233.102:9092 num.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 log.dirs=kfklog num.partitions=3 num.recovery.threads.per.data.dir=1 offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connect=192.168.233.102:2181,192.168.233.103:2181,192.168.233.104:2181 zookeeper.connection.timeout.ms=6000 group.initial.rebalance.delay.ms=0 [root@teacher kafka]# vim config/consumer.properties [root@teacher kafka]# grep -Pv \"^(#|$)\" config/consumer.properties bootstrap.servers=192.168.233.102:9092,192.168.233.103:9092,192.168.233.104:9092 zookeeper.connect=192.168.233.102:2181,192.168.233.103:2181,192.168.233.104:2181 group.id=test-consumer-group [root@teacher kafka]# vim config/producer.properties [root@teacher kafka]# grep -Pv \"^(#|$)\" config/producer.properties bootstrap.servers=192.168.233.102:9092,192.168.233.103:9092,192.168.233.104:9092 compression.type=none [root@teacher ~]# cd /usr/local/ [root@teacher local]# scp -r kafka kafka/ kafka_2.12-1.1.0/ [root@teacher local]# scp -r kafka_2.12-1.1.0 node2:/usr/local/ node2配置如下： [root@node2 ~]# java -version java version \"1.8.0_162\" Java(TM) SE Runtime Environment (build 1.8.0_162-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.162-b12, mixed mode) [root@node2 ~]# source /etc/profile [root@node2 ~]# echo $JAVA_HOME /usr/java/latest [root@node2 ~]# which java /usr/java/latest/bin/java [root@node2 ~]# tail -3 /etc/hosts 192.168.233.102 teacher 192.168.233.103 node2 192.168.233.104 node3 [root@node2 ~]# cd /usr/local/ [root@node2 local]# ln -s kafka_2.12-1.1.0 kafka [root@node2 local]# cd kafka [root@node2 kafka]# echo 1 > zkdata/myid [root@node2 kafka]# vim config/server.properties [root@node2 kafka]# vim config/consumer.properties [root@node2 kafka]# vim config/producer.properties [root@node2 kafka]# grep -Pv \"^(#|$)\" config/zookeeper.properties dataDir=zkdata clientPort=2181 maxClientCnxns=0 tickTime=2000 initLimit=5 syncLimit=2 server.0=192.168.233.102:2888:3888 server.1=192.168.233.103:2888:3888 server.2=192.168.233.104:2888:3888 [root@node2 kafka]# grep -Pv \"^(#|$)\" config/server.properties broker.id=1 listeners=PLAINTEXT://:9092 advertised.listeners=PLAINTEXT://192.168.233.103:9092 num.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 log.dirs=kfklog num.partitions=3 num.recovery.threads.per.data.dir=1 offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connect=192.168.233.102:2181,192.168.233.103:2181,192.168.233.104:2181 zookeeper.connection.timeout.ms=6000 group.initial.rebalance.delay.ms=0 [root@node2 kafka]# grep -Pv \"^(#|$)\" config/consumer.properties bootstrap.servers=192.168.233.102:9092,192.168.233.103:9092,192.168.233.104:9092 zookeeper.connect=192.168.233.102:2181,192.168.233.103:2181,192.168.233.104:2181 group.id=test-consumer-group [root@node2 kafka]# grep -Pv \"^(#|$)\" config/producer.properties bootstrap.servers=192.168.233.102:9092,192.168.233.103:9092,192.168.233.104:9092 compression.type=none [root@node2 kafka]# node3配置如下： [root@node3 ~]# cd /usr/local/ [root@node3 local]# ln -s kafka_2.12-1.1.0 kafka [root@node3 local]# ls bin etc games include kafka kafka_2.12-1.1.0 lib lib64 libexec sbin share src [root@node3 local]# cd kafka [root@node3 kafka]# ls bin config kfklog libs LICENSE NOTICE site-docs zkdata [root@node3 kafka]# echo 2 > zkdata/myid [root@node3 kafka]# vim config/server.properties [root@node3 kafka]# grep -Pv \"^(#|$)\" config/zookeeper.properties dataDir=zkdata clientPort=2181 maxClientCnxns=0 tickTime=2000 initLimit=5 syncLimit=2 server.0=192.168.233.102:2888:3888 server.1=192.168.233.103:2888:3888 server.2=192.168.233.104:2888:3888 [root@node3 kafka]# grep -Pv \"^(#|$)\" config/server.properties broker.id=2 listeners=PLAINTEXT://:9092 advertised.listeners=PLAINTEXT://192.168.233.104:9092 num.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 log.dirs=kfklog num.partitions=3 num.recovery.threads.per.data.dir=1 offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connect=192.168.233.102:2181,192.168.233.103:2181,192.168.233.104:2181 zookeeper.connection.timeout.ms=6000 group.initial.rebalance.delay.ms=0 [root@node3 kafka]# grep -Pv \"^(#|$)\" config/consumer.properties bootstrap.servers=192.168.233.102:9092,192.168.233.103:9092,192.168.233.104:9092 zookeeper.connect=192.168.233.102:2181,192.168.233.103:2181,192.168.233.104:2181 group.id=test-consumer-group [root@node3 kafka]# grep -Pv \"^(#|$)\" config/producer.properties bootstrap.servers=192.168.233.102:9092,192.168.233.103:9092,192.168.233.104:9092 compression.type=none [root@node3 kafka]# 启动zookeeper [root@teacher kafka]# bin/zookeeper-server-start.sh config/zookeeper.properties & [root@node2 kafka]# bin/zookeeper-server-start.sh config/zookeeper.properties & [root@node3 kafka]# bin/zookeeper-server-start.sh config/zookeeper.properties & 启动kafka [root@teacher kafka]# bin/kafka-server-start.sh config/server.properties & [root@node2 kafka]# bin/kafka-server-start.sh config/server.properties & [root@node3 kafka]# bin/kafka-server-start.sh config/server.properties &","categories":[{"name":"SQL","slug":"SQL","permalink":"https://cyylog.github.io/categories/SQL/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://cyylog.github.io/tags/kafka/"}]},{"title":"ansible-roles","slug":"Linux/ansible-roles","date":"2019-04-18T15:50:14.000Z","updated":"2020-09-28T15:00:49.877Z","comments":true,"path":"2019/04/18/linux/ansible-roles/","link":"","permalink":"https://cyylog.github.io/2019/04/18/linux/ansible-roles/","excerpt":"","text":"ansible-roles1.简介roles则是在ansible中，playbooks的目录组织结构。而模块化之后，成为roles的组织结构，易读，代码可重用，层次清晰。 2.目标通过role远程部署nginx并配置 3.目录结构[ files/：存储由copy或script等模块调用的文件； tasks/：此目录中至少应该有一个名为main.yml的文件，用于定义各task；其它的文件需要由main.yml进行“包含”调用； handlers/：此目录中至少应该有一个名为main.yml的文件，用于定义各handler；其它的文件需要由main.yml进行“包含”调用； vars/：此目录中至少应该有一个名为main.yml的文件，用于定义各variable；其它的文件需要由main.yml进行“包含”调用； templates/：存储由template模块调用的模板文本； meta/：此目录中至少应该有一个名为main.yml的文件，定义当前角色的特殊设定及其依赖关系；其它的文件需要由main.yml进行“包含”调用； default/：此目录中至少应该有一个名为main.yml的文件，用于设定默认变量；准备目录结构 mkdir roles/nginx/{files,handlers,tasks,templates,vars} -p touch roles/site.yaml roles/nginx/{handlers,tasks,vars}/main.yaml echo tiger &gt; roles/nginx/files/index.html yum install -y nginx &amp;&amp; cp /etc/nginx/nginx.conf roles/nginx/templates/nginx.conf.j24.编写任务vim roles/nginx/tasks/main.yaml --- - name: install nginx packge yum: name={{ item }} state=latest with_items: - epel-release - nginx - name: copy index.html copy: src=index.html dest=/usr/share/nginx/html/index.html - name: copy nginx.conf template template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf notify: restart nginx - name: make sure nginx service running service: name=nginx state=started enabled=yes 5.准备配置文件vim roles/nginx/templates/nginx.conf.j2 worker_processes {{ ansible_processor_cores }}; //调用内部已知变量 worker_connections {{ worker_connections }}; //自定义变量6.编写变量vim roles/nginx/vars/main.yaml worker_connections: 102407.编写处理程序vim roles/nginx/handlers/main.yaml --- - name: restart nginx service: name=nginx state=restarted8.编写剧本vim roles/site.yaml - hosts: host4 roles: - nginx9.实施ansible-playbook site.yaml --syntax-check //测试 ansible-playbook site.yaml //实施剧本 验证hosts 引用变量案例： # tree . ├── site.yml ├── templates │ └── order.j2 └── vars └── main.yml 2 directories, 3 files 总调度yml文件： # cat site.yml --- - hosts: 192.168.19.154 user: root vars: - PROJECT: \"JAVA\" SWITCH: \"ON\" DBPORT: \"8080\" tasks: - name: create{{ PROJECT }}directory file: path=/data/{{ PROJECT }} state=directory - name: template transfor java template: src=order.j2 dest=/data/{{ PROJECT }}/order.conf 注意:这里 - role: template 和 - template 是一样的！ 其他yml文件，如下： # cat templates/order.j2 project: {{ PROJECT }} switch: {{ SWITCH }} dbport: {{ DBPORT }} 测试： # ansible-playbook templates.yml --syntax-check playbook: templates.yml 执行： PLAY [192.168.19.154] ********************************************************** TASK [Gathering Facts] ********************************************************* ok: [192.168.19.154] TASK [createJAVAdirectory] ***************************************************** changed: [192.168.19.154] TASK [template transfor java] ************************************************** changed: [192.168.19.154] PLAY RECAP ********************************************************************* 192.168.19.154 : ok=3 changed=2 unreachable=0 failed=0 #cat /data/JAVA/order.conf project: JAVA switch: ON dbport: 8080","categories":[{"name":"Tools","slug":"Tools","permalink":"https://cyylog.github.io/categories/Tools/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/tags/Linux/"},{"name":"Ansible","slug":"Ansible","permalink":"https://cyylog.github.io/tags/Ansible/"}]},{"title":"ansible","slug":"Linux/ansible","date":"2019-04-16T15:50:14.000Z","updated":"2020-09-28T14:53:29.609Z","comments":true,"path":"2019/04/16/linux/ansible/","link":"","permalink":"https://cyylog.github.io/2019/04/16/linux/ansible/","excerpt":"","text":"自动化运维工具—ansible详解一、ansible 简介1、ansible 是什么？ ansible是目前最受运维欢迎的自动化运维工具，基于Python开发，集合了众多运维工具（SaltStack puppet、chef、func、fabric）的优点，实现了批量系统配置、批量程序部署、批量运行命令等功能。 ansible是基于 paramiko 开发的,并且基于模块化工作，本身没有批量部署的能力。真正具有批量部署的是ansible所运行的模块，ansible只是提供一种框架。ansible不需要在远程主机上安装client/agents，因为它们是基于ssh来和远程主机通讯的。ansible目前已经已经被红帽官方收购，是自动化运维工具中大家认可度最高的，并且上手容易，学习简单。是每位运维工程师必须掌握的技能之一。 2、ansible 特点 部署简单，只需在主控端部署Ansible环境，被控端无需做任何操作； 默认使用SSH协议对设备进行管理； 有大量常规运维操作模块，可实现日常绝大部分操作； 配置简单、功能强大、扩展性强； 支持API及自定义模块，可通过Python轻松扩展； 通过Playbooks来定制强大的配置、状态管理； 轻量级，无需在客户端安装agent，更新时，只需在操作机上进行一次更新即可； 提供一个功能强大、操作性强的Web管理界面和REST API接口——AWX平台。 3、ansible 架构图 Ansible：Ansible核心程序。HostInventory：记录由Ansible管理的主机信息，包括端口、密码、ip等。Playbooks：“剧本”YAML格式文件，多个任务定义在一个文件中，定义主机需要调用哪些模块来完成的功能。CoreModules：核心模块，主要操作是通过调用核心模块来完成管理任务。CustomModules：自定义模块，完成核心模块无法完成的功能，支持多种语言。ConnectionPlugins：连接插件，Ansible和Host通信使用 二、ansible 任务执行1、ansible 任务执行模式 Ansible 系统由控制主机对被管节点的操作方式可分为两类，即ad-hoc和playbook： ad-hoc模式(点对点模式)使用单个模块，支持批量执行单条命令。ad-hoc 命令是一种可以快速输入的命令，而且不需要保存起来的命令。就相当于bash中的一句话shell。 playbook模式(剧本模式)是Ansible主要管理方式，也是Ansible功能强大的关键所在。playbook通过多个task集合完成一类功能，如Web服务的安装部署、数据库服务器的批量备份等。可以简单地把playbook理解为通过组合多条ad-hoc操作的配置文件。 2、ansible 执行流程 简单理解就是Ansible在运行时， 首先读取ansible.cfg中的配置， 根据规则获取Inventory中的管理主机列表， 并行的在这些主机中执行配置的任务， 最后等待执行返回的结果。 3、ansible 命令执行过程 加载自己的配置文件，默认/etc/ansible/ansible.cfg； 查找对应的主机配置文件，找到要执行的主机或者组； 加载自己对应的模块文件，如 command； 通过ansible将模块或命令生成对应的临时py文件(python脚本)， 并将该文件传输至远程服务器； 对应执行用户的家目录的.ansible/tmp/XXX/XXX.PY文件； 给文件 +x 执行权限； 执行并返回结果； 删除临时py文件，sleep 0退出； 三、ansible 配置详解1、ansible 安装方式 ansible安装常用两种方式，yum 安装 和 pip 程序安装。下面我们来详细介绍一下这两种安装方式。 2、使用 pip（python的包管理模块）安装 首先，我们需要安装一个python-pip包，安装完成以后，则直接使用pip命令来安装我们的包，具体操作过程如下： yum install python-pip pip install ansible4、使用 yum 安装 yum 安装是我们很熟悉的安装方式了。我们需要先安装一个epel-release包，然后再安装我们的 ansible 即可。 yum install epel-release -y yum install ansible –y5、ansible 程序结构安装目录如下(yum安装)： 配置文件目录：/etc/ansible/ 执行文件目录：/usr/bin/ Lib库依赖目录：/usr/lib/pythonX.X/site-packages/ansible/ Help文档目录：/usr/share/doc/ansible-X.X.X/ Man文档目录：/usr/share/man/man1/ 6、ansible配置文件查找顺序 ansible与我们其他的服务在这一点上有很大不同，这里的配置文件查找是从多个地方找的，顺序如下： 检查环境变量ANSIBLE_CONFIG指向的路径文件(export ANSIBLE_CONFIG=/etc/ansible.cfg)； ~/.ansible.cfg，检查当前目录下的ansible.cfg配置文件； /etc/ansible.cfg检查etc目录的配置文件。 7、ansible配置文件 ansible 的配置文件为/etc/ansible/ansible.cfg，ansible 有许多参数，下面我们列出一些常见的参数： inventory = /etc/ansible/hosts #这个参数表示资源清单inventory文件的位置 library = /usr/share/ansible #指向存放Ansible模块的目录，支持多个目录方式，只要用冒号（：）隔开就可以 forks = 5 #并发连接数，默认为5 sudo_user = root #设置默认执行命令的用户 remote_port = 22 #指定连接被管节点的管理端口，默认为22端口，建议修改，能够更加安全 host_key_checking = False #设置是否检查SSH主机的密钥，值为True/False。关闭后第一次连接不会提示配置实例 timeout = 60 #设置SSH连接的超时时间，单位为秒 log_path = /var/log/ansible.log #指定一个存储ansible日志的文件（默认不记录日志）8、ansuble主机清单 在配置文件中，我们提到了资源清单，这个清单就是我们的主机清单，里面保存的是一些 ansible 需要连接管理的主机列表。我们可以来看看他的定义方式： 1、 直接指明主机地址或主机名： # green.example.com # blue.example.com # 192.168.100.1 # 192.168.100.10 2、 定义一个主机组[组名]把地址或主机名加进去 [mysql_test] 192.168.253.159 192.168.253.160 192.168.253.153 需要注意的是，这里的组成员可以使用通配符来匹配，这样对于一些标准化的管理来说就很轻松方便了。 我们可以根据实际情况来配置我们的主机列表，具体操作如下： [root@server ~]# vim /etc/ansible/hosts [web] 192.168.37.122 192.168.37.133四、ansible 常用命令1、ansible 命令集 /usr/bin/ansible Ansibe AD-Hoc 临时命令执行工具，常用于临时命令的执行/usr/bin/ansible-doc Ansible 模块功能查看工具/usr/bin/ansible-galaxy 下载/上传优秀代码或Roles模块 的官网平台，基于网络的/usr/bin/ansible-playbook Ansible 定制自动化的任务集编排工具/usr/bin/ansible-pull Ansible远程执行命令的工具，拉取配置而非推送配置（使用较少，海量机器时使用，对运维的架构能力要求较高）/usr/bin/ansible-vault Ansible 文件加密工具/usr/bin/ansible-console Ansible基于Linux Consoble界面可与用户交互的命令执行工具 其中，我们比较常用的是/usr/bin/ansible和/usr/bin/ansible-playbook。 2、ansible-doc 命令 ansible-doc 命令常用于获取模块信息及其使用帮助，一般用法如下： ansible-doc -l #获取全部模块的信息 ansible-doc -s MOD_NAME #获取指定模块的使用帮助 我们也可以查看一下ansible-doc的全部用法： [root@server ~]# ansible-doc Usage: ansible-doc [options] [module...] Options: -h, --help show this help message and exit # 显示命令参数API文档 -l, --list List available modules #列出可用的模块 -M MODULE_PATH, --module-path=MODULE_PATH #指定模块的路径 specify path(s) to module library (default=None) -s, --snippet Show playbook snippet for specified module(s) #显示playbook制定模块的用法 -v, --verbose verbose mode (-vvv for more, -vvvv to enable # 显示ansible-doc的版本号查看模块列表： connection debugging) --version show program&#39;s version number and exit 我们可以来看一下，以mysql相关的为例： [root@server ~]# ansible-doc -l |grep mysql mysql_db Add or remove MySQL databases from a remote... mysql_replication Manage MySQL replication mysql_user Adds or removes a user from a MySQL databas... mysql_variables Manage MySQL global variables [root@server ~]# ansible-doc -s mysql_user2、ansible 命令详解 命令的具体格式如下： ansible &lt;host-pattern&gt; [-f forks] [-m module_name] [-a args] 也可以通过ansible -h来查看帮助，下面我们列出一些比较常用的选项，并解释其含义： -a MODULE_ARGS #模块的参数，如果执行默认COMMAND的模块，即是命令参数，如： “date”，“pwd”等等-k，--ask-pass #ask for SSH password。登录密码，提示输入SSH密码而不是假设基于密钥的验证--ask-su-pass #ask for su password。su切换密码-K，--ask-sudo-pass #ask for sudo password。提示密码使用sudo，sudo表示提权操作--ask-vault-pass #ask for vault password。假设我们设定了加密的密码，则用该选项进行访问-B SECONDS #后台运行超时时间-C #模拟运行环境并进行预运行，可以进行查错测试-c CONNECTION #连接类型使用-f FORKS #并行任务数，默认为5-i INVENTORY #指定主机清单的路径，默认为/etc/ansible/hosts--list-hosts #查看有哪些主机组-m MODULE_NAME #执行模块的名字，默认使用 command 模块，所以如果是只执行单一命令可以不用 -m参数-o #压缩输出，尝试将所有结果在一行输出，一般针对收集工具使用-S #用 su 命令-R SU_USER #指定 su 的用户，默认为 root 用户-s #用 sudo 命令-U SUDO_USER #指定 sudo 到哪个用户，默认为 root 用户-T TIMEOUT #指定 ssh 默认超时时间，默认为10s，也可在配置文件中修改-u REMOTE_USER #远程用户，默认为 root 用户-v #查看详细信息，同时支持-vvv，-vvvv可查看更详细信息 3、ansible 配置公私钥 上面我们已经提到过 ansible 是基于 ssh 协议实现的，所以其配置公私钥的方式与 ssh 协议的方式相同，具体操作步骤如下： #1.生成私钥 [root@server ~]# ssh-keygen #2.向主机分发私钥 [root@server ~]# ssh-copy-id root@192.168.37.122 [root@server ~]# ssh-copy-id root@192.168.37.133 这样的话，就可以实现无密码登录，我们的实验过程也会顺畅很多。 注意，如果出现了一下报错： -bash: ssh-copy-id: command not found 那么就证明我们需要安装一个包： yum -y install openssh-clientsansible 把包安装上即可。 五、ansible 常用模块1、主机连通性测试 我们使用ansible web -m ping命令来进行主机连通性测试，效果如下： [root@server ~]# ansible web -m ping 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } 这样就说明我们的主机是连通状态的。接下来的操作才可以正常进行。 2、command 模块 这个模块可以直接在远程主机上执行命令，并将结果返回本主机。 举例如下： [root@server ~]# ansible web -m command -a &#39;ss -ntl&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:111 *:* LISTEN 0 5 192.168.122.1:53 *:* LISTEN 0 128 *:22 *:* LISTEN 0 128 127.0.0.1:631 *:* LISTEN 0 128 *:23000 *:* LISTEN 0 100 127.0.0.1:25 *:* LISTEN 0 128 :::111 :::* LISTEN 0 128 :::22 :::* LISTEN 0 128 ::1:631 :::* LISTEN 0 100 ::1:25 :::* 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:111 *:* LISTEN 0 128 *:22 *:* LISTEN 0 128 127.0.0.1:631 *:* LISTEN 0 128 *:23000 *:* LISTEN 0 100 127.0.0.1:25 *:* LISTEN 0 128 :::111 :::* LISTEN 0 128 :::22 :::* LISTEN 0 128 ::1:631 :::* LISTEN 0 100 ::1:25 :::* 命令模块接受命令名称，后面是空格分隔的列表参数。给定的命令将在所有选定的节点上执行。它不会通过shell进行处理，比如$HOME和操作如”&lt;”，”&gt;”，”|”，”;”，”&amp;” 工作（需要使用（shell）模块实现这些功能）。注意，该命令不支持| 管道命令。 下面来看一看该模块下常用的几个命令： chdir # 在执行命令之前，先切换到该目录executable # 切换shell来执行命令，需要使用命令的绝对路径free_form # 要执行的Linux指令，一般使用Ansible的-a参数代替。creates # 一个文件名，当这个文件存在，则该命令不执行,可以用来做判断removes # 一个文件名，这个文件不存在，则该命令不执行 下面我们来看看这些命令的执行效果： [root@server ~]# ansible web -m command -a &#39;chdir=/data/ ls&#39; #先切换到/data/ 目录，再执行“ls”命令 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; aaa.jpg fastdfs mogdata tmp web wKgleloeYoCAMLtZAAAWEekAtkc497.jpg 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; aaa.jpg fastdfs mogdata tmp web wKgleloeYoCAMLtZAAAWEekAtkc497.jpg [root@server ~]# ansible web -m command -a &#39;creates=/data/aaa.jpg ls&#39; #如果/data/aaa.jpg存在，则不执行“ls”命令 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; skipped, since /data/aaa.jpg exists 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; skipped, since /data/aaa.jpg exists [root@server ~]# ansible web -m command -a &#39;removes=/data/aaa.jpg cat /data/a&#39; #如果/data/aaa.jpg存在，则执行“cat /data/a”命令 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; hello 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; hello3、shell 模块 shell模块可以在远程主机上调用shell解释器运行命令，支持shell的各种功能，例如管道等。 [root@server ~]# ansible web -m shell -a &#39;cat /etc/passwd |grep &quot;keer&quot;&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; keer:x:10001:1000:keer:/home/keer:/bin/sh 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; keer:x:10001:10001::/home/keer:/bin/sh 只要是我们的shell命令，都可以通过这个模块在远程主机上运行，这里就不一一举例了。 4、copy 模块 这个模块用于将文件复制到远程主机，同时支持给定内容生成文件和修改权限等。 其相关选项如下： src #被复制到远程主机的本地文件。可以是绝对路径，也可以是相对路径。如果路径是一个目录，则会递归复制，用法类似于”rsync”content #用于替换”src”，可以直接指定文件的值dest #必选项，将源文件复制到的远程主机的绝对路径backup #当文件内容发生改变后，在覆盖之前把源文件备份，备份文件包含时间信息directory_mode #递归设定目录的权限，默认为系统默认权限force #当目标主机包含该文件，但内容不同时，设为”yes”，表示强制覆盖；设为”no”，表示目标主机的目标位置不存在该文件才复制。默认为”yes”others #所有的 file 模块中的选项可以在这里使用 用法举例如下：① 复制文件： [root@server ~]# ansible web -m copy -a &#39;src=~/hello dest=/data/hello&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;checksum&quot;: &quot;22596363b3de40b06f981fb85d82312e8c0ed511&quot;, &quot;dest&quot;: &quot;/data/hello&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;6f5902ac237024bdd0c176cb93063dc4&quot;, &quot;mode&quot;: &quot;0644&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 12, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1512437093.55-228281064292921/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0 } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;checksum&quot;: &quot;22596363b3de40b06f981fb85d82312e8c0ed511&quot;, &quot;dest&quot;: &quot;/data/hello&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;6f5902ac237024bdd0c176cb93063dc4&quot;, &quot;mode&quot;: &quot;0644&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 12, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1512437093.74-44694985235189/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0 }② 给定内容生成文件，并制定权限 [root@server ~]# ansible web -m copy -a &#39;content=&quot;I am keer\\n&quot; dest=/data/name mode=666&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;checksum&quot;: &quot;0421570938940ea784f9d8598dab87f07685b968&quot;, &quot;dest&quot;: &quot;/data/name&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;497fa8386590a5fc89090725b07f175c&quot;, &quot;mode&quot;: &quot;0666&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 10, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1512437327.37-199512601767687/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0 } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;checksum&quot;: &quot;0421570938940ea784f9d8598dab87f07685b968&quot;, &quot;dest&quot;: &quot;/data/name&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;497fa8386590a5fc89090725b07f175c&quot;, &quot;mode&quot;: &quot;0666&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 10, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1512437327.55-218104039503110/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0 } 我们现在可以去查看一下我们生成的文件及其权限： [root@server ~]# ansible web -m shell -a &#39;ls -l /data/&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; total 28 -rw-rw-rw- 1 root root 12 Dec 6 09:45 name 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; total 40 -rw-rw-rw- 1 root root 12 Dec 5 09:45 name 可以看出我们的name文件已经生成，并且权限为666。③ 关于覆盖 我们把文件的内容修改一下，然后选择覆盖备份： [root@server ~]# ansible web -m copy -a &#39;content=&quot;I am keerya\\n&quot; backup=yes dest=/data/name mode=666&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;backup_file&quot;: &quot;/data/name.4394.2017-12-06@09:46:25~&quot;, &quot;changed&quot;: true, &quot;checksum&quot;: &quot;064a68908ab9971ee85dbc08ea038387598e3778&quot;, &quot;dest&quot;: &quot;/data/name&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;8ca7c11385856155af52e560f608891c&quot;, &quot;mode&quot;: &quot;0666&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 12, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1512438383.78-228128616784888/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0 } 192.168.37.133 | SUCCESS =&gt; { &quot;backup_file&quot;: &quot;/data/name.5962.2017-12-05@09:46:24~&quot;, &quot;changed&quot;: true, &quot;checksum&quot;: &quot;064a68908ab9971ee85dbc08ea038387598e3778&quot;, &quot;dest&quot;: &quot;/data/name&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;8ca7c11385856155af52e560f608891c&quot;, &quot;mode&quot;: &quot;0666&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 12, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1512438384.0-170718946740009/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0 } 现在我们可以去查看一下： [root@server ~]# ansible web -m shell -a &#39;ls -l /data/&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; total 28 -rw-rw-rw- 1 root root 12 Dec 6 09:46 name -rw-rw-rw- 1 root root 10 Dec 6 09:45 name.4394.2017-12-06@09:46:25~ 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; total 40 -rw-rw-rw- 1 root root 12 Dec 5 09:46 name -rw-rw-rw- 1 root root 10 Dec 5 09:45 name.5962.2017-12-05@09:46:24~ 可以看出，我们的源文件已经被备份，我们还可以查看一下name文件的内容： [root@server ~]# ansible web -m shell -a &#39;cat /data/name&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; I am keerya 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; I am keerya 证明，这正是我们新导入的文件的内容。 5、file 模块 该模块主要用于设置文件的属性，比如创建文件、创建链接文件、删除文件等。 下面是一些常见的命令： force #需要在两种情况下强制创建软链接，一种是源文件不存在，但之后会建立的情况下；另一种是目标软链接已存在，需要先取消之前的软链，然后创建新的软链，有两个选项：yes|nogroup #定义文件/目录的属组。后面可以加上mode：定义文件/目录的权限owner #定义文件/目录的属主。后面必须跟上path：定义文件/目录的路径recurse #递归设置文件的属性，只对目录有效，后面跟上src：被链接的源文件路径，只应用于state=link的情况dest #被链接到的路径，只应用于state=link的情况state #状态，有以下选项： directory：如果目录不存在，就创建目录file：即使文件不存在，也不会被创建link：创建软链接hard：创建硬链接touch：如果文件不存在，则会创建一个新的文件，如果文件或目录已存在，则更新其最后修改时间absent：删除目录、文件或者取消链接文件 用法举例如下：① 创建目录： [root@server ~]# ansible web -m file -a &#39;path=/data/app state=directory&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;mode&quot;: &quot;0755&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;path&quot;: &quot;/data/app&quot;, &quot;size&quot;: 6, &quot;state&quot;: &quot;directory&quot;, &quot;uid&quot;: 0 } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;mode&quot;: &quot;0755&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;path&quot;: &quot;/data/app&quot;, &quot;size&quot;: 4096, &quot;state&quot;: &quot;directory&quot;, &quot;uid&quot;: 0 } 我们可以查看一下： [root@server ~]# ansible web -m shell -a &#39;ls -l /data&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; total 28 drwxr-xr-x 2 root root 6 Dec 6 10:21 app 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; total 44 drwxr-xr-x 2 root root 4096 Dec 5 10:21 app 可以看出，我们的目录已经创建完成。② 创建链接文件 [root@server ~]# ansible web -m file -a &#39;path=/data/bbb.jpg src=aaa.jpg state=link&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;dest&quot;: &quot;/data/bbb.jpg&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;mode&quot;: &quot;0777&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 7, &quot;src&quot;: &quot;aaa.jpg&quot;, &quot;state&quot;: &quot;link&quot;, &quot;uid&quot;: 0 } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;dest&quot;: &quot;/data/bbb.jpg&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;mode&quot;: &quot;0777&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 7, &quot;src&quot;: &quot;aaa.jpg&quot;, &quot;state&quot;: &quot;link&quot;, &quot;uid&quot;: 0 } 我们可以去查看一下： [root@server ~]# ansible web -m shell -a &#39;ls -l /data&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; total 28 -rw-r--r-- 1 root root 5649 Dec 5 13:49 aaa.jpg lrwxrwxrwx 1 root root 7 Dec 6 10:25 bbb.jpg -&gt; aaa.jpg 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; total 44 -rw-r--r-- 1 root root 5649 Dec 4 14:44 aaa.jpg lrwxrwxrwx 1 root root 7 Dec 5 10:25 bbb.jpg -&gt; aaa.jpg 我们的链接文件已经创建成功。③ 删除文件 [root@server ~]# ansible web -m file -a &#39;path=/data/a state=absent&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;path&quot;: &quot;/data/a&quot;, &quot;state&quot;: &quot;absent&quot; } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;path&quot;: &quot;/data/a&quot;, &quot;state&quot;: &quot;absent&quot; } 我们可以查看一下： [root@server ~]# ansible web -m shell -a &#39;ls /data/a&#39; 192.168.37.122 | FAILED | rc=2 &gt;&gt; ls: cannot access /data/a: No such file or directory 192.168.37.133 | FAILED | rc=2 &gt;&gt; ls: cannot access /data/a: No such file or directory 发现已经没有这个文件了。 6、fetch 模块 该模块用于从远程某主机获取（复制）文件到本地。 有两个选项： dest：用来存放文件的目录src：在远程拉取的文件，并且必须是一个file，不能是目录 具体举例如下： [root@server ~]# ansible web -m fetch -a &#39;src=/data/hello dest=/data&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;checksum&quot;: &quot;22596363b3de40b06f981fb85d82312e8c0ed511&quot;, &quot;dest&quot;: &quot;/data/192.168.37.122/data/hello&quot;, &quot;md5sum&quot;: &quot;6f5902ac237024bdd0c176cb93063dc4&quot;, &quot;remote_checksum&quot;: &quot;22596363b3de40b06f981fb85d82312e8c0ed511&quot;, &quot;remote_md5sum&quot;: null } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;checksum&quot;: &quot;22596363b3de40b06f981fb85d82312e8c0ed511&quot;, &quot;dest&quot;: &quot;/data/192.168.37.133/data/hello&quot;, &quot;md5sum&quot;: &quot;6f5902ac237024bdd0c176cb93063dc4&quot;, &quot;remote_checksum&quot;: &quot;22596363b3de40b06f981fb85d82312e8c0ed511&quot;, &quot;remote_md5sum&quot;: null } 我们可以在本机上查看一下文件是否复制成功。要注意，文件保存的路径是我们设置的接收目录下的被管制主机ip目录下： [root@server ~]# cd /data/ [root@server data]# ls 1 192.168.37.122 192.168.37.133 fastdfs web [root@server data]# cd 192.168.37.122 [root@server 192.168.37.122]# ls data [root@server 192.168.37.122]# cd data/ [root@server data]# ls hello [root@server data]# pwd /data/192.168.37.122/data7、cron 模块 该模块适用于管理cron计划任务的。 其使用的语法跟我们的crontab文件中的语法一致，同时，可以指定以下选项： day= #日应该运行的工作( 1-31, , /2, )hour= # 小时 ( 0-23, , /2, )minute= #分钟( 0-59, , /2, )month= # 月( 1-12, *, /2, )weekday= # 周 ( 0-6 for Sunday-Saturday,, )job= #指明运行的命令是什么name= #定时任务描述reboot # 任务在重启时运行，不建议使用，建议使用special_timespecial_time #特殊的时间范围，参数：reboot（重启时），annually（每年），monthly（每月），weekly（每周），daily（每天），hourly（每小时）state #指定状态，present表示添加定时任务，也是默认设置，absent表示删除定时任务user # 以哪个用户的身份执行 举例如下：① 添加计划任务 [root@server ~]# ansible web -m cron -a &#39;name=&quot;ntp update every 5 min&quot; minute=*/5 job=&quot;/sbin/ntpdate 172.17.0.1 &amp;&gt; /dev/null&quot;&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;envs&quot;: [], &quot;jobs&quot;: [ &quot;ntp update every 5 min&quot; ] } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;envs&quot;: [], &quot;jobs&quot;: [ &quot;ntp update every 5 min&quot; ] } 我们可以去查看一下： [root@server ~]# ansible web -m shell -a &#39;crontab -l&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; #Ansible: ntp update every 5 min */5 * * * * /sbin/ntpdate 172.17.0.1 &amp;&gt; /dev/null 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; #Ansible: ntp update every 5 min */5 * * * * /sbin/ntpdate 172.17.0.1 &amp;&gt; /dev/null 可以看出，我们的计划任务已经设置成功了。② 删除计划任务 如果我们的计划任务添加错误，想要删除的话，则执行以下操作： 首先我们查看一下现有的计划任务： [root@server ~]# ansible web -m shell -a &#39;crontab -l&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; #Ansible: ntp update every 5 min */5 * * * * /sbin/ntpdate 172.17.0.1 &amp;&gt; /dev/null #Ansible: df everyday * 15 * * * df -lh &gt;&gt; /tmp/disk_total &amp;&gt; /dev/null 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; #Ansible: ntp update every 5 min */5 * * * * /sbin/ntpdate 172.17.0.1 &amp;&gt; /dev/null #Ansible: df everyday * 15 * * * df -lh &gt;&gt; /tmp/disk_total &amp;&gt; /dev/null 然后执行删除操作： [root@server ~]# ansible web -m cron -a &#39;name=&quot;df everyday&quot; hour=15 job=&quot;df -lh &gt;&gt; /tmp/disk_total &amp;&gt; /dev/null&quot; state=absent&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;envs&quot;: [], &quot;jobs&quot;: [ &quot;ntp update every 5 min&quot; ] } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;envs&quot;: [], &quot;jobs&quot;: [ &quot;ntp update every 5 min&quot; ] } 删除完成后，我们再查看一下现有的计划任务确认一下： [root@server ~]# ansible web -m shell -a &#39;crontab -l&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; #Ansible: ntp update every 5 min */5 * * * * /sbin/ntpdate 172.17.0.1 &amp;&gt; /dev/null 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; #Ansible: ntp update every 5 min */5 * * * * /sbin/ntpdate 172.17.0.1 &amp;&gt; /dev/null 我们的删除操作已经成功。 8、yum 模块 顾名思义，该模块主要用于软件的安装。 其选项如下： name= #所安装的包的名称state= #present—&gt;安装， latest—&gt;安装最新的, absent—&gt; 卸载软件。update_cache #强制更新yum的缓存conf_file #指定远程yum安装时所依赖的配置文件（安装本地已有的包）。disable_pgp_check #是否禁止GPG checking，只用于presentor latest。disablerepo #临时禁止使用yum库。 只用于安装或更新时。enablerepo #临时使用的yum库。只用于安装或更新时。 下面我们就来安装一个包试试看： [root@server ~]# ansible web -m yum -a &#39;name=htop state=present&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;Loaded plugins: fastestmirror, langpacks\\nLoading mirror speeds from cached hostfile\\nResolving Dependencies\\n--&gt; Running transaction check\\n---&gt; Package htop.x86_64 0:2.0.2-1.el7 will be installed\\n--&gt; Finished Dependency Resolution\\n\\nDependencies Resolved\\n\\n================================================================================\\n Package Arch Version Repository Size\\n================================================================================\\nInstalling:\\n htop x86_64 2.0.2-1.el7 epel 98 k\\n\\nTransaction Summary\\n================================================================================\\nInstall 1 Package\\n\\nTotal download size: 98 k\\nInstalled size: 207 k\\nDownloading packages:\\nRunning transaction check\\nRunning transaction test\\nTransaction test succeeded\\nRunning transaction\\n Installing : htop-2.0.2-1.el7.x86_64 1/1 \\n Verifying : htop-2.0.2-1.el7.x86_64 1/1 \\n\\nInstalled:\\n htop.x86_64 0:2.0.2-1.el7 \\n\\nComplete!\\n&quot; ] } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;msg&quot;: &quot;Warning: RPMDB altered outside of yum.\\n** Found 3 pre-existing rpmdb problem(s), &#39;yum check&#39; output follows:\\nipa-client-4.4.0-12.el7.centos.x86_64 has installed conflicts freeipa-client: ipa-client-4.4.0-12.el7.centos.x86_64\\nipa-client-common-4.4.0-12.el7.centos.noarch has installed conflicts freeipa-client-common: ipa-client-common-4.4.0-12.el7.centos.noarch\\nipa-common-4.4.0-12.el7.centos.noarch has installed conflicts freeipa-common: ipa-common-4.4.0-12.el7.centos.noarch\\n&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;Loaded plugins: fastestmirror, langpacks\\nLoading mirror speeds from cached hostfile\\nResolving Dependencies\\n--&gt; Running transaction check\\n---&gt; Package htop.x86_64 0:2.0.2-1.el7 will be installed\\n--&gt; Finished Dependency Resolution\\n\\nDependencies Resolved\\n\\n================================================================================\\n Package Arch Version Repository Size\\n================================================================================\\nInstalling:\\n htop x86_64 2.0.2-1.el7 epel 98 k\\n\\nTransaction Summary\\n================================================================================\\nInstall 1 Package\\n\\nTotal download size: 98 k\\nInstalled size: 207 k\\nDownloading packages:\\nRunning transaction check\\nRunning transaction test\\nTransaction test succeeded\\nRunning transaction\\n Installing : htop-2.0.2-1.el7.x86_64 1/1 \\n Verifying : htop-2.0.2-1.el7.x86_64 1/1 \\n\\nInstalled:\\n htop.x86_64 0:2.0.2-1.el7 \\n\\nComplete!\\n&quot; ] } 安装成功。 9、service 模块 该模块用于服务程序的管理。 其主要选项如下： arguments #命令行提供额外的参数enabled #设置开机启动。name= #服务名称runlevel #开机启动的级别，一般不用指定。sleep #在重启服务的过程中，是否等待。如在服务关闭以后等待2秒再启动。(定义在剧本中。)state #有四种状态，分别为：started—&gt;启动服务， stopped—&gt;停止服务， restarted—&gt;重启服务， reloaded—&gt;重载配置 下面是一些例子：① 开启服务并设置自启动 [root@server ~]# ansible web -m service -a &#39;name=nginx state=started enabled=true&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;enabled&quot;: true, &quot;name&quot;: &quot;nginx&quot;, &quot;state&quot;: &quot;started&quot;, …… } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;enabled&quot;: true, &quot;name&quot;: &quot;nginx&quot;, &quot;state&quot;: &quot;started&quot;, …… } 我们可以去查看一下端口是否打开： [root@server ~]# ansible web -m shell -a &#39;ss -ntl&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:80 *:* 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:80 *:* 可以看出我们的80端口已经打开。② 关闭服务 我们也可以通过该模块来关闭我们的服务： [root@server ~]# ansible web -m service -a &#39;name=nginx state=stopped&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;name&quot;: &quot;nginx&quot;, &quot;state&quot;: &quot;stopped&quot;, …… } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;name&quot;: &quot;nginx&quot;, &quot;state&quot;: &quot;stopped&quot;, …… } 一样的，我们来查看一下端口： [root@server ~]# ansible web -m shell -a &#39;ss -ntl | grep 80&#39; 192.168.37.122 | FAILED | rc=1 &gt;&gt; 192.168.37.133 | FAILED | rc=1 &gt;&gt; 可以看出，我们已经没有80端口了，说明我们的nginx服务已经关闭了。 10、user 模块 该模块主要是用来管理用户账号。 其主要选项如下： comment # 用户的描述信息createhome # 是否创建家目录force # 在使用state=absent时, 行为与userdel –force一致.group # 指定基本组groups # 指定附加组，如果指定为(groups=)表示删除所有组home # 指定用户家目录move_home # 如果设置为home=时, 试图将用户主目录移动到指定的目录name # 指定用户名non_unique # 该选项允许改变非唯一的用户ID值password # 指定用户密码remove # 在使用state=absent时, 行为是与userdel –remove一致shell # 指定默认shellstate # 设置帐号状态，不指定为创建，指定值为absent表示删除system # 当创建一个用户，设置这个用户是系统用户。这个设置不能更改现有用户uid # 指定用户的uid 举例如下：① 添加一个用户并指定其 uid [root@server ~]# ansible web -m user -a &#39;name=keer uid=11111&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;comment&quot;: &quot;&quot;, &quot;createhome&quot;: true, &quot;group&quot;: 11111, &quot;home&quot;: &quot;/home/keer&quot;, &quot;name&quot;: &quot;keer&quot;, &quot;shell&quot;: &quot;/bin/bash&quot;, &quot;state&quot;: &quot;present&quot;, &quot;stderr&quot;: &quot;useradd: warning: the home directory already exists.\\nNot copying any file from skel directory into it.\\nCreating mailbox file: File exists\\n&quot;, &quot;system&quot;: false, &quot;uid&quot;: 11111 } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;comment&quot;: &quot;&quot;, &quot;createhome&quot;: true, &quot;group&quot;: 11111, &quot;home&quot;: &quot;/home/keer&quot;, &quot;name&quot;: &quot;keer&quot;, &quot;shell&quot;: &quot;/bin/bash&quot;, &quot;state&quot;: &quot;present&quot;, &quot;stderr&quot;: &quot;useradd: warning: the home directory already exists.\\nNot copying any file from skel directory into it.\\nCreating mailbox file: File exists\\n&quot;, &quot;system&quot;: false, &quot;uid&quot;: 11111 } 添加完成，我们可以去查看一下： [root@server ~]# ansible web -m shell -a &#39;cat /etc/passwd |grep keer&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; keer:x:11111:11111::/home/keer:/bin/bash 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; keer:x:11111:11111::/home/keer:/bin/bash② 删除用户 [root@server ~]# ansible web -m user -a &#39;name=keer state=absent&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;force&quot;: false, &quot;name&quot;: &quot;keer&quot;, &quot;remove&quot;: false, &quot;state&quot;: &quot;absent&quot; } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;force&quot;: false, &quot;name&quot;: &quot;keer&quot;, &quot;remove&quot;: false, &quot;state&quot;: &quot;absent&quot; } 一样的，删除之后，我们去看一下： [root@server ~]# ansible web -m shell -a &#39;cat /etc/passwd |grep keer&#39; 192.168.37.122 | FAILED | rc=1 &gt;&gt; 192.168.37.133 | FAILED | rc=1 &gt;&gt; 发现已经没有这个用户了。 11、group 模块 该模块主要用于添加或删除组。 常用的选项如下： gid= #设置组的GID号name= #指定组的名称state= #指定组的状态，默认为创建，设置值为absent为删除system= #设置值为yes，表示创建为系统组 举例如下：① 创建组 [root@server ~]# ansible web -m group -a &#39;name=sanguo gid=12222&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;gid&quot;: 12222, &quot;name&quot;: &quot;sanguo&quot;, &quot;state&quot;: &quot;present&quot;, &quot;system&quot;: false } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;gid&quot;: 12222, &quot;name&quot;: &quot;sanguo&quot;, &quot;state&quot;: &quot;present&quot;, &quot;system&quot;: false } 创建过后，我们来查看一下： [root@server ~]# ansible web -m shell -a &#39;cat /etc/group | grep 12222&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; sanguo:x:12222: 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; sanguo:x:12222: 可以看出，我们的组已经创建成功了。② 删除组 [root@server ~]# ansible web -m group -a &#39;name=sanguo state=absent&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;name&quot;: &quot;sanguo&quot;, &quot;state&quot;: &quot;absent&quot; } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;name&quot;: &quot;sanguo&quot;, &quot;state&quot;: &quot;absent&quot; } 照例查看一下： [root@server ~]# ansible web -m shell -a &#39;cat /etc/group | grep 12222&#39; 192.168.37.122 | FAILED | rc=1 &gt;&gt; 192.168.37.133 | FAILED | rc=1 &gt;&gt; 已经没有这个组的相关信息了。 12、script 模块 该模块用于将本机的脚本在被管理端的机器上运行。 该模块直接指定脚本的路径即可，我们通过例子来看一看到底如何使用的： 首先，我们写一个脚本，并给其加上执行权限： [root@server ~]# vim /tmp/df.sh #!/bin/bash date &gt;&gt; /tmp/disk_total.log df -lh &gt;&gt; /tmp/disk_total.log [root@server ~]# chmod +x /tmp/df.sh 然后，我们直接运行命令来实现在被管理端执行该脚本： [root@server ~]# ansible web -m script -a &#39;/tmp/df.sh&#39; 192.168.37.122 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;rc&quot;: 0, &quot;stderr&quot;: &quot;Shared connection to 192.168.37.122 closed.\\r\\n&quot;, &quot;stdout&quot;: &quot;&quot;, &quot;stdout_lines&quot;: [] } 192.168.37.133 | SUCCESS =&gt; { &quot;changed&quot;: true, &quot;rc&quot;: 0, &quot;stderr&quot;: &quot;Shared connection to 192.168.37.133 closed.\\r\\n&quot;, &quot;stdout&quot;: &quot;&quot;, &quot;stdout_lines&quot;: [] } 照例查看一下文件内容： [root@server ~]# ansible web -m shell -a 'cat /tmp/disk_total.log' 192.168.37.122 | SUCCESS | rc=0 >> Tue Dec 5 15:58:21 CST 2017 Filesystem Size Used Avail Use% Mounted on /dev/sda2 47G 4.4G 43G 10% / devtmpfs 978M 0 978M 0% /dev tmpfs 993M 84K 993M 1% /dev/shm tmpfs 993M 9.1M 984M 1% /run tmpfs 993M 0 993M 0% /sys/fs/cgroup /dev/sda3 47G 33M 47G 1% /app /dev/sda1 950M 153M 798M 17% /boot tmpfs 199M 16K 199M 1% /run/user/42 tmpfs 199M 0 199M 0% /run/user/0 192.168.37.133 | SUCCESS | rc=0 >> Tue Dec 5 15:58:21 CST 2017 Filesystem Size Used Avail Use% Mounted on /dev/sda2 46G 4.1G 40G 10% / devtmpfs 898M 0 898M 0% /dev tmpfs 912M 84K 912M 1% /dev/shm tmpfs 912M 9.0M 903M 1% /run tmpfs 912M 0 912M 0% /sys/fs/cgroup /dev/sda3 3.7G 15M 3.4G 1% /app /dev/sda1 1.9G 141M 1.6G 9% /boot tmpfs 183M 16K 183M 1% /run/user/42 tmpfs 183M 0 183M 0% /run/user/0 可以看出已经执行成功了。 13、setup 模块 该模块主要用于收集信息，是通过调用facts组件来实现的。 facts组件是Ansible用于采集被管机器设备信息的一个功能，我们可以使用setup模块查机器的所有facts信息，可以使用filter来查看指定信息。整个facts信息被包装在一个JSON格式的数据结构中，ansible_facts是最上层的值。 facts就是变量，内建变量 。每个主机的各种信息，cpu颗数、内存大小等。会存在facts中的某个变量中。调用后返回很多对应主机的信息，在后面的操作中可以根据不同的信息来做不同的操作。如redhat系列用yum安装，而debian系列用apt来安装软件。① 查看信息 我们可以直接用命令获取到变量的值，具体我们来看看例子： [root@server ~]# ansible web -m setup -a &#39;filter=&quot;*mem*&quot;&#39; #查看内存 192.168.37.122 | SUCCESS =&gt; { &quot;ansible_facts&quot;: { &quot;ansible_memfree_mb&quot;: 1116, &quot;ansible_memory_mb&quot;: { &quot;nocache&quot;: { &quot;free&quot;: 1397, &quot;used&quot;: 587 }, &quot;real&quot;: { &quot;free&quot;: 1116, &quot;total&quot;: 1984, &quot;used&quot;: 868 }, &quot;swap&quot;: { &quot;cached&quot;: 0, &quot;free&quot;: 3813, &quot;total&quot;: 3813, &quot;used&quot;: 0 } }, &quot;ansible_memtotal_mb&quot;: 1984 }, &quot;changed&quot;: false } 192.168.37.133 | SUCCESS =&gt; { &quot;ansible_facts&quot;: { &quot;ansible_memfree_mb&quot;: 1203, &quot;ansible_memory_mb&quot;: { &quot;nocache&quot;: { &quot;free&quot;: 1470, &quot;used&quot;: 353 }, &quot;real&quot;: { &quot;free&quot;: 1203, &quot;total&quot;: 1823, &quot;used&quot;: 620 }, &quot;swap&quot;: { &quot;cached&quot;: 0, &quot;free&quot;: 3813, &quot;total&quot;: 3813, &quot;used&quot;: 0 } }, &quot;ansible_memtotal_mb&quot;: 1823 }, &quot;changed&quot;: false } 我们可以通过命令查看一下内存的大小以确认一下是否一致： [root@server ~]# ansible web -m shell -a &#39;free -m&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; total used free shared buff/cache available Mem: 1984 404 1122 9 457 1346 Swap: 3813 0 3813 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; total used free shared buff/cache available Mem: 1823 292 1207 9 323 1351 Swap: 3813 0 3813 可以看出信息是一致的。② 保存信息 我们的setup模块还有一个很好用的功能就是可以保存我们所筛选的信息至我们的主机上，同时，文件名为我们被管制的主机的IP，这样方便我们知道是哪台机器出的问题。 我们可以看一看例子： [root@server tmp]# ansible web -m setup -a &#39;filter=&quot;*mem*&quot;&#39; --tree /tmp/facts 192.168.37.122 | SUCCESS =&gt; { &quot;ansible_facts&quot;: { &quot;ansible_memfree_mb&quot;: 1115, &quot;ansible_memory_mb&quot;: { &quot;nocache&quot;: { &quot;free&quot;: 1396, &quot;used&quot;: 588 }, &quot;real&quot;: { &quot;free&quot;: 1115, &quot;total&quot;: 1984, &quot;used&quot;: 869 }, &quot;swap&quot;: { &quot;cached&quot;: 0, &quot;free&quot;: 3813, &quot;total&quot;: 3813, &quot;used&quot;: 0 } }, &quot;ansible_memtotal_mb&quot;: 1984 }, &quot;changed&quot;: false } 192.168.37.133 | SUCCESS =&gt; { &quot;ansible_facts&quot;: { &quot;ansible_memfree_mb&quot;: 1199, &quot;ansible_memory_mb&quot;: { &quot;nocache&quot;: { &quot;free&quot;: 1467, &quot;used&quot;: 356 }, &quot;real&quot;: { &quot;free&quot;: 1199, &quot;total&quot;: 1823, &quot;used&quot;: 624 }, &quot;swap&quot;: { &quot;cached&quot;: 0, &quot;free&quot;: 3813, &quot;total&quot;: 3813, &quot;used&quot;: 0 } }, &quot;ansible_memtotal_mb&quot;: 1823 }, &quot;changed&quot;: false } 然后我们可以去查看一下： [root@server ~]# cd /tmp/facts/ [root@server facts]# ls 192.168.37.122 192.168.37.133 [root@server facts]# cat 192.168.37.122 {&quot;ansible_facts&quot;: {&quot;ansible_memfree_mb&quot;: 1115, &quot;ansible_memory_mb&quot;: {&quot;nocache&quot;: {&quot;free&quot;: 1396, &quot;used&quot;: 588}, &quot;real&quot;: {&quot;free&quot;: 1115, &quot;total&quot;: 1984, &quot;used&quot;: 869}, &quot;swap&quot;: {&quot;cached&quot;: 0, &quot;free&quot;: 3813, &quot;total&quot;: 3813, &quot;used&quot;: 0}}, &quot;ansible_memtotal_mb&quot;: 1984}, &quot;changed&quot;: false}六、Ansible playbook 简介 playbook 是 ansible 用于配置，部署，和管理被控节点的剧本。 通过 playbook 的详细描述，执行其中的一系列 tasks ，可以让远端主机达到预期的状态。playbook 就像 Ansible 控制器给被控节点列出的的一系列 to-do-list ，而被控节点必须要完成。 也可以这么理解，playbook 字面意思，即剧本，现实中由演员按照剧本表演，在Ansible中，这次由计算机进行表演，由计算机安装，部署应用，提供对外服务，以及组织计算机处理各种各样的事情。 七、Ansible playbook使用场景 执行一些简单的任务，使用ad-hoc命令可以方便的解决问题，但是有时一个设施过于复杂，需要大量的操作时候，执行的ad-hoc命令是不适合的，这时最好使用playbook。 就像执行shell命令与写shell脚本一样，也可以理解为批处理任务，不过playbook有自己的语法格式。 使用playbook你可以方便的重用这些代码，可以移植到不同的机器上面，像函数一样，最大化的利用代码。在你使用Ansible的过程中，你也会发现，你所处理的大部分操作都是编写playbook。可以把常见的应用都编写成playbook，之后管理服务器会变得十分简单。 八、Ansible playbook格式1、格式简介 playbook由YMAL语言编写。YAML( /ˈjæməl/ )参考了其他多种语言，包括：XML、C语言、Python、Perl以及电子邮件格式RFC2822，Clark Evans在2001年5月在首次发表了这种语言，另外Ingy döt Net与OrenBen-Kiki也是这语言的共同设计者。 YMAL格式是类似于JSON的文件格式，便于人理解和阅读，同时便于书写。首先学习了解一下YMAL的格式，对我们后面书写playbook很有帮助。以下为playbook常用到的YMAL格式： 1、文件的第一行应该以 “—“ (三个连字符)开始，表明YMAL文件的开始。 2、在同一行中，#之后的内容表示注释，类似于shell，python和ruby。 3、YMAL中的列表元素以”-”开头然后紧跟着一个空格，后面为元素内容。 4、同一个列表中的元素应该保持相同的缩进。否则会被当做错误处理。 5、play中hosts，variables，roles，tasks等对象的表示方法都是键值中间以”:”分隔表示，”:”后面还要增加一个空格。 下面是一个举例： --- #安装与运行mysql服务 - hosts: node1 remote_user: root tasks: - name: install mysql-server package yum: name=mysql-server state=present - name: starting mysqld service service: name=mysql state=started 我们的文件名称应该以.yml结尾，像我们上面的例子就是mysql.yml。其中，有三个部分组成： host部分：使用 hosts 指示使用哪个主机或主机组来运行下面的 tasks ，每个 playbook 都必须指定 hosts ，hosts也可以使用通配符格式。主机或主机组在 inventory 清单中指定，可以使用系统默认的/etc/ansible/hosts，也可以自己编辑，在运行的时候加上-i选项，指定清单的位置即可。在运行清单文件的时候，–list-hosts选项会显示那些主机将会参与执行 task 的过程中。remote_user：指定远端主机中的哪个用户来登录远端系统，在远端系统执行 task 的用户，可以任意指定，也可以使用 sudo，但是用户必须要有执行相应 task 的权限。tasks：指定远端主机将要执行的一系列动作。tasks 的核心为 ansible 的模块，前面已经提到模块的用法。tasks 包含 name 和要执行的模块，name 是可选的，只是为了便于用户阅读，不过还是建议加上去，模块是必须的，同时也要给予模块相应的参数。 使用ansible-playbook运行playbook文件，得到如下输出信息，输出内容为JSON格式。并且由不同颜色组成，便于识别。一般而言| 绿色代表执行成功，系统保持原样| 黄色代表系统代表系统状态发生改变| 红色代表执行失败，显示错误输出 执行有三个步骤：1、收集facts 2、执行tasks 3、报告结果 2、核心元素 Playbook的核心元素： Hosts：主机组；Tasks：任务列表；Variables：变量，设置方式有四种；Templates：包含了模板语法的文本文件；Handlers：由特定条件触发的任务； 3、基本组件 Playbooks配置文件的基础组件： Hosts：运行指定任务的目标主机remote_user：在远程主机上执行任务的用户；sudo_user：tasks：任务列表 格式： tasks： – name: TASK_NAME module: arguments notify: HANDLER_NAME handlers: – name: HANDLER_NAME module: arguments 模块，模块参数： 格式： (1) action: module arguments (2) module: arguments 注意：shell和command模块后面直接跟命令，而非key=value类的参数列表； handlers：任务，在特定条件下触发；接收到其它任务的通知时被触发； (1) 某任务的状态在运行后为changed时，可通过“notify”通知给相应的handlers； (2) 任务可以通过“tags“打标签，而后可在ansible-playbook命令上使用-t指定进行调用； 举例① 定义playbook [root@server ~]# cd /etc/ansible [root@server ansible]# vim nginx.yml --- - hosts: web remote_user: root tasks: - name: install nginx yum: name=nginx state=present - name: copy nginx.conf copy: src=/tmp/nginx.conf dest=/etc/nginx/nginx.conf backup=yes notify: reload #当nginx.conf发生改变时，通知给相应的handlers tags: reloadnginx #打标签 - name: start nginx service service: name=nginx state=started tags: startnginx #打标签 handlers: #注意，前面没有-，是两个空格 - name: reload service: name=nginx state=restarted #为了在进程中能看出来② 测试运行结果 写完了以后，我们就可以运行了： [root@server ansible]# ansible-playbook nginx.yml 现在我们可以看看两台机器的端口是否开启： [root@server ansible]# ansible web -m shell -a &#39;ss -nutlp |grep nginx&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; tcp LISTEN 0 128 *:80 *:* users:((&quot;nginx&quot;,pid=8304,fd=6),(&quot;nginx&quot;,pid=8303,fd=6)) 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; tcp LISTEN 0 128 *:80 *:* users:((&quot;nginx&quot;,pid=9671,fd=6),(&quot;nginx&quot;,pid=9670,fd=6))③ 测试标签 我们在里面已经打上了一个标签，所以可以直接引用标签。但是我们需要先把服务关闭，再来运行剧本并引用标签： [root@server ansible]# ansible web -m shell -a &#39;systemctl stop nginx&#39; [root@server ansible]# ansible-playbook nginx.yml -t startnginx④ 测试notify 我们还做了一个notify，来测试一下： 首先，它的触发条件是配置文件被改变，所以我们去把配置文件中的端口改一下： [root@server ansible]# vim /tmp/nginx.conf listen 8080; 然后我们重新加载一下这个剧本： [root@server ansible]# ansible-playbook nginx.yml -t reloadnginx 发现我们执行的就是reload段以及我们定义的notify部分。 我们来看一看我们的端口号： 发现我们执行的就是reload段以及我们定义的notify部分。 我们来看一看我们的端口号： [root@server ansible]# ansible web -m shell -a &#39;ss -ntlp | grep nginx&#39; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; LISTEN 0 128 *:8080 *:* users:((&quot;nginx&quot;,pid=2097,fd=6),(&quot;nginx&quot;,pid=2096,fd=6)) 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; LISTEN 0 128 *:8080 *:* users:((&quot;nginx&quot;,pid=3061,fd=6),(&quot;nginx&quot;,pid=3060,fd=6)) 可以看出，我们的nginx端口已经变成了8080。 4、variables 部分 上文中，我们说到了variables是变量，有四种定义方法，现在我们就来说说这四种定义方法： ① facts ：可直接调用 上一篇中，我们有说到setup这个模块，这个模块就是通过调用facts组件来实现的。我们这里的variables也可以直接调用facts组件。 具体的facters我们可以使用setup模块来获取，然后直接放入我们的剧本中调用即可。 ansible_all_ipv4_addresses：仅显示ipv4的信息 ---&gt; [u&#39;192.168.95.143&#39;] ansible_eth0[&#39;ipv4&#39;][&#39;address&#39;]：仅显示ipv4的信息 ---&gt; eth0 的ip地址 ansible_devices：仅显示磁盘设备信息 ansible_distribution：显示是什么系统，例：centos,suse等 ansible_distribution_version：仅显示系统版本 ansible_machine：显示系统类型，例：32位，还是64位 ansible_eth0：仅显示eth0的信息 ansible_hostname：仅显示主机名 ansible_kernel：仅显示内核版本 ansible_lvm：显示lvm相关信息 ansible_memtotal_mb：显示系统总内存 ansible_memfree_mb：显示可用系统内存 ansible_memory_mb：详细显示内存情况 ansible_swaptotal_mb：显示总的swap内存 ansible_swapfree_mb：显示swap内存的可用内存 ansible_mounts：显示系统磁盘挂载情况 ansible_processor：显示cpu个数(具体显示每个cpu的型号) ansible_processor_vcpus：显示cpu个数(只显示总的个数) ansible_python_version：显示python版本例如：批量修改主机 host 文件 --- - hosts: web vars: IP: \"{{ ansible_eth0['ipv4']['address'] }}\" tasks: - name: 将原有的hosts文件备份 shell: mv /etc/hosts /etc/hosts_bak - name: 将ansible端的hosts复制到各自机器上 copy: src=/root/hosts dest=/etc/ owner=root group=root mode=0644 - name: 在新的hosts文件后面追加各自机器内网ip和hostname lineinfile: dest=/etc/hosts line=\"{{ IP }} {{ ansible_hostname }}\" ② 用户自定义变量 我们也可以直接使用用户自定义变量，想要自定义变量有以下两种方式： 通过命令行传入 ansible-playbook命令的命令行中的-e VARS, --extra-vars=VARS，这样就可以直接把自定义的变量传入。 在playbook中定义变量 我们也可以直接在playbook中定义我们的变量： vars: - var1: value1 - var2: value2举例① 定义剧本 我们就使用全局替换把我们刚刚编辑的文件修改一下： [root@server ansible]# vim nginx.yml 这样一来，我们的剧本就定义完成了。② 拷贝配置文件 我们想要在被监管的机器上安装什么服务的话，就直接在我们的server端上把该服务的配置文件拷贝到我们的/tmp/目录下。这样我们的剧本才能正常运行。 我们就以keepalived服务为例： [root@server ansible]# cp /etc/keepalived/keepalived.conf /tmp/keepalived.conf ③ 运行剧本，变量由命令行传入 [root@server ansible]# ansible-playbook nginx.yml -e rpmname=keepalived④ 修改剧本，直接定义变量 同样的，我们可以直接在剧本中把变量定义好，这样就不需要在通过命令行传入了。以后想要安装不同的服务，直接在剧本里把变量修改一下即可。 [root@server ansible]# vim nginx.yml![img](E:/学习晋升文件汇总/Linux架构学习入门/4. network_manager/19-20天-企业自动化运维工具Aansible实战/assets/1204916-20171208112356562-1275040347.png)⑤ 运行定义过变量的剧本 我们刚刚已经把变量定义在剧本里面了。现在我们来运行一下试试看： [root@server ansible]# ansible-playbook nginx.yml 发现这样也是可以的~ ③ 通过roles传递变量 具体的，我们下文中说到 roles 的时候再详细说明。 ④ Host Inventory 我们也可以直接在主机清单中定义。 定义的方法如下： 向不同的主机传递不同的变量： IP/HOSTNAME varaiable=value var2=value2 向组中的主机传递相同的变量： [groupname:vars] variable=valueAnsible Inventory 内置参数 ![Ansible Inventory 内置参数](E:/学习晋升文件汇总/Linux架构学习入门/4. network_manager/19-20天-企业自动化运维工具Aansible实战/assets/Ansible Inventory 内置参数.png) 使用内置变量把用户名密码写在Inventory中，也就是/etc/ansible/hosts文件里，缺点就是暴露了账号密码，不安全。如果有多个主机需要使用同样的变量，可以用组变量的形式，书写格式如下： [web] 192.168.100.10 192.168.100.11 192.168.100.12 [web:vars] #给名为webservers的组定义一个变量，:vars是固定格式 ansible_ssh_port=22 ansible_ssh_user='root' ansible_ssh_pass='1234.com' 5、模板 templates 模板是一个文本文件，嵌套有脚本（使用模板编程语言编写）。 Jinja2：Jinja2是python的一种模板语言，以Django的模板语言为原本。模板支持： 字符串：使用单引号或双引号； 数字：整数，浮点数； 列表：[item1, item2, ...] 元组：(item1, item2, ...) 字典：{key1:value1, key2:value2, ...} 布尔型：true/false 算术运算： +, -, *, /, //, %, ** 比较操作： ==, !=, &gt;, &gt;=, &lt;, &lt;= 逻辑运算： and, or, not 通常来说，模板都是通过引用变量来运用的。 举例① 定义模板 我们直接把之前定义的/tmp/nginx.conf改个名，然后编辑一下，就可以定义成我们的模板文件了： [root@server ansible]# cd /tmp [root@server tmp]# mv nginx.conf nginx.conf.j2 [root@server tmp]# vim nginx.conf.j2 worker_processes {{ ansible_processor_vcpus }}; listen {{ nginxport }};② 修改剧本 我们现在需要去修改剧本来定义变量： [root@server ansible]# vim nginx.yml 需要修改的部分如图所示。 copy 也需要修改为 template ③ 运行剧本 上面的准备工作完成后，我们就可以去运行剧本了： [root@server ansible]# ansible-playbook nginx.yml -t reloadnginx PLAY [web] ********************************************************************* TASK [setup] ******************************************************************* ok: [192.168.37.122] ok: [192.168.37.133] TASK [copy nginx.conf] ********************************************************* ok: [192.168.37.122] ok: [192.168.37.133] PLAY RECAP ********************************************************************* 192.168.37.122 : ok=2 changed=0 unreachable=0 failed=0 192.168.37.133 : ok=2 changed=0 unreachable=0 failed=0 6、条件测试when语句：在task中使用，jinja2的语法格式。举例如下： tasks: - name: install conf file to centos7 template: src=files/nginx.conf.c7.j2 when: ansible_distribution_major_version == &quot;7&quot; - name: install conf file to centos6 template: src=files/nginx.conf.c6.j2 when: ansible_distribution_major_version == &quot;6&quot;循环：迭代，需要重复执行的任务； 对迭代项的引用，固定变量名为”item”，而后，要在task中使用with_items给定要迭代的元素列表；举例如下： tasks: - name: unstall web packages yum: name={{ item }} state=absent with_items: - httpd - php - php-mysql7、字典 ansible playbook 还支持字典功能。举例如下： - name: install some packages yum: name={{ item }} state=present with_items: - nginx - memcached - php-fpm - name: add some groups group: name={{ item }} state=present with_items: - group11 - group12 - group13 - name: add some users user: name={{ item.name }} group={{ item.group }} state=present with_items: - { name: &#39;user11&#39;, group: &#39;group11&#39; } - { name: &#39;user12&#39;, group: &#39;group12&#39; } - { name: &#39;user13&#39;, group: &#39;group13&#39; }8、角色订制：roles① 简介 对于以上所有的方式有个弊端就是无法实现复用假设在同时部署Web、db、ha 时或不同服务器组合不同的应用就需要写多个yml文件。很难实现灵活的调用。 roles 用于层次性、结构化地组织playbook。roles 能够根据层次型结构自动装载变量文件、tasks以及handlers等。要使用roles只需要在playbook中使用include指令即可。简单来讲，roles就是通过分别将变量(vars)、文件(file)、任务(tasks)、模块(modules)及处理器(handlers)放置于单独的目录中，并可以便捷地include它们的一种机制。角色一般用于基于主机构建服务的场景中，但也可以是用于构建守护进程等场景中。 ② 角色集合角色集合：roles/mysql/httpd/nginx/files/：存储由copy或script等模块调用的文件；tasks/：此目录中至少应该有一个名为main.yml的文件，用于定义各task；其它的文件需要由main.yml进行“包含”调用；handlers/：此目录中至少应该有一个名为main.yml的文件，用于定义各handler；其它的文件需要由main.yml进行“包含”调用；vars/：此目录中至少应该有一个名为main.yml的文件，用于定义各variable；其它的文件需要由main.yml进行“包含”调用；templates/：存储由template模块调用的模板文本；meta/：此目录中至少应该有一个名为main.yml的文件，定义当前角色的特殊设定及其依赖关系；其它的文件需要由main.yml进行“包含”调用；default/：此目录中至少应该有一个名为main.yml的文件，用于设定默认变量； ③ 角色定制实例1. 在roles目录下生成对应的目录结构 [root@server ansible]# cd roles/ [root@server roles]# ls [root@server roles]# mkdir -pv ./{nginx,mysql,httpd}/{files,templates,vars,tasks,handlers,meta,default} [root@server roles]# tree . ├── httpd │ ├── default │ ├── files │ ├── handlers │ ├── meta │ ├── tasks │ ├── templates │ └── vars ├── mysql │ ├── default │ ├── files │ ├── handlers │ ├── meta │ ├── tasks │ ├── templates │ └── vars └── nginx ├── default ├── files ├── handlers ├── meta ├── tasks ├── templates └── vars 24 directories, 0 files2. 定义配置文件 我们需要修改的配置文件为/tasks/main.yml，下面，我们就来修改一下： [root@server roles]# vim nginx/tasks/main.yml - name: cp copy: src=nginx-1.10.2-1.el7.ngx.x86_64.rpm dest=/tmp/nginx-1.10.2-1.el7.ngx.x86_64.rpm - name: install yum: name=/tmp/nginx-1.10.2-1.el7.ngx.x86_64.rpm state=latest - name: conf template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf tags: nginxconf notify: new conf to reload - name: start service service: name=nginx state=started enabled=true3. 放置我们所需要的文件到指定目录 因为我们定义的角色已经有了新的组成方式，所以我们需要把文件都放到指定的位置，这样，才能让配置文件找到这些并进行加载。 rpm包放在files目录下，模板放在templates目录下： [root@server nginx]# cp /tmp/nginx-1.10.2-1.el7.ngx.x86_64.rpm ./files/ [root@server nginx]# cp /tmp/nginx.conf.j2 ./templates/ [root@server nginx]# tree . ├── default ├── files │ └── nginx-1.10.2-1.el7.ngx.x86_64.rpm ├── handlers ├── meta ├── tasks │ └── main.yml ├── templates │ └── nginx.conf.j2 └── vars 7 directories, 3 files4. 修改变量文件 我们在模板中定义的变量，也要去配置文件中加上： [root@server nginx]# vim vars/main.yml nginxprot: 99995. 定义handlers文件 我们在配置文件中定义了notify，所以我么也需要定义handlers，我们来修改配置文件： [root@server nginx]# vim handlers/main.yml - name: new conf to reload service: name=nginx state=restarted6. 定义剧本文件 接下来，我们就来定义剧本文件，由于大部分设置我们都单独配置在了roles里面，所以，接下来剧本就只需要写一点点内容即可： [root@server ansible]# vim roles.yml - hosts: web remote_user: root roles: - nginx7. 启动服务 剧本定义完成以后，我们就可以来启动服务了： [root@server ansible]# ansible-playbook roles.yml PLAY [web] ********************************************************************* TASK [setup] ******************************************************************* ok: [192.168.37.122] ok: [192.168.37.133] TASK [nginx : cp] ************************************************************** ok: [192.168.37.122] ok: [192.168.37.133] TASK [nginx : install] ********************************************************* changed: [192.168.37.122] changed: [192.168.37.133] TASK [nginx : conf] ************************************************************ changed: [192.168.37.122] changed: [192.168.37.133] TASK [nginx : start service] *************************************************** changed: [192.168.37.122] changed: [192.168.37.133] RUNNING HANDLER [nginx : new conf to reload] *********************************** changed: [192.168.37.122] changed: [192.168.37.133] PLAY RECAP ********************************************************************* 192.168.37.122 : ok=6 changed=4 unreachable=0 failed=0 192.168.37.133 : ok=6 changed=4 unreachable=0 failed=0 启动过后照例查看端口号： [root@server ansible]# ansible web -m shell -a &quot;ss -ntulp |grep 9999&quot; 192.168.37.122 | SUCCESS | rc=0 &gt;&gt; tcp LISTEN 0 128 *:9999 *:* users:((&quot;nginx&quot;,pid=7831,fd=6),(&quot;nginx&quot;,pid=7830,fd=6),(&quot;nginx&quot;,pid=7829,fd=6)) 192.168.37.133 | SUCCESS | rc=0 &gt;&gt; tcp LISTEN 0 128 *:9999 *:* users:((&quot;nginx&quot;,pid=9654,fd=6),(&quot;nginx&quot;,pid=9653,fd=6),(&quot;nginx&quot;,pid=9652,fd=6)) 可以看出我们的剧本已经执行成功。 九、Ansible使用jinja2管理配置文件以及jinja2语法简介1、Jinja2介绍Jinja2是基于python的模板引擎，功能比较类似于PHP的smarty，J2ee的Freemarker和velocity。它能完全支持unicode，并具有集成的沙箱执行环境，应用广泛。jinja2使用BSD授权 Jinja2的语法是由variables(变量)和statement(语句)组成，如下； 1、variables：可以输出数据 my_variables {{ some_dudes_name | capitalize }} 2、statements: 可以用来创建条件和循环等if语句： {% if my_conditional %} ... {% endif %} for 语句： {% for item in all_items %} `item` …… {% endfor %} 从上面第二个variables的例子中可以看出，jinja2支持使用带过滤器的Unix型管道操作符，有很多的内置过滤器可供使用。我们可以仅仅用一堆简单if和for就可以建立几乎任何的常规配置文件，不过如果你有意更进一步，jinja2 documentation （http://jinja.pocoo.org/docs/dev/）包含了很多有趣的东西可供了解。我们可以看到ansible允许在模板中使用诸如绘制时间此类的一些额外的模板变量 第一个例子：引用变量 #cd roles/template/ . ├── meta │ └── main.yml ├── tasks │ ├── template.yml │ └── main.yml ├── templates │ ├── order.j2 └── vars └── main.yml 总调度yml文件： #cat templates.yml --- - hosts: 10.0.90.27 user: root gather_facts: false roles: - role: template 注意:这里 - role: template 和 - template 是一样的！ 其他yml文件，如下： #cat tasks/main.yml - include: template.yml #cat tasks/template.yml - name: create {{ PROJECT }} directory file: dest=/data/{{ PROJECT }} state=directory - name: template transfor java dir template: src=order.j2 dest=/data/{{ PROJECT }}/order.conf #cat templates/order.j2 project: {{ PROJECT }} switch: {{ SWITCH }} dbport: {{ DBPORT }} #cat vars/main.yml PROJECT: \"JAVA\" SWITCH: \"ON\" DBPORT: \"8080\" 测试： # ansible-playbook templates.yml --syntax-check playbook: templates.yml 执行： # ansible-playbook templates.yml PLAY [10.0.90.27] ************************************************************** TASK [template : include] *************************************************** included: /etc/ansible/roles/template/tasks/template.yml for 10.0.90.27 TASK [template : create JAVA directory] ************************************* changed: [10.0.90.27] TASK [template : template transfor java dir] ******************************** changed: [10.0.90.27] PLAY RECAP ********************************************************************* 10.0.90.27 : ok=3 changed=2 unreachable=0 failed=0 #到10.0.90.27查看结果 #cat /data/JAVA/order.conf project: JAVA switch: ON dbport: 8080 第二个例子：for 语句 为远程主机生成服务器列表，加入该列表从192.168.13.201 web01.test.com 到192.168.13.211 web11.test.com 结束，如果手动添加就很不科学了，这里需要使用jinja2语法的for循环通过模板批量生成对应的配置文件，如下： ansible目录结构： #cd /etc/ansible/roles/test_hosts . ├── meta │ └── main.yml ├── tasks │ ├── file1.yml │ └── main.yml ├── templates │ └── test1.j2 └── vars └── main.yml 各个目录下yml文件内容： # cat tasks/file1.yml - name: ansible jinja2 template for hosts config template: src=test1.j2 dest=/etc/httpd/conf/httpd.conf.test # cat tasks/main.yml - include: file1.yml # cat templates/test1.j2 {% for id in range(201,212) %} 192.168.13.{{ id }} web{{ \"%03d\" |format(id-200) }}.test.com {% endfor %} 解释： {{ id }} 提取for循环中对应的变量id值 \"%02d\" 调用的是python内置的字符串格式化输出（%d格式化整数）因为是01,02这种格式，所以是保留2位，故用02 然后将结果通过管道符 “|” 传递给format 函数做二次处理。 执行结果： #cat httpd.conf.test 192.168.13.201 web01.test.com 192.168.13.202 web02.test.com 192.168.13.203 web03.test.com 192.168.13.204 web04.test.com 192.168.13.205 web05.test.com 192.168.13.206 web06.test.com 192.168.13.207 web07.test.com 192.168.13.208 web08.test.com 192.168.13.209 web09.test.com 192.168.13.210 web10.test.com 192.168.13.211 web11.test.com 第三个例子：if语句 说明：如果定义端口号，就绑定定义的端口号，如果不定义端口号，就绑定默认端口号 ansible目录结果 #cd /etc/ansible/roles/mysql_cnf #tree . ├── meta │ └── main.yml ├── tasks │ └── main.yml ├── templates │ └── test3.j2 └── vars 主要的yml文件是templates目录下面的test3.j2 # cat templates/test3.j2 {% if PORT %} bind_address=10.0.90.27:{{ PORT }} {% else %} bind_address=10.0.90.27:3306 {% endif %} playbook主文件 # cat jinj2_test.yml --- - hosts: 10.0.90.27 user: root gather_facts: false vars: PORT: 3136 tasks: - name: copy file to client template: src=/etc/ansible/roles/mysql_cnf/templates/test3.j2 dest=/root/my.cnf 执行： # ansible-playbook jinj2_test.yml PLAY [10.0.90.27] ************************************************************** TASK [copy file to client] ***************************************************** changed: [10.0.90.27] PLAY RECAP ********************************************************************* 10.0.90.27 : ok=1 changed=1 unreachable=0 failed=0 查看 # cat my.cnf bind_address=10.0.90.27:3136 如果将vars变量去掉，执行结果： # cat jinj2_test.yml --- - hosts: 10.0.90.27 user: root gather_facts: false vars: PORT: false tasks: - name: copy file to client template: src=/etc/ansible/roles/mysql_cnf/templates/test3.j2 dest=/root/my.cnf 查看： # cat my.cnf bind_address=10.0.90.27:3306 3、Jinja default()设定精通程序编码的朋友皆知，default()默认值的设定有助于程序的健壮性和简洁性。所幸Jinja也支持该功能，上面的例子中生成Mysql配置文件中的端口定义，如果指定则PORT=3136，否则PORT=3306，我们将该案例改造为使用default()试试 编辑/etc/ansible/roles/mysql_cnf/templates/test3.j2内容如下,这种方法更简介。 bind_address=10.0.90.27:3306 2、ansible使用jiaja2生成apache多主机配置1、创建目录，创建好之后如下：#cd /etc/ansible/roles/apache_conf # tree ./ ./ ├── meta │ └── main.yml ├── tasks │ ├── file.yml │ └── main.yml ├── templates │ └── apache.config.j2 └── vars └── main.yml 4 directories, 5 files 2、创建tasks调度文件，如下：#cat file.yml - name: ansible jinja2 template for apache config template: src=apache.config.j2 dest=/etc/httpd/conf/httpd.conf.template #cat main.yml - include: file.yml 3、创建apache的jinja2模板文件，如下：#cat apache.config.j2 NameVirtualHost *:80 {% for vhost in apache_vhost %} &lt;VirtualHost *:80> ServerName {{ vhost.servername }} DocumentRoot {{ vhost.documentroot }} {% if vhost.serveradmin is defined %} ServerAdmin {{ vhost.serveradmin }} {% endif %} &lt;Directory \"{{ vhost.documentroot }}\"> AllowOverride All Options -Indexes FollowSymLinks Order allow,deny Allow from all &lt;/Directory> &lt;/VirtualHost> {% endfor %} 4、创建变量，如下：#cat vars/main.yml apache_vhost: - {servername: \"apache.test1.com\", documentroot: \"/data/test1/\"} - {servername: \"apache.test2.com\", documentroot: \"/data/test2/\"} 5、创建总调度yml文件，如下：#cat /etc/ansible/apache_test.yml --- - hosts: 10.0.90.27 user: root gather_facts: no roles: - { role: apache_conf } 6、测试：#ansible-playbook apache_test.yml --syntax-check playbook: apache_test.yml 7、执行测试#ansible-playbook apache_test.yml PLAY [10.0.90.27] ************************************************************** TASK [apache_conf : include] *************************************************** included: /etc/ansible/roles/apache_conf/tasks/file.yml for 10.0.90.27 TASK [apache_conf : ansible jinja2 template for apache config] ***************** changed: [10.0.90.27] PLAY RECAP ********************************************************************* 10.0.90.27 : ok=2 changed=1 unreachable=0 failed=0 8、到客户端查看#cat httpd.conf.template NameVirtualHost *:80 &lt;VirtualHost *:80> ServerName apache.test1.com DocumentRoot /data/test1/ &lt;Directory \"/data/test1/\"> AllowOverride All Options -Indexes FollowSymLinks Order allow,deny Allow from all &lt;/Directory> &lt;/VirtualHost> &lt;VirtualHost *:80> ServerName apache.test2.com DocumentRoot /data/test2/ &lt;Directory \"/data/test2/\"> AllowOverride All Options -Indexes FollowSymLinks Order allow,deny Allow from all &lt;/Directory> &lt;/VirtualHost> 3、ansible使用jiaja2生成nginx一个模板多种不同配置说明：为2台Nginx Proxy，1台Nginx Web通过一套模板生成对应的配置 1、ansible目录结构：# cd roles/nginx_conf/ #tree . ├── files ├── meta │ └── main.yml ├── tasks │ ├── file.yml │ └── main.yml ├── templates │ └── nginx.conf.j2 └── vars └── main.yml 2、tasks目录下文件内容：#cat tasks/file.yml - name: nginx.j2 template transfer example template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf.template #cat tasks/main.yml - include: file.yml 3、nginx模板文件#cat templates/nginx.conf.j2 {% if nginx_use_proxy %} {% for proxy in nginx_proxies %} upstream {{ proxy.name }} #server 127.0.0.1:{{ proxy.port }}; server {{ ansible_eth0.ipv4.address }}:{{ proxy.port }}; } {% endfor %} {% endif%} server { listen 80; servername {{ nginx_server_name }}; access_log off; error_log /etc/nginx/nginx_error.log; rewrite ^ https://$server_name$request_uri? permanent; } server { listen 443 ssl; server_name {{ nginx_server_name }}; ssl_certificate /etc/nginx/ssl/{{ nginx_ssl_cert_name }}; ssl_certificate_key /etc/nginx/ssl/{{ nginx_ssl_cert_key }}; root {{ nginx_web_root }}; index index.html index.html; {% if nginx_use_auth %} auth_basic \"Restricted\"; auth_basic_user_file /etc/nginx/{{ project_name }}.htpasswd; {% endif %} {% if nginx_use_proxy %} {% for proxy in nginx_proxies %} location {{ proxy.location }} { proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-Proto http; proxy_set_header X-Url-Scheme $scheme; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_redirect off; proxy_pass http://{{ proxy.name }}; break; } {% endfor %} {% endif %} {% if nginx_server_static %} location / { try_files $url $url/ =404; } {% endif %} } 4、ansible变量文件cat vars/main.yml nginx_server_name: www.testnginx.com nginx_web_root: /data/html/ nginx_proxies: - name: suspicious location: / port: 1234 - name: suspicious-api location: /api port: 4567 5、ansible主playbook文件#cat nginx_test.yml ##The first roles - name: Nginx Proxy Server's Config Dynamic Create hosts: \"10.0.90.25:10.0.90.26\" remote_user: root vars: nginx_use_proxy: true nginx_ssl_cert_name: ifa.crt nginx_ssl_cert_key: ifa.key nginx_use_auth: true project_name: suspicious nginx_server_static: true gather_facts: true roles: - role: nginx_conf ##The second roles - name: Nginx WebServer's Config Dynamic Create hosts: 10.0.90.27 remote_user: root vars: nginx_use_proxy: false nginx_ssl_cert_name: ifa.crt nginx_ssl_cert_key: ifa.crt nginx_use_auth: false project_name: suspicious nginx_server_static: false gather_facts: false roles: - role: nginx_conf 6、测试并执行：#ansible-playbook nginx_test.yml --syntax-check playbook: nginx_test.yml 执行： # ansible-playbook nginx_test.yml PLAY [Nginx Proxy Server's Config Dynamic Create] ****************************** TASK [setup] ******************************************************************* ok: [10.0.90.25] ok: [10.0.90.26] TASK [nginx_conf : include] **************************************************** included: /etc/ansible/roles/nginx_conf/tasks/file.yml for 10.0.90.25, 10.0.90.26 TASK [nginx_conf : nginx.j2 template transfer example] ************************* changed: [10.0.90.26] changed: [10.0.90.25] PLAY [Nginx WebServer's Config Dynamic Create] ********************************* TASK [nginx_conf : include] **************************************************** included: /etc/ansible/roles/nginx_conf/tasks/file.yml for 10.0.90.27 TASK [nginx_conf : nginx.j2 template transfer example] ************************* changed: [10.0.90.27] PLAY RECAP ********************************************************************* 10.0.90.25 : ok=3 changed=1 unreachable=0 failed=0 10.0.90.26 : ok=3 changed=1 unreachable=0 failed=0 10.0.90.27 : ok=2 changed=1 unreachable=0 failed=0 7、查看检测执行结果到Nginx Proxy 服务器查看配置文件 #cat nginx.conf.template upstream suspicious #server 127.0.0.1:1234; server 10.0.90.26:1234; } upstream suspicious-api #server 127.0.0.1:4567; server 10.0.90.26:4567; } server { listen 80; servername www.testnginx.com; access_log off; error_log /etc/nginx/nginx_error.log; rewrite ^ https://$server_name$request_uri? permanent; } server { listen 443 ssl; server_name www.testnginx.com; ssl_certificate /etc/nginx/ssl/ifa.crt; ssl_certificate_key /etc/nginx/ssl/ifa.key; root /data/html/; index index.html index.html; auth_basic \"Restricted\"; auth_basic_user_file /etc/nginx/suspicious.htpasswd; location / { proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-Proto http; proxy_set_header X-Url-Scheme $scheme; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_redirect off; proxy_pass http://suspicious; break; } location /api { proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-Proto http; proxy_set_header X-Url-Scheme $scheme; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_redirect off; proxy_pass http://suspicious-api; break; } location / { try_files $url $url/ =404; } } 到Nginx Web 服务器上查看配置文件 #cat nginx.conf.template server { listen 80; servername www.testnginx.com; access_log off; error_log /etc/nginx/nginx_error.log; rewrite ^ https://$server_name$request_uri? permanent; } server { listen 443 ssl; server_name www.testnginx.com; ssl_certificate /etc/nginx/ssl/ifa.crt; ssl_certificate_key /etc/nginx/ssl/ifa.crt; root /data/html/; index index.html index.html; } 到这里，就结束了。用同样的模板通过简单的if和变量设置就可以完成不同类型主机的Nginx conf配置，所以一方面在了解Ansible强大的模板功能的同时，也需要看到模板质量的重要性。","categories":[{"name":"Tools","slug":"Tools","permalink":"https://cyylog.github.io/categories/Tools/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/tags/Linux/"},{"name":"Ansible","slug":"Ansible","permalink":"https://cyylog.github.io/tags/Ansible/"}]},{"title":"RabbitMQ消息中间件","slug":"SQL/RabbitMQ消息中间件","date":"2019-04-16T15:40:20.000Z","updated":"2020-05-25T13:57:22.799Z","comments":true,"path":"2019/04/16/sql/rabbitmq-xiao-xi-zhong-jian-jian/","link":"","permalink":"https://cyylog.github.io/2019/04/16/sql/rabbitmq-xiao-xi-zhong-jian-jian/","excerpt":"","text":"RabbitMQ 消息中间件1、消息中间件1、简介消息中间件也可以称消息队列，是指用高效可靠的消息传递机制进行与平台无关的数据交流，并基于数据通信来进行分布式系统的集成。通过提供消息传递和消息队列模型，可以在分布式环境下扩展进程的通信。 当下主流的消息中间件有RabbitMQ、Kafka、ActiveMQ、RocketMQ等。其能在不同平台之间进行通信，常用来屏蔽各种平台协议之间的特性，实现应用程序之间的协同。优点在于能够在客户端和服务器之间进行同步和异步的连接，并且在任何时刻都可以将消息进行传送和转发，是分布式系统中非常重要的组件，主要用来解决应用耦合、异步通信、流量削峰等问题。 2、作用1、消息中间件主要作用 解耦 冗余(存储) 扩展性 削峰 可恢复性 顺序保证 缓冲 异步通信 2、消息中间件的两种模式1、P2P模式P2P模式包含三个角色：消息队列（Queue）、发送者(Sender)、接收者(Receiver)。每个消息都被发送到一个特定的队列，接收者从队列中获取消息。队列保留着消息，直到它们被消费或超时。 P2P的特点： 每个消息只有一个消费者（Consumer），即一旦被消费，消息就不再在消息队列中 发送者和接收者之间在时间上没有依赖性，也就是说当发送者发送了消息之后，不管接收者有没有正在运行它不会影响到消息被发送到队列 接收者在成功接收消息之后需向队列应答成功 如果希望发送的每个消息都会被成功处理的话，那么需要P2P模 2、Pub/Sub模式Pub/Sub模式包含三个角色：主题（Topic）、发布者（Publisher）、订阅者（Subscriber） 。多个发布者将消息发送到Topic，系统将这些消息传递给多个订阅者。 Pub/Sub的特点： 每个消息可以有多个消费者 发布者和订阅者之间有时间上的依赖性。针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息 为了消费消息，订阅者必须保持运行的状态 如果希望发送的消息可以不被做任何处理、或者只被一个消息者处理、或者可以被多个消费者处理的话，那么可以采用Pub/Sub模型 3、常用中间件介绍与对比1、KafkaKafka是LinkedIn开源的分布式发布-订阅消息系统，目前归属于Apache顶级项目。Kafka主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输。0.8版本开始支持复制，不支持事务，对消息的重复、丢失、错误没有严格要求，适合产生大量数据的互联网服务的数据收集业务。 2、RabbitMQRabbitMQ是使用Erlang语言开发的开源消息队列系统，基于AMQP协议来实现。AMQP的主要特征是面向消息、队列、路由（包括点对点和发布/订阅）、可靠性、安全。AMQP协议更多用在企业系统内对数据一致性、稳定性和可靠性要求很高的场景，对性能和吞吐量的要求还在其次。 3、RocketMQRocketMQ是阿里开源的消息中间件，它是纯Java开发，具有高吞吐量、高可用性、适合大规模分布式系统应用的特点。RocketMQ思路起源于Kafka，但并不是Kafka的一个Copy，它对消息的可靠传输及事务性做了优化，目前在阿里集团被广泛应用于交易、充值、流计算、消息推送、日志流式处理、binglog分发等场景。 RabbitMQ比Kafka可靠，Kafka更适合IO高吞吐的处理，一般应用在大数据日志处理或对实时性（少量延迟），可靠性（少量丢数据）要求稍低的场景使用，比如ELK日志收集。 2、RabbitMQ 详解1、RabbitMQ 介绍RabbitMQ是一个在AMQP（Advanced Message Queuing Protocol ）基础上实现的，可复用的企业消息系统。它可以用于大型软件系统各个模块之间的高效通信，支持高并发，支持可扩展。它支持多种客户端如：Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP等，支持AJAX，持久化，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。 RabbitMQ是使用Erlang编写的一个开源的消息队列，本身支持很多的协议：AMQP，XMPP, SMTP, STOMP，也正是如此，使的它变的非常重量级，更适合于企业级的开发。它同时实现了一个Broker构架，这意味着消息在发送给客户端时先在中心队列排队，对路由(Routing)、负载均衡(Load balance)或者数据持久化都有很好的支持。 2、RabbitMQ 特点 可靠性 灵活的路由 扩展性 高可用性 多种协议 多语言客户端 管理界面 插件机制 3、AMQP 介绍AMQP，即Advanced Message Queuing Protocol,一个提供统一消息服务的应用层标准高级消息队列协议,是应用层协议的一个开放标准,为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同的开发语言等条件的限制。 4、什么和是消息队列MQ 全称为Message Queue, 消息队列。是一种应用程序对应用程序的通信方法。应用程序通过读写出入队列的消息（针对应用程序的数据）来通信，而无需专用连接来链接它们。 消息传递指的是程序之间通过在消息中发送数据进行通信，而不是通过直接调用彼此来通信。队列的使用除去了接收和发送应用程序同时执行的要求。 在项目中，将一些无需即时返回且耗时的操作提取出来，进行了异步处理，而这种异步处理的方式大大的节省了服务器的请求响应时间，从而提高了系统的吞吐量。 消息队列的使用场景是怎样的？ 5、RabbitMQ 应用场景对于一个大型的软件系统来说，它会有很多的组件或者说模块或者说子系统或者（subsystem or Component or submodule）。那么这些模块的如何通信？这和传统的IPC有很大的区别。传统的IPC很多都是在单一系统上的，模块耦合性很大，不适合扩展（Scalability）；如果使用socket那么不同的模块的确可以部署到不同的机器上，但是还是有很多问题需要解决。比如：1）信息的发送者和接收者如何维持这个连接，如果一方的连接中断，这期间的数据如何防止丢失？2）如何降低发送者和接收者的耦合度？3）如何让Priority高的接收者先接到数据？4）如何做到load balance？有效均衡接收者的负载？5）如何有效的将数据发送到相关的接收者？也就是说将接收者subscribe 不同的数据，如何做有效的filter。6）如何做到可扩展，甚至将这个通信模块发到cluster上？7）如何保证接收者接收到了完整，正确的数据？AMDQ协议解决了以上的问题，而RabbitMQ实现了AMQP。 6、RabbitMQ 概念介绍 Broker：简单来说就是消息队列服务器实体。 Exchange：消息交换机，它指定消息按什么规则，路由到哪个队列。 Queue：消息队列载体，每个消息都会被投入到一个或多个队列。 Binding：绑定，它的作用就是把exchange和queue按照路由规则绑定起来。 Routing Key：路由关键字，exchange根据这个关键字进行消息投递。 vhost：虚拟主机，一个broker里可以开设多个vhost，用作不同用户的权限分离。 producer：消息生产者，就是投递消息的程序。 consumer：消息消费者，就是接受消息的程序。 channel：消息通道，在客户端的每个连接里，可建立多个channel，每个channel代表一个会话任务。 RabbitMQ从整体上来看是一个典型的生产者消费者模型，主要负责接收、存储和转发消息 7、RabbitMQ 使用流程AMQP模型中，消息在producer中产生，发送到MQ的exchange上，exchange根据配置的路由方式发到相应的Queue上，Queue又将消息发送给consumer，消息从queue到consumer有push和pull两种方式。 消息队列的使用过程大概如下： 客户端连接到消息队列服务器，打开一个channel。 客户端声明一个exchange，并设置相关属性。 客户端声明一个queue，并设置相关属性。 客户端使用routing key，在exchange和queue之间建立好绑定关系。 客户端投递消息到exchange。 exchange接收到消息后，就根据消息的key和已经设置的binding，进行消息路由，将消息投递到一个或多个队列里。 exchange也有几个类型，完全根据key进行投递的叫做Direct交换机，例如，绑定时设置了routing key为”abc”，那么客户端提交的消息，只有设置了key为”abc”的才会投递到队列。 3、RabbitMQ 单机安装部署1、下载下载地址：http://www.rabbitmq.com/download.html 2、Windows上安装1、安装安装Erlang下载erlang：http://www.erlang.org/download/otp_win64_17.3.exe 安装： erlang安装完成。 2、安装安装RabbitMQ RabbitMQ安装完成。 启动、停止、重新安装等。 3、启用管理工具第一步：点击打开RabbitMQ的命令窗口。如图： 第二步：输入命令rabbitmq-plugins enable rabbitmq_management 这个命令的意思是安装RabbitMQ的插件。 第三步：测试是否安装成功。 方法：访问地址：http://127.0.0.1:15672/ 默认账号：guest/guest 3、Linux上安装1、安装 erlang添加yum支持 cd /usr/local/src/ mkdir rabbitmq cd rabbitmq wget http://packages.erlang-solutions.com/erlang-solutions-1.0-1.noarch.rpm rpm -ivh erlang-solutions-1.0-1.noarch.rpm rpm --import http://packages.erlang-solutions.com/rpm/erlang_solutions.asc yum install erlang2、安装RabbitMQ1、用 yum 安装 RabbitMQ rpm --import https://github.com/rabbitmq/signing-keys/releases/download/2.0/rabbitmq-release-signing-key.asc # this example assumes the CentOS 7 version of the package yum install rabbitmq-server-3.7.13-1.el7.noarch.rpm rpm --import https://www.rabbitmq.com/rabbitmq-release-signing-key.asc # this example assumes the CentOS 7 version of the package yum install rabbitmq-server-3.7.13-1.el7.noarch.rpm 2、用 rpm 手动安装 下载： wget https://github.com/rabbitmq/rabbitmq-server/releases/download/v3.7.13/rabbitmq-server-3.7.13-1.el7.noarch.rpm上传rabbitmq-server-3.7.13-1.el7.noarch.rpm文件到/usr/local/src/rabbitmq/ 安装： rpm -ivh rabbitmq-server-3.7.13-1.el7.noarch.rpm几个常用命令： service rabbitmq-server start service rabbitmq-server stop service rabbitmq-server restart chkconfig rabbitmq-server on //设置开机自启设置配置文件： cd /etc/rabbitmq cp /usr/share/doc/rabbitmq-server-3.7.13/rabbitmq.config.example /etc/rabbitmq/ mv rabbitmq.config.example rabbitmq.config设置用户远程访问： vim /etc/rabbitmq/rabbitmq.config 去掉后面的逗号 开启web界面管理工具 rabbitmq-plugins enable rabbitmq_management service rabbitmq-server restart防火墙开放15672端口(CentOS7 不用操作) /sbin/iptables -I INPUT -p tcp --dport 15672 -j ACCEPT /etc/rc.d/init.d/iptables save4、客户端的简单介绍1、界面的介绍 注意设置虚拟主机与添加用户这块。 命令行添加用户，设置tags rabbitmqctl list_users rabbitmqctl add_user username password rabbitmqctl set_user_tags username administrator关于虚拟主机，Virtual Host，其实是一个虚拟概念，类似于权限控制组，一个Virtual Host里面可以有若干个Exchange和Queue，但是权限控制的最小粒度是Virtual Host 用户角色有下面几种： 超级管理员(administrator) 可登陆管理控制台，可查看所有的信息，并且可以对用户，策略(policy)进行操作。 监控者(monitoring) 可登陆管理控制台，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等) 策略制定者(policymaker) 可登陆管理控制台, 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)。 普通管理者(management) 仅可登陆管理控制台，无法看到节点信息，也无法对策略进行管理。 其他 无法登陆管理控制台，通常就是普通的生产者和消费者。 4、Mac 安装教程1、安装在Mac下安装RabbitMQ是非常简单的，一般默认RabbitMQ服务器依赖的Erlang已经安装，只需要用下面两个命令就可以完成RabbitMQ的安装（前提是homebrew已经被安装）： brew update brew install rabbitmq耐心等待，安装完成后需要将/usr/local/sbin添加到$PATH，可以将下面这两行加到~/.bash_profile： # RabbitMQ Config export PATH=$PATH:/usr/local/sbin编辑完后:wq保存退出，使环境变量立即生效。 source ~/.bash_profile2、启动RabbitMQ服务上面配置完成后，需要关闭终端窗口，重新打开，然后输入下面命令即可启动RabbitMQ服务： rabbitmq-server3、登录Web管理界面浏览器输入localhost：15672,账号密码全输入guest即可登录。 5、RabbitMQ常用的命令1、基本命令启动监控管理器：rabbitmq-plugins enable rabbitmq_management关闭监控管理器：rabbitmq-plugins disable rabbitmq_management启动rabbitmq：rabbitmq-service start关闭rabbitmq：rabbitmq-service stop查看所有的队列：rabbitmqctl list_queues清除所有的队列：rabbitmqctl reset关闭应用：rabbitmqctl stop_app启动应用：rabbitmqctl start_app 2、用户和权限设置添加用户：rabbitmqctl add_user username password分配角色：rabbitmqctl set_user_tags username administrator新增虚拟主机：rabbitmqctl add_vhost vhost_name将新虚拟主机授权给新用户：rabbitmqctl set_permissions -p vhost_name username “.*” “.*” “.*”(后面三个”*”代表用户拥有配置、写、读全部权限) 3、角色说明 超级管理员(administrator)可登陆管理控制台，可查看所有的信息，并且可以对用户，策略(policy)进行操作。 监控者(monitoring)可登陆管理控制台，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等) 策略制定者(policymaker)可登陆管理控制台, 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)。 普通管理者(management)仅可登陆管理控制台，无法看到节点信息，也无法对策略进行管理。 其他无法登陆管理控制台，通常就是普通的生产者和消费者。 4、RabbitMQ 集群部署及配置消息中间件RabbitMQ，一般以集群方式部署，主要提供消息的接受和发送，实现各微服务之间的消息异步。以下将介绍RabbitMQ+HA方式进行部署。 1、原理介绍RabbitMQ是依据erlang的分布式特性（RabbitMQ底层是通过Erlang架构来实现的，所以rabbitmqctl会启动Erlang节点，并基于Erlang节点来使用Erlang系统连接RabbitMQ节点，在连接过程中需要正确的Erlang Cookie和节点名称，Erlang节点通过交换Erlang Cookie以获得认证）来实现的，所以部署Rabbitmq分布式集群时要先安装Erlang，并把其中一个服务的cookie复制到另外的节点。 RabbitMQ集群中，各个RabbitMQ为对等节点，即每个节点均提供给客户端连接，进行消息的接收和发送。节点分为内存节点和磁盘节点，一般的，均应建立为磁盘节点，为了防止机器重启后的消息消失； RabbitMQ的Cluster集群模式一般分为两种，普通模式和镜像模式。消息队列通过RabbitMQ HA镜像队列进行消息队列实体复制。 普通模式下，以两个节点（rabbit01、rabbit02）为例来进行说明。对于Queue来说，消息实体只存在于其中一个节点rabbit01（或者rabbit02），rabbit01和rabbit02两个节点仅有相同的元数据，即队列的结构。当消息进入rabbit01节点的Queue后，consumer从rabbit02节点消费时，RabbitMQ会临时在rabbit01、rabbit02间进行消息传输，把A中的消息实体取出并经过B发送给consumer。所以consumer应尽量连接每一个节点，从中取消息。即对于同一个逻辑队列，要在多个节点建立物理Queue。否则无论consumer连rabbit01或rabbit02，出口总在rabbit01，会产生瓶颈。 镜像模式下，将需要消费的队列变为镜像队列，存在于多个节点，这样就可以实现RabbitMQ的HA高可用性。作用就是消息实体会主动在镜像节点之间实现同步，而不是像普通模式那样，在consumer消费数据时临时读取。缺点就是，集群内部的同步通讯会占用大量的网络带宽。 2、部署 RabbitMQ Cluster多台机器部署RabbitMQ的cluster， 1、环境要求1、所有节点需要再同一个局域网内； 2、所有节点需要有相同的 erlang cookie，否则不能正常通信，为了实现cookie内容一致，采用scp的方式进行。 3、准备三台虚拟机，配置相同 rabbitmq01 192.168.101.11 rabbitmq02 192.168.101.12 rabbitmq03 192.168.101.13 操作系统：centos7.5 2、部署过程1、所有节点配置/etc/hostsnode1 192.168.101.11 node2 192.168.101.12 node3 192.168.101.13 2、所有节点安装 erLang 和 rabbitmq1、安装erlang 安装依赖包 yum install -y *epel* gcc-c++ unixODBC unixODBC-devel openssl-devel ncurses-devel编译安装 wget http://erlang.org/download/otp_src_21.3.tar.gz tar -zxvf otp_src_21.3.tar.gz cd otp_src_21.3 ./configure --prefix=/usr/local/bin/erlang --without-javac make &amp;&amp; make install echo &quot;export PATH=$PATH:/usr/local/bin/erlang/bin:/usr/local/bin/rabbitmq_server-3.6.15/sbin&quot; &gt;&gt; /etc/profile source /etc/profile出现 erl 命令则说明安装成功； 2、安装rabbitmq 编译安装 wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.15/rabbitmq-server-generic-unix-3.6.15.tar.xz yum install -y xz xz -d rabbitmq-server-generic-unix-3.6.15.tar.xz tar -xvf rabbitmq-server-generic-unix-3.6.15.tar -C /usr/local/bin/ echo &quot;export PATH=$PATH:/usr/local/bin/erlang/bin:/usr/local/bin/rabbitmq_server-3.6.15/sbin&quot; &gt;&gt; /etc/profile source /etc/profile3、导入 rabbitmq 的管理界面 rabbitmq-plugins enable rabbitmq_management4、设置 erlang 找到erlang cookie文件的位置，官方在介绍集群的文档中提到过.erlang.cookie 一般会存在这两个地址：第一个是home/.erlang.cookie；第二个地方就是/var/lib/rabbitmq/.erlang.cookie。如果我们使用解压缩方式安装部署的rabbitmq，那么这个文件会在{home}目录下，也就是$home/.erlang.cookie。如果我们使用rpm等安装包方式进行安装的，那么这个文件会在/var/lib/rabbitmq目录下。 这里将 node1 的该文件复制到 node2、node3，注意这个文件的权限是 400（默认即是400），因此采用scp的方式只拷贝内容即可； 可以通过cat $home/.erlang.cookie来查看三台机器的cookie是否一致，设置erlang的目的是要保证集群内的cookie内容一致。 使用-detached参数运行各节点 rabbitmqctl stop rabbitmq-server -detached然后可以通过 rabbitmqctl cluster_status查看节点状态。 注意：要先拷贝cookie到另外两台机器上，保证三台机器上的cookie是一致的，然后再启动服务。 由于guest这个用户,只能在本地访问,所以我们要新增一个用户并赋予权限: 添加用户并设置密码: rabbitmqctl add_user admin 123456添加权限（使admin用户对虚拟主机“/” 具有所有权限）: rabbitmqctl set_permissions -p &quot;/&quot; admin &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;修改用户角色（加入administrator用户组） rabbitmqctl set_user_tags admin administrator然后就可以远程访问了，然后可直接配置用户权限等信息。到此,就可以通过http://ip:15672 使用admin 123456 进行登陆了。 到这里的话，每个节点是作为单独的一台RabbitMQ存在的，也可以正常提供服务了 3、组成集群rabbitmq-server 启动时，会一起启动节点和应用，它预先设置RabbitMQ应用为standalone模式。要将一个节点加入到现有的集群中，你需要停止这个应用，并将节点设置为原始状态。如果使用./rabbitmqctl stop，应用和节点都将被关闭。所以使用rabbitmqctl stop_app仅仅关闭应用。 1、将 node2、node3与 node1 组成集群，这里以node2为例 node2# rabbitmqctl stop_app node2# rabbitmqctl join_cluster rabbit@node1 ####这里集群的名字一定不要写错了 node2# rabbitmqctl start_app2、将node3重复上述操作，也加入node1的集群。 node3# rabbitmqctl stop_app node3# rabbitmqctl join_cluster rabbit@node1 ####这里集群的名字一定不要写错了 node3# rabbitmqctl start_app则此时 node2 与 node3 也会自动建立连接，集群配置完毕； #使用内存节点加入集群 node2 # rabbitmqctl join_cluster --ram rabbit@node13、在 RabbitMQ 集群任意节点上执行 rabbitmqctl cluster_status来查看是否集群配置成功。 node3# rabbitmqctl cluster_status Cluster status of node rabbit@node3 ... [{nodes,[{disc,[rabbit@node1,rabbit@node2,rabbit@node3]}]}, {running_nodes,[rabbit@node1,rabbit@node2,rabbit@node3]}, {cluster_name,&lt;&quot;rabbit@node1&quot;&gt;}, #集群的名称默认为 rabbit@node1 {partitions,[]}, {alarms,[{rabbit@node1,[]},{rabbit@node2,[]},{rabbit@node3,[]}]}] 4、也可通过在web页面上的“Queues”的列表中，查看有如下显示为“同步镜像到node2”，则也表示集群配置成功 5、设置镜像队列策略 在任意一个节点上执行如下操作（这里在node1上执行） 首先，在web界面，登陆后，点击“Admin–Virtual Hosts（页面右侧）”，在打开的页面上的下方的“Add a new virtual host”处增加一个虚拟主机，同时给用户“admin”和“guest”均加上权限（在页面直接设置、点点点即可）； 然后，在linux中执行如下命令 rabbitmqctl set_policy -p coresystem ha-all &quot;^&quot; &#39;{&quot;ha-mode&quot;:&quot;all&quot;}&#39;“coresystem” vhost名称， “^”匹配所有的队列， ha-all 策略名称为ha-all, ‘{“ha-mode”:”all”}’ 策略模式为 all 即复制到所有节点，包含新增节点。 则此时镜像队列设置成功。（这里的虚拟主机coresystem是代码中需要用到的虚拟主机，虚拟主机的作用是做一个消息的隔离，本质上可认为是一个rabbitmq-server，是否增加虚拟主机，增加几个，这是由开发中的业务决定，即有哪几类服务，哪些服务用哪一个虚拟主机，这是一个规划）。 6、镜像队列策略设置说明 rabbitmqctl set_policy [-p Vhost] Name Pattern Definition [Priority] -p Vhost： 可选参数，针对指定vhost下的queue进行设置 Name: policy的名称 Pattern: queue的匹配模式(正则表达式) Definition：镜像定义，包括三个部分ha-mode, ha-params, ha-sync-mode ha-mode:指明镜像队列的模式，有效值为 all/exactly/nodes all：表示在集群中所有的节点上进行镜像 exactly：表示在指定个数的节点上进行镜像，节点的个数由ha-params指定 nodes：表示在指定的节点上进行镜像，节点名称通过ha-params指定 ha-params：ha-mode模式需要用到的参数 ha-sync-mode：进行队列中消息的同步方式，有效值为automatic和manual priority：可选参数，policy的优先级将所有队列设置为镜像队列，即队列会被复制到各个节点，各个节点状态保持一直。完成这 6 个步骤后，RabbitMQ 高可用集群搭建完成，最后一个步骤就是搭建均衡器。 7、安装并配置负载均衡器HA 注意：如果使用阿里云，可以使用阿里云的内网slb来实现负载均衡，不用自己搭建HA。 1、在192.168.101.11安装HAProxy yum -y install HAProxy2、修改 /etc/haproxy/haproxy.cfg vim /etc/haproxy/haproxy.cfg global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults log global mode tcp option tcplog option dontlognull retries 3 option redispatch maxconn 2000 contimeout 5s clitimeout 120s srvtimeout 120s listen rabbitmq_cluster 192.168.101.11:5670 mode tcp balance roundrobin server rabbit1 192.168.101.11:5672 check inter 5000 rise 2 fall 2 server rabbit2 192.168.101.12:5672 check inter 5000 rise 2 fall 2 3、重启HAProxy service haproxy restart登录浏览器输入地址http://192.168.101.11:8100/rabbitmqstats查看HAProxy的状态 三、常见问题 常见错误： 1、使用 rabbitmq-server -detached命令启动rabbitmq时，出现以下提示Warning: PID file not written; -detached was passed，此时使用rabbitmqctl status提示服务已启动，可知此问题不用解决。 2、由于更改hostname文件，在每次rabbitmqctl stop或者rabbitmqctl cluster_status等，只要是rabbitmq的命令就报错，提示大概如下 Cluster status of node rabbit@web2 ... Error: unable to connect to node rabbit@web2: nodedown DIAGNOSTICS =========== attempted to contact: [rabbit@web2] rabbit@web2: * connected to epmd (port 4369) on web2 * epmd reports node &#39;rabbit&#39; running on port 25672 * TCP connection succeeded but Erlang distribution failed * Hostname mismatch: node &quot;rabbit@mq2&quot; believes its host is different. Please ensure that hostnames resolve the same way locally and on &quot;rabbit@mq2&quot; current node details: - node name: &#39;rabbitmq-cli-11@web2&#39; - home dir: /root - cookie hash: SGwxMdJ3PjEXG1asIEFpBg==此时先ps aux | grep mq，然后kill -9 该进程，然后再rabbitmq-server -detached即可解决。（即先强杀，再重新启动） 3、使用rabbitmqctl stop，rabbitmq-server -detached重新启动后，原先添加的用户admin、虚拟主机coresystem等均丢失，还需要重新添加。 采用脚本启动，在脚本中写好启动好需要加载的各配置项（创建admin用户并授权，创建虚拟主机并授权，配置镜像队列）。 4、命令 rabbitmqctl stop_app #仅关闭应用，不关闭节点 rabbitmqctl start_app #开启应用 rabbitmq--server -detached #启动节点和应用 rabbitmqctl stop #关闭节点和应用4、常用命令： Rabbitmq服务器的主要通过rabbitmqctl和rabbimq-plugins两个工具来管理，以下是一些常用功能。 1、 服务器启动与关闭 启动: rabbitmq-server –detached 关闭: rabbitmqctl stop 若单机有多个实例，则在rabbitmqctl后加 –n 指定名称2、插件管理 开启某个插件：rabbitmq-plugins enable xxx 关闭某个插件：rabbitmq-plugins disable xxx 注意：重启服务器后生效。3、virtual_host管理 新建virtual_host:rabbitmqctl add_vhost xxx 撤销virtual_host:rabbitmqctl delete_vhost xxx 4、用户管理 新建用户：rabbitmqctl add_user xxxpwd 删除用户: rabbitmqctl delete_user xxx 查看用户：rabbitmqctl list_users 改密码: rabbimqctl change_password {username} {newpassword} 设置用户角色：rabbitmqctlset_user_tags {username} {tag ...} Tag可以为 administrator,monitoring, management 5、 权限管理 权限设置：set_permissions [-pvhostpath] {user} {conf} {write} {read} Vhostpath: Vhost路径 user: 用户名 Conf: 一个正则表达式match哪些配置资源能够被该用户访问。 Write: 一个正则表达式match哪些配置资源能够被该用户读。 Read: 一个正则表达式match哪些配置资源能够被该用户访问。6、获取服务器状态信息 服务器状态：rabbitmqctl status ##其中可查看rabbitmq的版本信息7、获取集群状态信息 rabbitmqctl cluster_status","categories":[{"name":"SQL","slug":"SQL","permalink":"https://cyylog.github.io/categories/SQL/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://cyylog.github.io/tags/RabbitMQ/"}]},{"title":"Nginx编译安装","slug":"Linux/Nginx/Nginx编译安装","date":"2019-03-27T15:14:08.000Z","updated":"2020-05-25T13:56:42.367Z","comments":true,"path":"2019/03/27/linux/nginx/nginx-bian-yi-an-zhuang/","link":"","permalink":"https://cyylog.github.io/2019/03/27/linux/nginx/nginx-bian-yi-an-zhuang/","excerpt":"","text":"nginx 编译安装与配置使用1、安装编译环境yum -y install gcc gcc-c++ 2、安装pcre软件包（使nginx支持http rewrite模块）yum install -y pcre pcre-devel 3、安装openssl-devel（使nginx支持ssl）yum install -y openssl openssl-devel 4、安装zlibyum install -y zlib zlib-devel 5、创建用户nginxuseradd nginx passwd nginx 6、安装nginx[root@localhost ～]#wget http://192.168.233.100/nginx.org/download/nginx-1.14.2.tar.gz [root@localhost ～]#tar -vzxf nginx-1.14.2.tar.gz -C /usr/local [root@localhost ～]#cd nginx-1.14.2/ [root@localhost nginx-1.14.2]# ./configure \\ --group=nginx \\ --user=nginx \\ --prefix=/usr/local/nginx \\ --sbin-path=/usr/sbin/nginx \\ --conf-path=/etc/nginx/nginx.conf \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --http-client-body-temp-path=/tmp/nginx/client_body \\ --http-proxy-temp-path=/tmp/nginx/proxy \\ --http-fastcgi-temp-path=/tmp/nginx/fastcgi \\ --pid-path=/var/run/nginx.pid \\ --lock-path=/var/lock/nginx \\ --with-http_stub_status_module \\ --with-http_ssl_module \\ --with-http_gzip_static_module \\ --with-pcre [root@localhost nginx-1.11.3]# make &&make install 7、Nginx 编译参数# 查看 nginx 安装的模块 [root@tianyun ~]# nginx -V # 模块参数具体功能 --with-cc-opt='-g -O2 -fPIE -fstack-protector //设置额外的参数将被添加到CFLAGS变量。（FreeBSD或者ubuntu使用） --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' --prefix=/usr/share/nginx //指向安装目录 --conf-path=/etc/nginx/nginx.conf //指定配置文件 --http-log-path=/var/log/nginx/access.log //指定访问日志 --error-log-path=/var/log/nginx/error.log //指定错误日志 --lock-path=/var/lock/nginx.lock //指定lock文件 --pid-path=/run/nginx.pid //指定pid文件 --http-client-body-temp-path=/var/lib/nginx/body //设定http客户端请求临时文件路径 --http-fastcgi-temp-path=/var/lib/nginx/fastcgi //设定http fastcgi临时文件路径 --http-proxy-temp-path=/var/lib/nginx/proxy //设定http代理临时文件路径 --http-scgi-temp-path=/var/lib/nginx/scgi //设定http scgi临时文件路径 --http-uwsgi-temp-path=/var/lib/nginx/uwsgi //设定http uwsgi临时文件路径 --with-debug //启用debug日志 --with-pcre-jit //编译PCRE包含“just-in-time compilation” --with-ipv6 //启用ipv6支持 --with-http_ssl_module //启用ssl支持 --with-http_stub_status_module //获取nginx自上次启动以来的状态 --with-http_realip_module //允许从请求标头更改客户端的IP地址值，默认为关 --with-http_auth_request_module //实现基于一个子请求的结果的客户端授权。如果该子请求返回的2xx响应代码，所述接入是允许的。如果它返回401或403中，访问被拒绝与相应的错误代码。由子请求返回的任何其他响应代码被认为是一个错误。 --with-http_addition_module //作为一个输出过滤器，支持不完全缓冲，分部分响应请求 --with-http_dav_module //增加PUT,DELETE,MKCOL：创建集合,COPY和MOVE方法 默认关闭，需编译开启 --with-http_geoip_module //使用预编译的MaxMind数据库解析客户端IP地址，得到变量值 --with-http_gunzip_module //它为不支持“gzip”编码方法的客户端解压具有“Content-Encoding: gzip”头的响应。 --with-http_gzip_static_module //在线实时压缩输出数据流 --with-http_image_filter_module //传输JPEG/GIF/PNG 图片的一个过滤器）（默认为不启用。gd库要用到） --with-http_spdy_module //SPDY可以缩短网页的加载时间 --with-http_sub_module //允许用一些其他文本替换nginx响应中的一些文本 --with-http_xslt_module //过滤转换XML请求 --with-mail //启用POP3/IMAP4/SMTP代理模块支持 --with-mail_ssl_module //启用ngx_mail_ssl_module支持启用外部模块支持 8、修改配置文件/etc/nginx/nginx.conf# 全局参数设置 worker_processes 1; #设置nginx启动进程的数量，一般设置成与逻辑cpu数量相同 error_log logs/error.log; #指定错误日志 worker_rlimit_nofile 102400; #设置一个nginx进程能打开的最大文件数 pid /var/run/nginx.pid; events { worker_connections 1024; #设置一个进程的最大并发连接数 } # http 服务相关设置 http { include mime.types; default_type application/octet-stream; log_format main 'remote_addr - remote_user [time_local] \"request\" ' 'status body_bytes_sent \"$http_referer\" ' '\"http_user_agent\" \"http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; #设置访问日志的位置和格式 sendfile on; #是否调用sendfile函数输出文件，一般设置为on，若nginx是用来进行磁盘IO负载应用时，可以设置为off，降低系统负载 gzip on; #是否开启gzip压缩 keepalive_timeout 65; #设置长连接的超时时间 # 虚拟服务器的相关设置 server { listen 80; #设置监听的端口 server_name localhost; #设置绑定的主机名、域名或ip地址 charset koi8-r; # 设置编码字符 location / { root /var/www/nginx; #设置服务器默认网站的根目录位置 index index.html index.htm; #设置默认打开的文档 } error_page 500 502 503 504 /50x.html; #设置错误信息返回页面 location = /50x.html { root html; #这里的绝对位置是/var/www/nginx/html } } } 9、检测 nginx 配置文件是否正确[root@localhost ~]#/usr/local/nginx/sbin/nginx -t 10、启动nginx服务/usr/local/nginx/sbin/nginx 11、通过 nginx 命令控制 nginx 服务nginx -c /path/to/nginx.conf # 以特定目录下的配置文件启动nginx: nginx -s reload # 修改配置后重新加载生效 nginx -s reopen # 重新打开日志文件 nginx -s stop # 快速停止nginx nginx -s quit # 完整有序的停止nginx nginx -t # 测试当前配置文件是否正确 nginx -t -c /path/to/nginx.conf # 测试特定的nginx配置文件是否正确 12、实现nginx开机自启 a、添加启动脚本 vim /etc/init.d/nginx #!/bin/sh # # nginx - this script starts and stops the nginx daemon # # chkconfig: - 85 15 # description: Nginx is an HTTP(S) server, HTTP(S) reverse \\ # proxy and IMAP/POP3 proxy server # processname: nginx # config: /etc/nginx/nginx.conf # config: /etc/sysconfig/nginx # pidfile: /var/run/nginx.pid # Source function library. . /etc/rc.d/init.d/functions # Source networking configuration. . /etc/sysconfig/network # Check that networking is up. [ \"$NETWORKING\" = \"no\" ] && exit 0 nginx=\"/usr/sbin/nginx\" prog=$(basename $nginx) NGINX_CONF_FILE=\"/etc/nginx/nginx.conf\" [ -f /etc/sysconfig/nginx ] && . /etc/sysconfig/nginx lockfile=/var/lock/subsys/nginx make_dirs() { # make required directories user=`nginx -V 2>&1 | grep \"configure arguments:\" | sed 's/[^*]*--user=\\([^ ]*\\).*/\\1/g' -` options=`$nginx -V 2>&1 | grep 'configure arguments:'` for opt in $options; do if [ `echo $opt | grep '.*-temp-path'` ]; then value=`echo $opt | cut -d \"=\" -f 2` if [ ! -d \"$value\" ]; then # echo \"creating\" $value mkdir -p $value && chown -R $user $value fi fi done } start() { [ -x $nginx ] || exit 5 [ -f $NGINX_CONF_FILE ] || exit 6 make_dirs echo -n $\"Starting $prog: \" daemon $nginx -c $NGINX_CONF_FILE retval=$? echo [ $retval -eq 0 ] && touch $lockfile return $retval } stop() { echo -n $\"Stopping $prog: \" killproc $prog -QUIT retval=$? echo [ $retval -eq 0 ] && rm -f $lockfile return $retval } restart() { configtest || return $? stop sleep 1 start } reload() { configtest || return $? echo -n $\"Reloading $prog: \" killproc $nginx -HUP RETVAL=$? echo } force_reload() { restart } configtest() { $nginx -t -c $NGINX_CONF_FILE } rh_status() { status $prog } rh_status_q() { rh_status >/dev/null 2>&1 } case \"$1\" in start) rh_status_q && exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart|configtest) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 ;; *) echo $\"Usage: $0 {start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest}\" exit 2 esac b、添加权限 chmod +x /etc/init.d/nginx c、重载系统启动文件 systemctl daemon-reload d、设置开机自启 systemctl start nginx 10、nginx 日志文件详解 ​ nginx 日志文件分为 log_format 和 access_log 两部分 ​ log_format 定义记录的格式，其语法格式为 ​ log_format 样式名称 样式详情 ​ 配置文件中默认有 log_format main &#39;remote_addr - remote_user [time_local] &quot;request&quot; &#39; &#39;status body_bytes_sent &quot;$http_referer&quot; &#39; &#39;&quot;http_user_agent&quot; &quot;http_x_forwarded_for&quot;&#39;; 点击这里 点击这里 变量 说明 $remote_addr和$http_x_forwarded_for 客户端的ip $remote_user 客户端的名称 $time_local 访问时的本地时间 $request 请求的URL和http协议 $status 访问的状态码 $body_bytes_sent 发送给客户端的主体内容大小 $http_referer 记录客户端是从哪个页面链接访问过来的，若没有链接，则访问‘-’ $http_user_agent 记录客户端使用的浏览器的相关信息","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://cyylog.github.io/tags/Nginx/"}]},{"title":"iptables笔记","slug":"Linux/iptables笔记","date":"2019-02-27T16:12:16.000Z","updated":"2020-09-28T14:51:02.524Z","comments":true,"path":"2019/02/28/linux/iptables-bi-ji/","link":"","permalink":"https://cyylog.github.io/2019/02/28/linux/iptables-bi-ji/","excerpt":"","text":"防火墙之 iptables1.1 安全优化配置原则尽可能不给服务器配置外网ip ,可以通过代理转发或者通过防火墙映射.并发不是特别大情况有外网ip,可以开启防火墙服务. 大并发的情况，不能开iptables,影响性能，利用硬件防火墙提升架构安全 1.1.1 生产中 iptables 的实际应用主要应用方向 1、主机防火墙（filter表的INPUT链）。 2、局域网共享上网(nat 表的 POSTROUTING 链)。半个路由器，NAT功能。 3、端口及IP映射(nat 表的 PREROUTING 链)，硬防的NAT功能。 4、IP一对一映射。 其他说明： ①iptables是基于内核的防火墙，功能非常强大，基于数据包的过滤！特别是可以在一台非常低的硬件配置下跑的非常好。 注：iptables主要工作在OSI七层的2.3.4层。七层的控制可以使用squid代理+iptables。 ②iptabes：生产中根据具体情况，一般，内网关闭，外网打开。大并发的情况不能开iptables，影响性能，iptables是要消耗CPU的，所以大并发的情况下，我们使用硬件防火墙的各方面做的很仔细。selinux：生产中也是关闭的。可以做ids的入侵检测。 ③实际生产中尽可能不给服务器配置外网IP。可以通过代理转发。比如，nagios就不需要外网。 ④并发不是很大的情况下，再外网的IP环境，开防火墙。 ⑤第一次直接默认规则生成配置文件，以后就在配置文件中进行修改（编辑添加删除）。 ⑥封掉IP：根据IP地址和网络连接数进行封杀。（定时任务，定时封掉，判断，存在就不再进行二次封杀） 1.1.2 常用案例功能小结：1）linux主机防火墙，单机作为防火墙（表filter）。 2）局域网共享上网（表nat postrouting）。 3）外部地址映射为内部地址和端口（表nat prerouting） 1.2 iptables 防火墙简介Netfilter/Iptables(以下简称Iptables)是unix/linux自带的一款优秀且开放源代码的完全自由的基于包过滤的防火墙工具，它的功能十分强大，使用非常灵活，可以对流入和流出服务器的数据包进行很精细的控制.特别是它可以在一台非常低的硬件配置服务器上跑的非常好（赛扬500HZ cpu 64M 内存的惲况下部署网关防火墙），提供近400人的上网服务丝毫不逊色专业路由器防火墙。 iptables + zebra + squid (常用网络开源产品）。 iptables是linux2.4及2.6内核中集成的服务，其功能与安全性比其老一蜚ipfwadm，ipchains 强大的多，iptables主要工作在0SI七层的二、三、四层，如果重新编译内核，iptables也可以支持 7 层控制（squid代理+iptables）。 1.2.1 iptables名词和术语不少刚接触到iptables的朋友可能会对iptables防火墙的相关名词搞的很晕，不知道其所云的具体意思，而是就最基本的能让大家容易快速理解和掌握的思路来描述： 容器：包含或者说属于的关系 1.2.2 什么是容器谁不知道啊，容器就是装东西的，如（箱、包、坛）。没错，恭喜你答对了.词典里解释说，容器就是用来包装或装载物品的贮存器（如箱、罐、坛）或者成形或柔软不成形的包覆材料. 在iptables里的呢，就是用来描述这种包含或者说属于的关系。 1.2.3 什么是 Netfilter/iptables ?Netfilter是表（tables）的容器，这样解释大家肯定还是晕。举个例子，如果把Netfilter看成是某个小区的一栋楼。那么表（tables)就是楼里的其中的一套房子。这套房子”表（tables)”属于这栋“Netfilter”。 1.2.4 什么是表（tables）？表（tables）是链的容器，即所有的链（chains）都属于其对应的表（tables）.如上，如果把Netfilter看成是某个小区的一栋楼.那么表（tables）就是楼里的其中的一套房子。 1.2.5 什么是链（chains）？链（chains）是规则（Policys）的容器。接上，如果把表（tables）当作有一套房子，那么链（chains）就可以说是房子里的家具（柜子等）。 1.2.6 什么是规则（Policy）？规则（Policy）就比较容易理解了，就是iptables系列过滤信息的规范和具体方法条款了.可以理解为柜子如何增加并摆放柜子东西等。 基本术语如下表格所示： Netfilter 表（tables**）** 链（chains**）** 规则（Policy**）** 一栋楼 楼里的房子 房子里的柜子 柜子里衣服，摆放规则 1.3 iptables 表和链描述完iptables术语后，相信大家对iptables的表和链有了初步的了解了，默认情况下，iptables根据功能和表的定义划分包含三个表，filter,nat,mangle,其每个表又包含不同的操作链（chains )。 实际iptables包含4张表和五个链,巧主要记住两张表即可filter和nat表即可。 下面表格展示了表和链的对应关系。 四个表： 表（tables**）** 链（chains**）** Filter 1 这是默认表，实现防火墙数据过滤功能。 1-INPUT 对于指定到本地套接字的包，即到达本地防火墙服务器的数据包。 1-FORWARD 路由穿过的数据包，即经过本地防火墙服务器的数据包。 1-OUTPUT 本地创建的数据包 NAT2 当遇到新创建的数据包连接时将参考这个表 2-FREROUTING 一进来就对数据包进行改变 2-OUTPUT 本地创建的数据包在路由前进行改变 2-POSTROUTING 在数据包即将出去时改变数据包信息 Mangle3 这个表专门用于改变数据包 3-INPUT 进入到设备本身的包 3-FORWARD 对路由后的数据包信息进行修改 3-FREROUTING 在路由之前更改传入的包 3-OUTPUT 本地创建的数据包在路由之前改变 3-POSTROUTING 在数据包即将离开时更改数据包信息 raw4 此表用处较少，可以忽略不计。This table is used mainly for configuring exemptions from connection tracking in combination with the NOTRACK target. 4-PREROUTING for packets arriving via any network interface 4-OUTPUT for packets generated by local processes 五个链 表（tables**）** 链（chains**）** INPUT FORWARD OUTPUT PREROUTING POSTROUTING Filter √ √ √ × × NAT × × √ √ √ Managle √ √ √ √ √ raw × × √ √ × 说明：√ 表示有，× 表示无。 图 - iptables中的表与链的结构关系 1.3.1 filter表的详细介绍 filter**表** 主要和主机自身相关，真正负责主机防火墙功能的（过滤流入流出主机的数据包）filter表是iptables默认使用的表，这个表定义了三个链（chains）工作场景:**主机防火墙** INPUT 负责过滤所有目标是本机地址的数据包通俗来说：就是过滤进入主机的数据包 FORWARD 负责转发流经主机的数据包。起到转发的作用，和NAT关系很大。LVS NAT 模式，net.ipv4.ip_forward=0 OUTPUT 处理所有源地址是本机地址的数据包通俗的讲：就是处理从主机发出的数据包 对于filter表的控制是我们实现本机防火墙功能的重要手段，特别是INPUT链的控制。 1.3.2 NAT表信息详细介绍 NAT表 负责网络地址转换的，即来源与目的的IP地址和port的转换。应用：和主机本身无关，一般用于局域网共享上网或者特殊的端口转换相关.工作场景：1、用于路由(zebra)或网关(iptables),共享上网(POSTROUTING)2、做内部外部IP地址一对一映射(dmz),硬件防火墙映射IP到内部服务器，FTP服务(PREROUTING)3、WEB,单个端口的映射，直接映射80端口(PREROUTING)这个表定义了3个链，nat功能相当于网络的acl控制。和网络交换机acl类似。 OUTPUT 和主机放出去的数据包有关，改变主机发出数据包的目的地址。 PREROUTING 在数据包到达防火墙时，进行路由判断之前执行的规则，作用是改变数据包的目的地址、目的端口等就是收信时，根据规则重写收件人的地址例如：把公网IP： xxx.xxx.xxx.xxx 映射到局域网的 x.x.x.x 服务器如果是web服务，可以把80转换为局域网的服务器9000端口上。 POSTROUTING 在数据包离开防火墙时进行路由判断之后执行的规则，作用改变数据包的源地址，源端口等。写好收件人的地址，要让家人回信时能够有地址可回。例如。默认笔记本和虚拟机都是局域网地址，在出网的时候被路由器将源地址改为公网地址。生产应用：局域网共享上网。 1.3.3 Mangle表信息详细介绍 Mangle表 主要负责修改数据包中特殊的路由标记，如TTL,TOS,MARK等，这个表定义了5个链(chains). 由于这个表与特殊标记相关，一般倩况下，我们用不到这个mangle表。 这里就不做详细介绍了。 1.4 iptables工作流程1.4.1 工作流程说明前面介绍已经提到，iptables是采用数据包过滤机制工作的，所以它会对请求的数据包的包头数据进行分析，并根据我们预先设定的规则进行匹配来决定是否可以进入主机。 iptables是采用数据包过滤机制工作的，所以它会对请求的数据包的包头数据进行分析，并根据我们预先设定的规则进行匹配来决定是否可以进入主机。 数据包的流向是从左向右的。 图 - iptables包处理流程图 图 - iptables包处理流程图(简化) 抽象说明：上图可以用北京地铁1,2号线来描述： 1号线：主要是NAT功能 案例： 1)局域网上网共享（路由和网关），使用NAT的POSTROUTING链。 2)外部IP和端口映射为内部IP和端口（DMZ功能），使用NAT的PREROUTING链 2号线：主要是FILTER功能，即防火墙功能FILTER INPUT FORWARD 案例： 主要应用就是主机服务器防火墙，使用FILTER的INPUT链 图 - iptables数据包转发流程图 1.4.2 iptables工作流程小结 1、防火墙是一层层过滤的。实际是按照配置规则的顺序从上到下，从前到后进行过滤的。 2、如果匹配上了规则，即明确表明是阻止还是通过，此时数据包就不在向下匹配新规则了。 3、如果所有规则中没有明确表明是阻止还是通过这个数据包，也就是没有匹配上规则，向下进行匹配，直到匹配默认规则得到明确的阻止还是通过。 4、防火墙的默认规则是对应链的所有的规则执行完以后才会执行的（最后执行的规则）。 1.5 iptables操作系统环境说明 [root@clsn ~]# cat /etc/redhat-release CentOS release 6.9 (Final) [root@clsn ~]# hostname -I 10.0.0.188 172.16.1.188软件版本 [root@clsn ~]# iptables -V iptables v1.4.71.5.1 iptables参数说明 参数 参数说明 显示相关参数 -n/–numeric 以数字的方式显示地址或端口信息 -L/ –list 列出一个链或所有链中的规则信息 –list-rules/-S Print the rules in a chain or all chains –line-number 当列出规则信息时，打印规则行号 -v 显示详细信息，可以叠加 -h 显示帮助信息 初始化相关参数 iptables -F 清除所有规则，不会处理默认的规则 iptables -X 删除用户自定义的链 iptables -Z 链的计数器清零（数据包计数器与数据包字节计数器） 配置常用参数 -t 表名称 指定配置哪个表，指定配置表名称。 –append/-A 链名称 附加或追加上相应规则策略，到指定链(链名称必须大写)，默认将配置的规则插入到最后一条。 –check/-C Check for the existence of a rule –insert/-I 链名称 插入相应规则策略，到指定链上，默认将配置的规则插入到第一条（可以根据规则序号插入到指定位置）–封IP地址使用。 –delete/-D 链名称 删除指定的规则(可以根据规则序号进行删除) –replace/-R Replace rule rulenum (1 = first) in chain -P(**大写)**链名称 改变链上的最终默认规则策略 –new/-N 创建新的用户定义链 -p 协议名称**[!] –proto** 指定规则的协议名称 all tcp udp icmp –dport 指定匹配的目标端口信息 –sport 指定匹配的源端口信息 -j 动作 匹配数据包后的动作 ACCEPT 允许 DROP 丢弃(没有响应) REJECT 拒绝(回应请求者明确的拒绝) MASQUERADE 伪装上网时使用 SNAT 共享地址上网 DNAT 目的地址改写 -i**[!] –in-interface** 在INPUT链配置规则中，指定从哪一个网卡接口进入的流量（只能配置在INPUT链上） -o**[!] –out-interface** 在OUTPUT链配置规则中，指定从哪一个网接口出去的流量（只能配置在OUTPUT链上） -s [!] –source 指定源IP地址或源网段信息 -d**[!] –destination** 指定目标IP地址或目标网段信息 扩展参数 -m 模块 表示增加扩展，匹配功能扩展匹配（可以加载扩展参数） multiport 实现不连续多端口扩展匹配 icmp 使用icmp的扩展 state 状态模块扩展 –icmp-type 只有类型8是真正会影响ping，或者也可以采用any；了解很多icmp类型iptables -p icmp -h –limit n/{second/minute/hour} 指定时间内的请求速率”n”为速率，后面为时间分别为：秒分 时 –limit-burst [n] 在同一时间内允许通过的请求”n”为数字，不指定默认为5 –exact/-x 扩展数字（显示精确数值） !**的使用实例** [root@clsn ~]# iptables ! -V Not 1.4.7 ;-) [root@clsn ~]# iptables -V iptables v1.4.7注意：在iptables中所有链名必须大写，表明必须小写，动作必须大写，匹配必须小写。 1.5.2 配置前准备在配置防火墙首先要其中防火墙 [root@clsn ~]# /etc/init.d/iptables start iptables: Applying firewall rules: [ OK ]清除iptables所有规则 [root@clsn ~]# iptables -Z [root@clsn ~]# iptables -X [root@clsn ~]# iptables -F查看iptables的规则 [root@clsn ~]# iptables -nvL Chain INPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination查看其他的表配置（-t 参数） [root@clsn ~]# iptables -nL -t raw Chain PREROUTING (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination查看配置规则的顺序号 [root@clsn ~]# iptables -nvL -line-number --line-number # 显示规则的序号1.6 iptables filter表配置实例1.6.1 基础配置配置实例一：配置22/ssh端口访问控制规则 iptables -A INPUT -p tcp --dprot 22 -j DROP # 禁止所有人访问22端口 iptables -I INPUT -p tcp --dprot 22 -j ACCEPT # 恢复连接方法 iptables -I INPUT 2 -p tcp --dprot 22 -j ACCEPT # 通过插入指定行号信息，指定将规则插入到第几行 iptables -D INPUT -p tcp --dport 22 -j ACCEPT # 删除指定规则 iptables -D INPUT 2 # 根据规则行号，删除相应的规则只允许10.0.0.1的ip通过ssh连接这台服务器 iptables -I INPUT -s 10.0.0.1 -p tcp --dport 22 -j ACCEPT 配置实例二：禁止网段连入（禁止172.16.1.0网段访问172.16.1.188） iptables -A INPUT -s 172.16.1.0/24 -d 172.16.1.188 -j DROP配置实例三：禁止某个172.16.1.0网段访问服务器主机的22端口 iptables -A INPUT -s 172.16.1.0/24 -d 172.16.1.188 -p tcp --dport 22 -j DROP方向说明： # 在入方向控制 iptables -I INPUT -i eth0 -p tcp --dport 22 -j ACCEPT # 在出方向控制 iptables -I OUTPUT -o eth0 -p tcp --sport 22 -j DROP1.6.2 配置实例四：除10.0.0.0网段可以进行连接服务器主机意外，其余网段都禁止 第一种方式： iptables -A INPUT -s 10.0.0.0/24 -d 172.16.1.8 -j ACCEPT 修改默认规则，将默认规则改为拒绝 第二种方式： ！ — 表示对规则信息进行取反 iptables -A INPUT ! -s 10.0.0.0/24 -d 172.16.1.8 -j DROP --- centos6用法 iptables -A INPUT -s ! 10.0.0.0/24 -d 172.16.1.8 -j DROP --- centos5用法说明：只有iptables帮助手册中指定的参数可以用取反符号（iptables –help） 1.6.3 配置实例五：测试匹配列举端口范围。iptables -A INPUT -p tcp --dport 22:80 -j DROP # 设置连续多端口控制策略 iptables -A INPUT -p tcp -m multiport --dport 22,80 -j DROP # 设置不连续多端口控制策略 -m 参数表示增加扩展匹配功能，multiport 实现不连续多端口扩展匹配 1.6.4 配置实例六：匹配ICMP类型 禁止ping策略原则 iptables服务器是ping命令发起者或是接受者 发起者： input链： 禁止icmp-type 0 0 Echo Reply——回显应答（Ping应答) iptables -A INPUT -i eth0 -p icmp --icmp-type 0 -j DROPoutput链： 禁止icmp-type 8 8 Echo request——回显请求（Ping请求） iptables -A OUTPUT -o eth0 -p icmp --icmp-type 8 -j DROP 接受者： input链： 禁止icmp-type 8 8 Echo request——回显请求（Ping请求） iptables -A INPUT -i eth0 -p icmp --icmp-type 8 -j DROP output链： 禁止icmp-type 0 0 Echo Reply——回显应答（Ping应答) iptables -A OUTPUT -o eth0 -p icmp --icmp-type 0 -j DROP简化配置： iptables -A INPUT -i eth0 -p icmp -m icmp --icmp-type any -j DROP #禁止所有类型的icmp 指定类型禁止icmp iptables -A INPUT -p icmp --icmp-type 8 iptables -A INPUT -p icmp --icmp-type 8 -j DROP iptables -A INPUT -p icmp -m icmp --icmp-type any -j ACCEPT iptables -A FORWARD -s 192.168.1.0/24 -p icmp -m icmp --icmp-type any -j ACCEPT 说明：只有类型8是真正会影响ping，或者也可以采用any；了解很多icmp类型iptables -p icmp -h ICMP**类型的说明** TYPE CODE Description Query Error 0 0 Echo Reply——回显应答（Ping应答） x 3 0 Network Unreachable——网络不可达 x 3 1 Host Unreachable——主机不可达 x 3 2 Protocol Unreachable——协议不可达 x 3 3 Port Unreachable——端口不可达 x 3 4 Fragmentation needed but no frag. bit set——需要进行分片但设置不分片比特 x 3 5 Source routing failed——源站选路失败 x 3 6 Destination network unknown——目的网络未知 x 3 7 Destination host unknown——目的主机未知 x 3 8 Source host isolated (obsolete)——源主机被隔离（作废不用） x 3 9 Destination network administratively prohibited——目的网络被强制禁止 x 3 10 Destination host administratively prohibited——目的主机被强制禁止 x 3 11 Network unreachable for TOS——由于服务类型TOS，网络不可达 x 3 12 Host unreachable for TOS——由于服务类型TOS，主机不可达 x 3 13 Communication administratively prohibited by filtering——由于过滤，通信被强制禁止 x 3 14 Host precedence violation——主机越权 x 3 15 Precedence cutoff in effect——优先中止生效 x 4 0 Source quench——源端被关闭（基本流控制） 5 0 Redirect for network——对网络重定向 5 1 Redirect for host——对主机重定向 5 2 Redirect for TOS and network——对服务类型和网络重定向 5 3 Redirect for TOS and host——对服务类型和主机重定向 8 0 Echo request——回显请求（Ping请求） x 9 0 Router advertisement——路由器通告 10 0 Route solicitation——路由器请求 11 0 TTL equals 0 during transit——传输期间生存时间为0 x 11 1 TTL equals 0 during reassembly——在数据报组装期间生存时间为0 x 12 0 IP header bad (catchall error)——坏的IP首部（包括各种差错） x 12 1 Required options missing——缺少必需的选项 x 13 0 Timestamp request (obsolete)——时间戳请求（作废不用） x 14 Timestamp reply (obsolete)——时间戳应答（作废不用） x 15 0 Information request (obsolete)——信息请求（作废不用） x 16 0 Information reply (obsolete)——信息应答（作废不用） x 17 0 Address mask request——地址掩码请求 x 18 0 Address mask reply——地址掩码应答 数据来源：http://www.cnitblog.com/yang55xiaoguang/articles/59581.html 1.6.5 防火墙状态机制配置状态集简单说明: 状态集 说明 NEW 表示新建立连接的数据包状态 ESTABLISHED 表示新建立连接数据包发送之后，回复响应的数据包状态 RELATED 表示借助已经建立的链路，发送新的连接数据包 INVALID 无效无法识别的数据包 注意：允许关联的状态包通过（web服务不要使用FTP服务） 防火墙服务配置在FTP服务器上时，需要配置以下策略 iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT iptables -A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT实现发现sent_syn状态 iptables -A INPUT -m state --state NEW -j DROP # 防火墙所连接客户端上配置实现发现sent_rcvd状态 iptables -I INPUT -i eth0 -s 10.0.0.201 -m state --state ESTABLISHED -j DROP # 防护墙上配置的1.6.6 使用iptables实现限速功能limit是iptables的一个匹配模块，用它结合iptables的其它命令可以实现限速的功能。 不过首先必须明确，limit本身只是一个“匹配”模块。我们知道，iptables的基本原理是“匹配–处理”，limit在这个工作过程中只能起到匹配的作用，它本身是无法对网络数据包进行任何处理的。我看到网上有些limit的例子里面说只 用一条包含limit匹配规则的iptables语句就可以实现限速，那是错误的。 实际上，利用imit来限速需要包括两个步骤: 1.对符合limit匹配规则包放行 2.丢弃/拒绝未放行的包 示例： iptables -I INPUT -s 10.0.0.7 -p icmp --icmp-type 8 -m limit --limit 6/min --limit-burst 5 -j ACCEPT iptables -I INPUT -s 10.0.0.7 -p icmp --icmp-type 8 -j DROP 语句含义：当来自10.0.0.7 的ping包超过5个时进行限速，限制为每10s一个。 参数说明： 参数 参数含义 –limit n/{second/minute/hour} 指定时间内的请求速率”n”为速率，后面为时间分别为：秒 分 时 –limit-burst [n] 在同一时间内允许通过的请求”n”为数字，不指定默认为5 limit 模块具体是如何工作的。？ limit的匹配是基于令牌桶 (Token bucket）模型的。 令牌桶是一种网络通讯中常见的缓冲区工作原理，它有两个重要的参数，令牌桶容量n和令牌产生速率s。 我们可以把令牌当成是门票，而令牌桶则是负责制作和发放门票的管理员，它手里最多有n张令牌。一开始，管理员开始手里有n张令牌。每当一个数据包到达后，管理员就看看手里是否还有可用的令牌。如果有，就把令牌发给这个数据包，limit就告诉iptables，这个数据包被匹配了。而当管理员把手上所有的令牌都发完了，再来的数据包就拿不到令牌了。这时，limit模块就告诉iptables，这个数据包不能被匹配。除了发放令牌之外，只要令牌桶中的令牌数量少于n，它就会以速率s来产生新的令牌，直到令牌数量到达n为止。 通过令牌桶机制，即可以有效的控制单位时间内通过（匹配）的数据包数量，又可以容许短时间内突发的大量数据包的通过（只要数据包数量不超过令牌桶n）。 limit模块提供了两个参数–limit和–limit-burst，分别对应于令牌产生速率和令牌桶容量。除了令牌桶模型外，limit匹配的另外一个重要概念是匹配项。在limit中，每个匹配项拥有一个单独的令牌桶，执行独立的匹配计算。 1.6.7防火墙配置清除防火墙规则 [root@clsn ~]# iptables -F [root@clsn ~]# iptables -X [root@clsn ~]# iptables -Z修改默认规则为拒绝（修改前先放行22端口，保证自己能够连上主机） [root@clsn ~]# iptables -A INPUT -p tcp --dport 22 -j ACCEPT [root@clsn ~]# iptables -P INPUT DROP [root@clsn ~]# iptables -P FORWARD DROP放行指定的端口 [root@clsn ~]# iptables -A INPUT -i lo -j ACCEPT [root@clsn ~]# iptables -A INPUT -p tcp -m multiport --dport 80,443 -j ACCEPT [root@clsn ~]# iptables -A INPUT -s 172.16.1.0/24 -j ACCEPT [root@clsn ~]# iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT #multiport多个不连续 21:25,80,443保存iptables**配置** \\01. 第一种方式 [root@clsn ~]# /etc/init.d/iptables save iptables: Saving firewall rules to /etc/sysconfig/iptables:[ OK ] [root@clsn ~]# cat /etc/sysconfig/iptables # Generated by iptables-save v1.4.7 on Tue Apr 4 12:24:43 2017 *filter :INPUT DROP [0:0] :FORWARD DROP [0:0] :OUTPUT ACCEPT [159:10664] -A INPUT -s 10.0.0.0/24 -j ACCEPT -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT -A INPUT -p tcp -m multiport --dports 80,443 -j ACCEPT -A INPUT -s 172.16.1.0/24 -j ACCEPT -A INPUT -i lo -j ACCEPT -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT COMMIT # Completed on Tue Apr 4 12:24:43 2017\\02. 第二种方式 iptables-save &gt;/etc/sysconfig/iptables1.7 iptables nat表配置实例(理论)(理论掌握) 1.7.1 iptables实现共享上网 图 - SNAT 配置原理图 第一个里程碑：配置内网服务器，设置网关地址 /etc/init.d/iptables stop # 内网服务器停止防火墙服务 ifdown eth0 # 模拟关闭内网服务器外网网卡 setup # 修改内网网卡网关和DNS地址信息也可以使用命令添加默认网关 route add default gw 172.16.1.188查看默认的路由信息 [root@test ~]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 172.16.1.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1 169.254.0.0 0.0.0.0 255.255.0.0 U 1003 0 0 eth1 0.0.0.0 172.16.1.188 0.0.0.0 UG 0 0 0 eth1说明：内网服务器网关地址指定为共享上网服务器内网网卡地址 第二个里程碑：配置共享上网服务器，开启共享上网服务器路由转发功能 [root@clsn ~]# vim /etc/sysctl.conf [root@clsn ~]# sysctl -p ~~~ net.ipv4.ip_forward = 1 ~~~第三个里程碑：配置共享上网服务器，实现内网访问外网的NAT映射 iptables -t nat -A POSTROUTING -s 172.16.1.0/24 -o eth0 -j SNAT --to-source 10.0.0.188参数详解： 参数 参数说明 -s 172.16.1.0/24 指定将哪些内网网段进行映射转换 -o eth0 指定在共享上网哪个网卡接口上做NAT地址转换 -j SNAT 将源地址进行转换变更 -j DNAT 将目标地址进行转换变更 –to-source ip**地址** 将源地址映射为什么IP地址 –to-destination ip**地址** 将目标地址映射为什么IP地址 当filter表中的forward默认为drop策略时，如何配置forward链？ 图 - forward工作原理 配置示例 iptables -A FORWARD -i eth1 -s 172.16.1.0/24 -j ACCEPT # iptables -A FORWARD -o eth0 -s 172.16.1.0/24 -j ACCEPT # 可以不进行配置 iptables -A FORWARD -i eth0 -d 172.16.1.0/24 -j ACCEPT # iptables -A FORWARD -o eth1 -d 172.16.1.0/24 -j ACCEPT # 可以不进行配置当外网ip不固定时如何配置？ iptables -t nat -A POSTROUTING -s 172.16.1.0/24 -o eth0 -j MASQUERADE # 伪装共享上网说明：在工作中如何没有固定外网IP地址，可以采取以上伪装映射的方式进行共享上网 配置映射方法小结 \\01. 指定哪些网段需要进行映射 -s 172.16.1.0/24 \\02. 指定在哪做映射 -o eth0 \\03. 用什么方法做映射 -j SNAT/DNAT MASQUERADE \\04. 映射成什么地址 –to-source ip地址/–to-destination ip地址 1.7.2 iptables实现外网IP的端口映射到内网IP的端口实际需求：将网关的IP和9000端口映射到内网服务器的22端口 端口映射 10.0.0.188:9000 –&gt;172.16.1.180:22 配置实例： iptables -t nat -A PREROUTING -d 10.0.0.188 -p tcp --dport 9000 -i eth0 -j DNAT --to-destination 172.16.1.7:22参数说明: 参数 参数说明 -d 10.0.0.188 目标地址。 -j DNAT 目的地址改写。 1.7.3 IP一对一映射 图 - DNAT 映射原理 实际需求：将ip 地址172.16.1.180 映射到10.0.0.188 通过辅助IP配置： ip addr add 10.0.0.81/24 dev eth0 label eth0:0 # 添加辅助IP iptables -t nat -I PREROUTING -d 10.0.0.81 -j DNAT --to-destination 172.16.1.51 iptables -t nat -I POSTROUTING -s 172.16.1.51 -o eth0 -j SNAT --to-source 10.0.0.81适合内网的机器访问NAT外网的IP iptables -t nat -I POSTROUTING -s 172.16.1.0/255.255.240.0 -d 10.0.0.81 -j SNAT --to-source 172.16.1.8检查配置： ping 10.0.0.81 -t tcpdump|grep -i icmp（两台机器上分别监测） telnet 10.0.0.81 221.7.4 映射多个外网IP上网 方法1： iptables -t nat -A POSTROUTING -s 10.0.1.0/255.255.240.0 -o eth0 -j SNAT --to-source 124.42.60.11-124.42.60.16​ 在三层交换机或路由器，划分VLAN。 方法2： iptables -t nat -A POSTROUTING -s 10.0.1.0/22 -o eth0 -j SNAT --to-source 124.42.60.11 iptables -t nat -A POSTROUTING -s 10.0.2.0/22 -o eth0 -j SNAT --to-source 124.42.60.12​ 扩大子网，会增加广播风暴。 1.7.5 系统防火墙与网络内核优化标准参数有关iptables的内核优化 调整内核参数文件/etc/sysctl.conf 以下是我的生产环境的某个服务器的配置： 解决time-wait**过多**的解决办法： net.ipv4.tcp_fin_timeout = 2 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_keepalive_time = 600 net.ipv4.tcp_max_tw_buckets = 36000 net.ipv4.ip_local_port_range = 4000 65000 net.ipv4.tcp_max_syn_backlog = 16384 net.ipv4.route.gc_timeout = 100 net.ipv4.tcp_syn_retries = 1 net.ipv4.tcp_synack_retries = 1在dmesg中显示 ip_conntrack: table full, dropping packet. 的错误提示，什么原因？ 如何解决？ #iptables优化 net.nf_conntrack_max = 25000000 net.netfilter.nf_conntrack_max = 25000000 net.netfilter.nf_conntrack_tcp_timeout_established = 180 net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120 net.netfilter.nf_conntrack_tcp_timeout_close_wait = 60 net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 1201.8 自定义链的配置(了解) 图 - 自定义链原理 创建自定义链 #示例：在filter表中创建NOICMP自定义链 iptables -t filter -N NOICMP引用自定义链 #示例：在INPUT链中引用刚才创建的自定义链 iptables -t filter -I INPUT -p icmp -j NOICMP重命名自定义链 #示例：将IN_WEB自定义链重命名为WEB iptables -E NOICMP ACCEPTICMP删除自定义链 删除自定义链需要满足两个条件 1、自定义链没有被引用 2、自定义链中没有任何规则 # 示例： 删除引用数为0且不包含任何规则的ACCEPTICMP链 iptables -X ACCEPTICMP1.9 附录-防火墙状态机制状态机制是iptables中较为特殊的一部分，这也是iptables和比较老的ipchains的一个比较大的区別之一，运行状态机制（连接跟踪）的防火墙称作带有状态机制的防火墙，以下简称为状态防火墙.状态防火墙比非状态防火墙要安全，因为它允许我们编写更严密的规则。 在iptables上一共有四种状态，分别被称为NEW、ESTABLISHED、INVALID、RELATED,这四种状态对于TCP、UDP、ICMP三种协议均有效。下面，我们来分别阐述四种状态的特性. 🔔 NEW meaning that the packet has started a new connection, or otherwise associated with a connection which has not seen packets in both directions NEW说明这个包是我们看到的第一个包。意思就是，这是conntrack模块看到的某个连接的第一个包，它即格被匹配了。比如，我们看到一个SYN包，是我们所留意的连接的第一个包，就要匹配它。 🔔 ESTABLISHED meaning that the packet is associated with a connection which has seen packets in both directions ESTABLISHED已经注意到两个方向上的数据传输，而且会继续匹配这个连接的包.处于ESTABLISHED状态的连接是非常容易理解的.只要发送并接到应答，连接就是ESTABLISHED的了。一个连接要从NEW变为ESTABLISHED,只需要接到应答包即可，不管这个包是发往防火墙的，还是要由防火墙转发的.ICMP的错误和重定向等信息包也被看作是ESTABLISHED,只要它们是我们所发出的信息的应答。 🔔 RELATED meaning that the packet is starting a new connection, but is associated with an existing connection, such as an FTP data transfer, or an ICMP error. RELATED是个比较麻烦的状态.当一个连接和某个已处于ESTABLISHED状态的连接有关系时，就被认为是RELATED的了，换句话说，一个连接要想是RELATED的，首先要有一个ESTABLISHED的连接。这个ESTABLISHED连接再产生一个主连接之外的连接，这个新的连接就是RELATED的了，当然前提是conntrack模块要能理解RELATED。ftp是个很好的例子，FTP-data连接就是和FTP-control有关联的，如果没有在iptables的策略中配RELATED状态，FTP-data的连接是无法正确建立的，还有其他的例子，比如，通过IRC的DCC连接#有了这个状态，ICMP应答、FTP传输、DCC等才能穿过防火墙正常工作.注意，大部分还有一些UDP协议都依赖这个机制。这些协议是很复杂的，它们把连接信息放在数据包里，并且要求这些信息能被正确理解。 🔔 INVALID meaning that the packet is associated with no known connection INVALID说明数据包不能被识别属于哪个连接或没有任何状态.有几个原因可以产生这种情况，比如，内存溢出，收到不知厲于哪个连接的ICMP错误信息。一般地，我们DROP这个状态的任何东西，因为防火墙认为这是不安全的东西 1.9.1 iptables配置哲学如何防止自己被关在门外？ 01、去机房重启系统或者登陆服努器删除刚才的禁止规则。 02、让机房人员重启服务器或者让机房人员拿用户密码登录进去。 03、通过服务器的远程管理卡管理（推荐）。 04、先写一个定时任务，每5分钟就停止防火墙。 05、测试环境测试好，写成脚本，批置执行 配置禁用22端口策略: iptables -I INPUT -p tcp - dport 22 -j DROP # 说明：利用-I参数，实现强行阻止访问22端口，将Jffc规则放在第一位删除配置的禁止连接22端口的规则 iptables -t filter -D INPUT -p tcp —dport 22 -j DROP iptables -F /etc/init.d/iptables restart 1.10 参考文献 [1] http://www.aichengxu.com/linux/3122717.htm [2] http://blog.csdn.net/huguohu2006/article/details/6453522 [3] http://blog.csdn.net/lin_credible/article/details/8614907 [4] http://blog.chinaunix.net/uid-27057175-id-5179329.html [5] http://blog.51cto.com/oldboy/974194 [6] http://blog.sina.com.cn/s/blog_773d9b6701018rwo.html [7] http://www.zsythink.net/archives/1625 # iptables -t 表名 动作(命令) 链名 匹配条件 -j 目标动作 -t 表名 raw mangle nat filter （如果不写-t 默认使用filter表） 链名(各表对应的链) raw(PROROUTING、OUPUT) mangle(PROROUTING、FORWARD、OUTPUT、POSTROUTING) nat(PREROUTING、INPUT、FORWARD、OUTPUT、POSTROUING) filter(INPUT、FORWARD、OUTPUT) 动作(官方叫命令)： -A 添加规则： (append追加) # iptables -t filter -A INPUT -p icmp -j REJECT(拒绝) //拒绝所有的icmp，即任何人不能ping同你 # iptables -t filter -A INPUT -p tcp --dport 22 -s 10.18.44.158 -j REJECT 拒绝源地址10.18.44.158的tcp协议的22端口 -I 插入规则 ： # iptables -t filter -I INPUT 2 -p tcp --dport 22 -s 10.18.44.171 -j REJECT //INPUT不加数字默认是第一行，数字代表插入到哪一行前边 -R 替换规则： # iptables -t filter -R INPUT 1 -p tcp --dport 22 -s 10.18.44.181 -j REJECT //替换的时候 INPUT必须跟上行号 -D 删除规则 # iptables -t filter -D INPUT -p icmp -j REJECT # iptables -D INPUT 2 -P 修改默认策略：只能使用DROP和ACCEPT # iptables -P INPUT DROP # iptables -P INPUT ACCEPT -N 添加自定义链 #iptables -N tiger #iptables -A tiger -p tcp --dport 22 -s 10.18.44.208 -j REJECT 存储规则 自定义链里的规则在没有被调用的情况下不生效 使用自定义链(关联自定义链) #iptables -A INPUT -j tiger 修改自定义链名称 #iptables -E tiger TIGER 删掉自定义链： 1.不能被关联 2.必须是空链 #iptables -X 链名 -F 清空规则 # iptables -F 链名 //指定链的所有规则 #iptables -F //所有的规则 -Z 计数清零 字节数 数据包个数 #iptables -Z //喜爱嗯看到计数用#iptables -L -n -v (v可有号多个) -L 查看规则 -n以数字的形式显示ip和端口协议 --line 显示规则行号 -v流量计数(verbose 数据包的计数) #iptables -L #iptables -L -n #iptables -L -n --line #iptables -L -n -v 匹配条件： 基本匹配 在使用协议的时候不必非得写端口，但是使用端口是必须跟协议 协议 /etc/protocols ---icmp协议簇 /etc/servers --TCP/IP协议簇 -p tcp udp icmp -ptcp 端口 --sport //源端口 # iptables -A INPUT -p tcp --sport 22 -s 10.18.44.208 -j REJECT # iptables _A INPUT -p tcp --sport 22:30 -s 10.18.44.208,10.18.44.209,10.18.44.210 -j REJECT --dport //目标端口 # iptables -A INPUT -p tcp --dport 22 -s 10.18.44.208 -j REJECT ip -s #iptables -A INPUT -p tcp --dport 20:30 -s 10.18.44.208/24 -j REJECT // 在iptables中 10.18.44.208/24 其实是10.18.44.0/24 -d","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"iptables","slug":"iptables","permalink":"https://cyylog.github.io/tags/iptables/"}]},{"title":"Redis-简介","slug":"SQL/redis","date":"2018-12-04T17:55:39.000Z","updated":"2020-10-30T04:13:57.645Z","comments":true,"path":"2018/12/05/sql/redis/","link":"","permalink":"https://cyylog.github.io/2018/12/05/sql/redis/","excerpt":"","text":"redis redis简介什么是redis REmote DIctionary Server(Redis) 是一个由Salvatore Sanfilippo写的key-value存储系统。 Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Hash), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。 redis是一个开源的、使用C语言编写的、支持网络交互的、可基于内存也可持久化的Key-Value数据库。 redis的官网：redis.io 注:域名后缀io属于国家域名，是british Indian Ocean territory，即英属印度洋领地 1.Redis是一个key-value存储系统。 和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。 2.在此基础上，redis支持各种不同方式的排序。 与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。 Redis 是一个高性能的key-value数据库。 redis的出现，很大程度补偿了memcached这类key/value存储的不足，在部 分场合可以对关系数据库起到很好的补充作用。它提供了Java，C/C++，C#，PHP，JavaScript，Perl，Object-C，Python，Ruby，Erlang等客户端，使用很方便。 3.Redis支持主从同步。 数据可以从主服务器向任意数量的从服务器上同步，从服务器可以是关联其他从服务器的主服务器。这使得Redis可执行单层树复制。存盘可以有意无意的对数据进行写操作。由于完全实现了发布/订阅机制，使得从数据库在任何地方同步树时，可订阅一个频道并接收主服务器完整的消息发布记录。同步对读取操作的可扩展性和数据冗余很有帮助。 Redis 与其他 key - value 缓存产品有以下三个特点： - Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 - Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 - Redis支持数据的备份，即master-slave模式的数据备份。redis优势- 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。 - 丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。 - 原子 – Redis的所有操作都是原子性的，意思就是要么成功执行要么失败完全不执行。单个操作是原子性的。多个操作也支持事务，即原子性，通过MULTI和EXEC指令包起来。 - 丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性。redis安装下载地址http://redis.io/download，下载最新稳定版本。 $ wget http://download.redis.io/releases/redis-5.0.4.tar.gz $ tar xzf redis-5.0.4.tar.gz $ cd redis-5.0.4.tar.gz $ yum install -y make gcc $ makeredis简单配置# cp redis.conf redis.conf.bak # vim redis.conf ---修改如下 bind 127.0.0.1 #只监听内网IP daemonize yes #开启后台模式将on改为yes timeout 300 #连接超时时间 port 6379 #端口号 databases 0 存储Session的Redis库编号 dir ./ #本地数据库存放目录该目录需要存在 pidfile /var/run/redis_6379.pid #定义pid文件 logfile /var/log/redis_6379.log #定义log文件 requirepass cyy # 设置密码配置redis为systemctl启动# cd /lib/systemd/system # vim /lib/systemd/system/redis.service [Unit] Description=Redis After=network.target [Service] ExecStart=/usr/local/redis-5.0.4/src/redis-server /usr/local/redis-5.0.4/redis.conf --daemonize no ExecStop=/usr/local/redis-5.0.4/src/redis-cli -h 127.0.0.1 -p 6379 shutdown [Install] WantedBy=multi-user.target ===================== 参数详解: • [Unit] 表示这是基础信息 • Description 是描述 • After 是在那个服务后面启动，一般是网络服务启动后启动 • [Service] 表示这里是服务信息 • ExecStart 是启动服务的命令 • ExecStop 是停止服务的指令 • [Install] 表示这是是安装相关信息 • WantedBy 是以哪种方式启动：multi-user.target表明当系统以多用户方式（默认的运行级别）启动时，这个服务需要被自动运行。redis启动make完后 redis-5.0.4目录下会出现编译后的redis服务程序redis-server,还有用于测试的客户端程序redis-cli,两个程序位于安装目录 src 目录下： 下面启动redis服务. $ src/redis-server 注意这种方式启动redis 使用的是默认配置。也可以通过启动参数告诉redis使用指定配置文件使用下面命令启动。 $ cd src $ ./redis-server ../redis.conf redis.conf 是一个默认的配置文件。我们可以根据需要使用自己的配置文件。redis客户端测试$ src/redis-cli 127.0.0.1:6379&gt; set 2020 GZ OK 127.0.0.1:6379&gt; get 2020 &quot;GZ&quot; 127.0.0.1:6379&gt; ping PONG redis配置redis的配置默认位于redis安装目录下，文件名未redis.conf Redis CONFIG 命令格式如下： redis 127.0.0.1:6379&gt; CONFIG GET CONFIG_SETTING_NAME 也可以通过命令查看或设置相关配置 127.0.0.1:6379&gt; config get loglevel 1) &quot;loglevel&quot; 2) &quot;notice&quot; 通过* 查看所有配置 127.0.0.1:6379&gt; config get * 1) &quot;dbfilename&quot; 2) &quot;dump.rdb&quot; 3) &quot;requirepass&quot; 4) &quot;&quot; 5) &quot;masterauth&quot; 6) &quot;&quot; 7) &quot;cluster-announce-ip&quot; 8) &quot;&quot; 9) &quot;unixsocket&quot; 10) &quot;&quot; 11) &quot;logfile&quot; 12) &quot;&quot; 13) &quot;pidfile&quot; 14) &quot;&quot; 15) &quot;slave-announce-ip&quot; 16) &quot;&quot; 17) &quot;replica-announce-ip&quot; 18) &quot;&quot; 19) &quot;maxmemory&quot; 20) &quot;0&quot; 21) &quot;proto-max-bulk-len&quot; 22) &quot;536870912&quot; 23) &quot;client-query-buffer-limit&quot; 24) &quot;1073741824&quot; 25) &quot;maxmemory-samples&quot; 26) &quot;5&quot; 27) &quot;lfu-log-factor&quot; 28) &quot;10&quot; 29) &quot;lfu-decay-time&quot; 30) &quot;1&quot; 31) &quot;timeout&quot; 32) &quot;0&quot; 33) &quot;active-defrag-threshold-lower&quot; 34) &quot;10&quot; 35) &quot;active-defrag-threshold-upper&quot; 36) &quot;100&quot; 37) &quot;active-defrag-ignore-bytes&quot; 38) &quot;104857600&quot; 39) &quot;active-defrag-cycle-min&quot; 40) &quot;5&quot; 41) &quot;active-defrag-cycle-max&quot; 42) &quot;75&quot; 43) &quot;active-defrag-max-scan-fields&quot; 44) &quot;1000&quot; 45) &quot;auto-aof-rewrite-percentage&quot; 46) &quot;100&quot; 47) &quot;auto-aof-rewrite-min-size&quot; 48) &quot;67108864&quot; 49) &quot;hash-max-ziplist-entries&quot; 50) &quot;512&quot; 51) &quot;hash-max-ziplist-value&quot; 52) &quot;64&quot; 53) &quot;stream-node-max-bytes&quot; 54) &quot;4096&quot; 55) &quot;stream-node-max-entries&quot; 56) &quot;100&quot; 57) &quot;list-max-ziplist-size&quot; 58) &quot;-2&quot; 59) &quot;list-compress-depth&quot; 60) &quot;0&quot; 61) &quot;set-max-intset-entries&quot; 62) &quot;512&quot; 63) &quot;zset-max-ziplist-entries&quot; 64) &quot;128&quot; 65) &quot;zset-max-ziplist-value&quot; 66) &quot;64&quot; 67) &quot;hll-sparse-max-bytes&quot; 68) &quot;3000&quot; 69) &quot;lua-time-limit&quot; 70) &quot;5000&quot; 71) &quot;slowlog-log-slower-than&quot; 72) &quot;10000&quot; 73) &quot;latency-monitor-threshold&quot; 74) &quot;0&quot; 75) &quot;slowlog-max-len&quot; 76) &quot;128&quot; 77) &quot;port&quot; 78) &quot;6379&quot; 79) &quot;cluster-announce-port&quot; 80) &quot;0&quot; 。。。编辑配置 可以通过修改redis.conf文件或者使用 CONFIG setm命令修改配置 CONFIG set 语法如下 redis 127.0.0.1:6379&gt; CONFIG SET CONFIG_SETTING_NAME NEW_CONFIG_VALUE 示例 redis 127.0.0.1:6379&gt; CONFIG SET loglevel &quot;notice&quot; OK redis 127.0.0.1:6379&gt; CONFIG GET loglevel 1) &quot;loglevel&quot; 2) &quot;notice&quot;相关配置参数详解 Redis配置文件参数说明: 1. Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程 daemonize no 2. 当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定 pidfile /var/run/redis.pid 3. 指定Redis监听端口，默认端口为6379，作者在自己的一篇博文中解释了为什么选用6379作为默认端口，因为6379在手机按键上MERZ对应的号码，而MERZ取自意大利歌女Alessia Merz的名字 port 6379 4. 绑定的主机地址 bind 127.0.0.1 5.当 客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能 timeout 300 6. 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose loglevel verbose 7. 日志记录方式，默认为标准输出，如果配置Redis为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给/dev/null logfile stdout 8. 设置数据库的数量，默认数据库为0，可以使用SELECT &lt;dbid&gt;命令在连接上指定数据库id databases 16 9. 指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合 save &lt;seconds&gt; &lt;changes&gt; Redis默认配置文件中提供了三个条件： save 900 1 save 300 10 save 60 10000 分别表示900秒（15分钟）内有1个更改，300秒（5分钟）内有10个更改以及60秒内有10000个更改。 10. 指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大 rdbcompression yes 11. 指定本地数据库文件名，默认值为dump.rdb dbfilename dump.rdb 12. 指定本地数据库存放目录 dir ./ 13. 设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步 slaveof &lt;masterip&gt; &lt;masterport&gt; 14. 当master服务设置了密码保护时，slav服务连接master的密码 masterauth &lt;master-password&gt; 15. 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过AUTH &lt;password&gt;命令提供密码，默认关闭 requirepass foobared 16. 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息 maxclients 128 17. 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，当此方法处理 后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis新的vm机制，会把Key存放内存，Value会存放在swap区 maxmemory &lt;bytes&gt; 18. 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no appendonly no 19. 指定更新日志文件名，默认为appendonly.aof appendfilename appendonly.aof 20. 指定更新日志条件，共有3个可选值： no：表示等操作系统进行数据缓存同步到磁盘（快） always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全） everysec：表示每秒同步一次（折衷，默认值） appendfsync everysec 21. 指定是否启用虚拟内存机制，默认值为no，简单的介绍一下，VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中（在后面的文章我会仔细分析Redis的VM机制） vm-enabled no 22. 虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享 vm-swap-file /tmp/redis.swap 23. 将所有大于vm-max-memory的数据存入虚拟内存,无论vm-max-memory设置多小,所有索引数据都是内存存储的(Redis的索引数据 就是keys),也就是说,当vm-max-memory设置为0的时候,其实是所有value都存在于磁盘。默认值为0 vm-max-memory 0 24. Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page大小最好设置为32或者64bytes；如果存储很大大对象，则可以使用更大的page，如果不 确定，就使用默认值 vm-page-size 32 25. 设置swap文件中的page数量，由于页表（一种表示页面空闲或使用的bitmap）是在放在内存中的，，在磁盘上每8个pages将消耗1byte的内存。 vm-pages 134217728 26. 设置访问swap文件的线程数,最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为4 vm-max-threads 4 27. 设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启 glueoutputbuf yes 28. 指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法 hash-max-zipmap-entries 64 hash-max-zipmap-value 512 29. 指定是否激活重置哈希，默认为开启（后面在介绍Redis的哈希算法时具体介绍） activerehashing yes redis数据类型Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。 stringstring 是 redis 最基本的类型，你可以理解成与 Memcached 一模一样的类型，一个 key 对应一个 value。 string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据。比如jpg图片或者序列化的对象。 string 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB 127.0.0.1:6379&gt; set name cyylog OK 127.0.0.1:6379&gt; get name &quot;cyylog&quot; 我们使用了 Redis 的 SET 和 GET 命令。键为 name，对应的值为 cyyloghashRedis hash 是一个键值(key=&gt;value)对集合。 Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。 示例: 127.0.0.1:6379&gt; hmset myhash name tiger name2 five OK 127.0.0.1:6379&gt; hget myhash name &quot;tiger&quot; 127.0.0.1:6379&gt; hget myhash name2 &quot;five&quot; 实例中我们使用了 Redis HMSET, HGET 命令，HMSET 设置了两个 field=&gt;value 对, HGET 获取对应 field 对应的 value。 每个 hash 可以存储 232 -1 键值对（40多亿）。ListRedis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 示例 127.0.0.1:6379&gt; lpush mylist redis1 (integer) 1 127.0.0.1:6379&gt; lpush mylist redis2 redis3 redis4 (integer) 4 127.0.0.1:6379&gt; lpush mylist tiger 2020 (integer) 6 127.0.0.1:6379&gt; lrange mylist 0 3 1) &quot;2020&quot; 2) &quot;tiger&quot; 3) &quot;redis4&quot; 4) &quot;redis3&quot; 127.0.0.1:6379&gt; lrange mylist 0 10 1) &quot;2020&quot; 2) &quot;tiger&quot; 3) &quot;redis4&quot; 4) &quot;redis3&quot; 5) &quot;redis2&quot; 6) &quot;redis1&quot; 上述示例我们通过 lpush 创建list并添加数据，通过lrange获取列表中的数据 列表最多可存储 232 - 1 元素 (4294967295, 每个列表可存储40多亿)。setRedis的Set是string类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 sadd 命令 添加一个 string 元素到 key 对应的 set 集合中，成功返回1，如果元素已经在集合中返回 0，如果 key 对应的 set 不存在则返回错误。 示例 127.0.0.1:6379&gt; sadd myset GZ BJ ZZ BK (integer) 4 127.0.0.1:6379&gt; sadd myset TJ (integer) 1 127.0.0.1:6379&gt; sadd myset TJ (integer) 0 127.0.0.1:6379&gt; smembers myset 1) &quot;BJ&quot; 2) &quot;BK&quot; 3) &quot;TJ&quot; 4) &quot;GZ&quot; 5) &quot;ZZ&quot; 127.0.0.1:6379&gt; sadd myset SZ (integer) 1 127.0.0.1:6379&gt; smembers myset 1) &quot;BJ&quot; 2) &quot;BK&quot; 3) &quot;TJ&quot; 4) &quot;GZ&quot; 5) &quot;ZZ&quot; 6) &quot;SZ&quot; 注意：以上实例中 TJ 添加了两次，但根据集合内元素的唯一性，第二次插入的元素将被忽略。 集合中最大的成员数为 232 - 1(4294967295, 每个集合可存储40多亿个成员)。 zsetRedis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但分数(score)却可以重复。 zadd 命令 添加元素到集合，元素在集合中存在则更新对应score 示例 127.0.0.1:6379&gt; zadd myzset 0 GZ (integer) 1 127.0.0.1:6379&gt; zadd myzset 0 BJ (integer) 1 127.0.0.1:6379&gt; zadd myzset 0 ZZ (integer) 1 127.0.0.1:6379&gt; zrangebyscore myzset 0 5 1) &quot;BJ&quot; 2) &quot;GZ&quot; 3) &quot;ZZ&quot; 注意：Redis支持多个数据库，并且每个数据库的数据是隔离的不能共享，并且基于单机才有，如果是集群就没有数据库的概念。 Redis是一个字典结构的存储服务器，而实际上一个Redis实例提供了多个用来存储数据的字典，客户端可以指定将数据存储在哪个字典中。这与我们熟知的在一个关系数据库实例中可以创建多个数据库类似，所以可以将其中的每个字典都理解成一个独立的数据库。 每个数据库对外都是一个从0开始的递增数字命名，Redis默认支持16个数据库（可以通过配置文件支持更多，无上限），可以通过配置databases来修改这一数字。客户端与Redis建立连接后会自动选择0号数据库，不过可以随时使用SELECT命令更换数据库，如要选择1号数据库： 示例: 127.0.0.1:6379&gt; select 1 OK 127.0.0.1:6379[1]&gt; get name (nil) 127.0.0.1:6379[1]&gt; get myzset (nil) 127.0.0.1:6379[1]&gt; select 0 OK 127.0.0.1:6379&gt; get name &quot;cyylog&quot; 127.0.0.1:6379&gt; zrangebyscore myzset 0 2 1) &quot;BJ&quot; 2) &quot;GZ&quot; 3) &quot;ZZ&quot; 然而这些以数字命名的数据库又与我们理解的数据库有所区别。首先Redis不支持自定义数据库的名字，每个数据库都以编号命名，开发者必须自己记录哪些数据库存储了哪些数据。另外Redis也不支持为每个数据库设置不同的访问密码，所以一个客户端要么可以访问全部数据库，要么连一个数据库也没有权限访问。最重要的一点是多个数据库之间并不是完全隔离的，比如FLUSHALL命令可以清空一个Redis实例中所有数据库中的数据。综上所述，这些数据库更像是一种命名空间，而不适宜存储不同应用程序的数据。比如可以使用0号数据库存储某个应用生产环境中的数据，使用1号数据库存储测试环境中的数据，但不适宜使用0号数据库存储A应用的数据而使用1号数据库B应用的数据，不同的应用应该使用不同的Redis实例存储数据。由于Redis非常轻量级，一个空Redis实例占用的内在只有1M左右，所以不用担心多个Redis实例会额外占用很多内存。redis命令Redis 命令用于在 redis 服务上执行操作。 要在 redis 服务上执行命令需要一个 redis 客户端。Redis 客户端在我们之前下载的的 redis 的安装包中。 语法 Redis 客户端的基本语法为： $ redis-cli 在远程服务上执行命令 如果需要在远程 redis 服务上执行命令，同样我们使用的也是 redis-cli 命令。 语法 $ redis-cli -h host -p port -a password redis数据备份和恢复Redis SAVE 命令用于创建当前数据库的备份。 语法 redis Save 命令基本语法如下： 127.0.0.1:6379&gt; save OK 该命令会在redis安装目录下创建dump.rdb文件 恢复数据 如果需要恢复数据，只需将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可。获取 redis 目录可以使用 CONFIG 命令，如下所示： 127.0.0.1:6379&gt; config get dir 1) &quot;dir&quot; 2) &quot;/usr/local/redis/src&quot; 以上命令 CONFIG GET dir 输出的 redis 安装目录为/usr/local/redis/src Bgsave 创建 redis 备份文件也可以使用命令 BGSAVE，该命令在后台执行。 127.0.0.1:6379&gt; BGSAVE Background saving startedredis安全我们可以通过 redis 的配置文件设置密码参数，这样客户端连接到 redis 服务就需要密码验证，这样可以让你的 redis 服务更安全。 127.0.0.1:6379&gt; CONFIG get requirepass 1) &quot;requirepass&quot; 2) &quot;&quot; 默认情况下 requirepass 参数是空的，这就意味着你无需通过密码验证就可以连接到 redis 服务。 你可以通过以下命令来修改该参数： 127.0.0.1:6379&gt; CONFIG set requirepass &quot;tiger&quot; OK 127.0.0.1:6379&gt; CONFIG get requirepass 1) &quot;requirepass&quot; 2) &quot;tiger&quot; 登录redis-cli -h 127.0.0.1 -p 6379 -a tiger 或者登陆后认证 127.0.0.1:6379&gt; AUTH &quot;tiger&quot; OK 127.0.0.1:6379&gt; SET name &quot;Test value&quot; OK 127.0.0.1:6379&gt; GET name &quot;Test value&quot;redis持久化redis持久化 – 两种方式 开启持久化功能后，重启redis后，数据会自动通过持久化文件恢复！！ redis提供了两种持久化的方式，分别是RDB（Redis DataBase）和AOF（Append Only File）。 RDB，是在不同的时间点，将redis存储的数据生成快照并存储到磁盘等介质上； AOF，则是换了一个角度来实现持久化，那就是将redis执行过的所有写指令记录下来，在下次redis重新启动时，只要把这些写指令从前到后再重复执行一遍，就可以实现数据恢复了。 RDB和AOF两种方式也可以同时使用，在这种情况下，如果redis重启的话，则会优先采用AOF方式来进行数据恢复，这是因为AOF方式的数据恢复完整度更高。 如果你没有数据持久化的需求，也完全可以关闭RDB和AOF方式，这样的话，redis将变成一个纯内存数据库，就像memcache一样。 redis持久化 – RDB RDB方式，是将redis某一时刻的数据持久化到磁盘中，是一种快照式的持久化方法。 redis在进行数据持久化的过程中，会先将数据写入到一个临时文件中，待持久化过程都结束了，才会用这个临时文件替换上次持久化好的文件。正是这种特性，让我们可以随时来进行备份，因为快照文件总是完整可用的。 -------------&gt;----------&gt;------------------&gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 -----------&gt; --------&gt; 1 2 3 4 5 1 2 3 4 5 6 7 8 9 10 11 12 13 对于RDB方式，redis会单独创建（fork）一个子进程来进行持久化，而主进程是不会进行任何IO操作的，这样就确保了redis极高的性能。 如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。 虽然RDB有不少优点，但它的缺点也是不容忽视的。如果你对数据的完整性非常敏感，那么RDB方式就不太适合你，因为即使你每5分钟都持久化一次，当redis故障时，仍然会有近5分钟的数据丢失。所以，redis还提供了另一种持久化方式，那就是AOF。 redis持久化 – AOF AOF，英文是Append Only File，即只允许追加不允许改写的文件。 AOF方式是将执行过的写指令记录下来，在数据恢复时按照从前到后的顺序再将指令都执行一遍，就这么简单。 通过配置redis.conf中的appendonly yes就可以打开AOF功能。如果有写操作（如SET等），redis就会被追加到AOF文件的末尾。 默认的AOF持久化策略是每秒钟fsync一次（fsync是指把缓存中的写指令记录到磁盘中），因为在这种情况下，redis仍然可以保持很好的处理性能，即使redis故障，也只会丢失最近1秒钟的数据。 如果在追加日志时，恰好遇到磁盘空间满、inode满或断电等情况导致日志写入不完整，也没有关系，redis提供了redis-check-aof工具，可以用来进行日志修复。 因为采用了追加方式，如果不做任何处理的话，AOF文件会变得越来越大，为此，redis提供了AOF文件重写（rewrite）机制，即当AOF文件的大小超过所设定的阈值时，redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集。举个例子或许更形象，假如我们调用了100次INCR指令，在AOF文件中就要存储100条指令，但这明显是很低效的，完全可以把这100条指令合并成一条SET指令，这就是重写机制的原理。 在进行AOF重写时，仍然是采用先写临时文件，全部完成后再替换的流程，所以断电、磁盘满等问题都不会影响AOF文件的可用性，这点可以放心。 AOF方式的另一个好处，我们通过一个“场景再现”来说明。某同学在操作redis时，不小心执行了FLUSHALL，导致redis内存中的数据全部被清空了，这是很悲剧的事情。不过这也不是世界末日，只要redis配置了AOF持久化方式，且AOF文件还没有被重写（rewrite），我们就可以用最快的速度暂停redis并编辑AOF文件，将最后一行的FLUSHALL命令删除，然后重启redis，就可以恢复redis的所有数据到FLUSHALL之前的状态了。是不是很神奇，这就是AOF持久化方式的好处之一。但是如果AOF文件已经被重写了，那就无法通过这种方法来恢复数据了。 虽然优点多多，但AOF方式也同样存在缺陷，比如在同样数据规模的情况下，AOF文件要比RDB文件的体积大。而且，AOF方式的恢复速度也要慢于RDB方式。 1 2 3 4 5 6 6zi 0 如果你直接执行BGREWRITEAOF命令，那么redis会生成一个全新的AOF文件，其中便包括了可以恢复现有数据的最少的命令集。 如果运气比较差，AOF文件出现了被写坏的情况，也不必过分担忧，redis并不会贸然加载这个有问题的AOF文件，而是报错退出。这时可以通过以下步骤来修复出错的文件： 1.备份被写坏的AOF文件 2.运行redis-check-aof –fix进行修复 3.用diff -u来看下两个文件的差异，确认问题点 4.重启redis，加载修复后的AOF文件 redis持久化 – AOF重写 AOF重写的内部运行原理，有必要了解一下。 在重写即将开始之际，redis会创建（fork）一个“重写子进程”，这个子进程会首先读取现有的AOF文件，并将其包含的指令进行分析压缩并写入到一个临时文件中。 与此同时，主工作进程会将新接收到的写指令一边累积到内存缓冲区中，一边继续写入到原有的AOF文件中，这样做是保证原有的AOF文件的可用性，避免在重写过程中出现意外。 当“重写子进程”完成重写工作后，它会给父进程发一个信号，父进程收到信号后就会将内存中缓存的写指令追加到新AOF文件中。 当追加结束后，redis就会用新AOF文件来代替旧AOF文件，之后再有新的写指令，就都会追加到新的AOF文件中了。 redis持久化 – 如何选择RDB和AOF 对于我们应该选择RDB还是AOF，官方的建议是两个同时使用。这样可以提供更可靠的持久化方案。 写入速度快 AOF 写入速度慢 RDB redis的事务处理 众所周知，事务是指“一个完整的动作，要么全部执行，要么什么也没有做”。 在聊redis事务处理之前，要先和大家介绍四个redis指令，即MULTI、EXEC、DISCARD、WATCH。这四个指令构成了redis事务处理的基础。 1.MULTI用来组装一个事务； 2.EXEC用来执行一个事务； 3.DISCARD用来取消一个事务； 4.WATCH用来监视一些key，一旦这些key在事务执行之前被改变，则取消事务的执行。 一个MULTI和EXEC的例子： redis&gt; MULTI //标记事务开始 OK redis&gt; INCR user_id //多条命令按顺序入队 QUEUED redis&gt; INCR user_id QUEUED redis&gt; INCR user_id QUEUED redis&gt; PING QUEUED redis&gt; EXEC //执行 1) (integer) 1 2) (integer) 2 3) (integer) 3 4) PONG 在上面的例子中，看到了QUEUED的字样，这表示我们在用MULTI组装事务时，每一个命令都会进入到内存队列中缓存起来，如果出现QUEUED则表示我们这个命令成功插入了缓存队列，在将来执行EXEC时，这些被QUEUED的命令都会被组装成一个事务来执行。 对于事务的执行来说，如果redis开启了AOF持久化的话，那么一旦事务被成功执行，事务中的命令就会通过write命令一次性写到磁盘中去，如果在向磁盘中写的过程中恰好出现断电、硬件故障等问题，那么就可能出现只有部分命令进行了AOF持久化，这时AOF文件就会出现不完整的情况，这时，可以使用redis-check-aof工具来修复这一问题，这个工具会将AOF文件中不完整的信息移除，确保AOF文件完整可用。 有关事务，经常会遇到的是两类错误： 1.调用EXEC之前的错误 2.调用EXEC之后的错误 “调用EXEC之前的错误”，有可能是由于语法有误导致的，也可能时由于内存不足导致的。只要出现某个命令无法成功写入缓冲队列的情况，redis都会进行记录，在客户端调用EXEC时，redis会拒绝执行这一事务。（这时2.6.5版本之后的策略。在2.6.5之前的版本中，redis会忽略那些入队失败的命令，只执行那些入队成功的命令）。我们来看一个这样的例子： 127.0.0.1:6379&gt; multi OK 127.0.0.1:6379&gt; haha //一个明显错误的指令 (error) ERR unknown command &#39;haha&#39; 127.0.0.1:6379&gt; ping QUEUED 127.0.0.1:6379&gt; exec //redis无情的拒绝了事务的执行，原因是“之前出现了错误” (error) EXECABORT Transaction discarded because of previous errors. 而对于“调用EXEC之后的错误”，redis则采取了完全不同的策略，即redis不会理睬这些错误，而是继续向下执行事务中的其他命令。这是因为，对于应用层面的错误，并不是redis自身需要考虑和处理的问题，所以一个事务中如果某一条命令执行失败，并不会影响接下来的其他命令的执行。我们也来看一个例子： 127.0.0.1:6379&gt; multi OK 127.0.0.1:6379&gt; set age 23 QUEUED //age不是集合，所以如下是一条明显错误的指令 127.0.0.1:6379&gt; sadd age 15 QUEUED 127.0.0.1:6379&gt; set age 29 QUEUED 127.0.0.1:6379&gt; exec //执行事务时，redis不会理睬第2条指令执行错误 1) OK 2) (error) WRONGTYPE Operation against a key holding the wrong kind of value 3) OK 127.0.0.1:6379&gt; get age &quot;29&quot; //可以看出第3条指令被成功执行了 最后一个指令“WATCH”，这是一个很好用的指令，它可以帮我们实现类似于“乐观锁”的效果，即CAS（check and set）。 WATCH本身的作用是“监视key是否被改动过”，而且支持同时监视多个key，只要还没真正触发事务，WATCH都会尽职尽责的监视，一旦发现某个key被修改了，在执行EXEC时就会返回nil，表示事务无法触发。 127.0.0.1:6379&gt; set age 23 OK 127.0.0.1:6379&gt; watch age //开始监视age OK 127.0.0.1:6379&gt; set age 24 //在EXEC之前，age的值被修改了 OK 127.0.0.1:6379&gt; multi OK 127.0.0.1:6379&gt; set age 25 QUEUED 127.0.0.1:6379&gt; get age QUEUED 127.0.0.1:6379&gt; exec //触发EXEC (nil) //事务无法被执行redis主从 + 哨兵主从 - 用法像MySQL一样，redis是支持主从同步的，而且也支持一主多从以及多级从结构。 主从结构，一是为了纯粹的冗余备份，二是为了提升读性能，比如很消耗性能的SORT就可以由从服务器来承担。 redis的主从同步是异步进行的，这意味着主从同步不会影响主逻辑，也不会降低redis的处理性能。 主从架构中，可以考虑关闭主服务器的数据持久化功能，只让从服务器进行持久化，这样可以提高主服务器的处理性能。 在主从架构中，从服务器通常被设置为只读模式，这样可以避免从服务器的数据被误修改。但是从服务器仍然可以接受CONFIG等指令，所以还是不应该将从服务器直接暴露到不安全的网络环境中。如果必须如此，那可以考虑给重要指令进行重命名，来避免命令被外人误执行。主从 - 同步原理从服务器会向主服务器发出SYNC指令，当主服务器接到此命令后，就会调用BGSAVE指令来创建一个子进程专门进行数据持久化工作，也就是将主服务器的数据写入RDB文件中。在数据持久化期间，主服务器将执行的写指令都缓存在内存中。 在BGSAVE指令执行完成后，主服务器会将持久化好的RDB文件发送给从服务器，从服务器接到此文件后会将其存储到磁盘上，然后再将其读取到内存中。这个动作完成后，主服务器会将这段时间缓存的写指令再以redis协议的格式发送给从服务器。 另外，要说的一点是，即使有多个从服务器同时发来SYNC指令，主服务器也只会执行一次BGSAVE，然后把持久化好的RDB文件发给多个下游。在redis2.8版本之前，如果从服务器与主服务器因某些原因断开连接的话，都会进行一次主从之间的全量的数据同步；而在2.8版本之后，redis支持了效率更高的增量同步策略，这大大降低了连接断开的恢复成本。 主服务器会在内存中维护一个缓冲区，缓冲区中存储着将要发给从服务器的内容。从服务器在与主服务器出现网络瞬断之后，从服务器会尝试再次与主服务器连接，一旦连接成功，从服务器就会把“希望同步的主服务器ID”和“希望请求的数据的偏移位置（replication offset）”发送出去。主服务器接收到这样的同步请求后，首先会验证主服务器ID是否和自己的ID匹配，其次会检查“请求的偏移位置”是否存在于自己的缓冲区中，如果两者都满足的话，主服务器就会向从服务器发送增量内容。 增量同步功能，需要服务器端支持全新的PSYNC指令。这个指令，只有在redis-2.8之后才具有。sentinel介绍Sentinel(哨兵)是用于监控redis集群中Master状态的工具，其已经被集成在redis2.4+的版本中 Sentinel作用： 1)：Master状态检测 2)：如果Master异常，则会进行Master-Slave切换，将其中一个Slave作为Master，将之前的Master作为Slave 3)：Master-Slave切换后，master_redis.conf、slave_redis.conf和sentinel.conf的内容都会发生改变，即master_redis.conf中会多一行slaveof的配置，sentinel.conf的监控目标会随之调换 Sentinel工作方式： 1)：每个Sentinel以每秒钟一次的频率向它所知的Master，Slave以及其他 Sentinel 实例发送一个 PING 命令 2)：如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel 标记为主观下线。 3)：如果一个Master被标记为主观下线，则正在监视这个Master的所有 Sentinel 要以每秒一次的频率确认Master的确进入了主观下线状态。 4)：当有足够数量的 Sentinel（大于等于配置文件指定的值）在指定的时间范围内确认Master的确进入了主观下线状态， 则Master会被标记为客观下线 5)：在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率向它已知的所有Master，Slave发送 INFO 命令 6)：当Master被 Sentinel 标记为客观下线时，Sentinel 向下线的 Master 的所有 Slave 发送 INFO 命令的频率会从 10 秒一次改为每秒一次 7)：若没有足够数量的 Sentinel 同意 Master 已经下线， Master 的客观下线状态就会被移除。 若 Master 重新向 Sentinel 的 PING 命令返回有效回复， Master 的主观下线状态就会被移除。 ============================================================= 主观下线和客观下线 主观下线：Subjectively Down，简称 SDOWN，指的是当前 Sentinel 实例对某个redis服务器做出的下线判断。 客观下线：Objectively Down， 简称 ODOWN，指的是多个 Sentinel 实例在对Master Server做出 SDOWN 判断，并且通过 SENTINEL is-master-down-by-addr 命令互相交流之后，得出的Master Server下线判断，然后开启failover.主从同步部署测试环境:centos7.4 redis-master:192.168.19.129 vm1 redis-slave1:192.168.19.136 vm4 redis-slave2:192.168.19.135 vm5 1.首先三台服务器将redis单机部署完成。 编辑master的redis配置文件: [root@redis-master ~]# cd /usr/local/redis-5.0.4 [root@redis-master redis]# vim redis.conf 2.修改slave1的配置文件：[root@redis-slave1 ~]# cd /data/application/redis/[root@redis-slave1 redis]# vim redis.conf —修改如下： 3.配置slave2的配置文件:[root@redis-slave2 ~]# cd /data/application/redis/[root@redis-slave2 redis]# vim redis.conf —修改如下 和slave1 相同 4.重启三台redis 5.测试主从 三台均测试无误，主从同步部署完成 配置哨兵模式1.每台机器上修改redis主配置文件redis.conf文件设置：bind 0.0.0.0 ---配置主从时已经完成 2.每台机器上修改sentinel.conf配置文件：修改如下配置 [root@redis-master src]# cd .. [root@redis-master redis]# vim sentinel.conf sentinel monitor mymaster 192.168.233.10 6379 2 (slave上面写的是master的ip，master写自己ip) sentinel down-after-milliseconds mymaster 3000 sentinel failover-timeout mymaster 10000 protected-mode no关闭加密 protected-mode no 构成master客观下线的前提，至少有两个sentinel(哨兵)主观认为master已经下线 sentinel monitor mymaster 192.168.19.129 6379 2 sentinel每隔一定时间向其已知的master发送ping指令，在设置的这个时间内如果没有收master返回的数据包，就会把master标记为主观下线。单位为毫秒 sentinel down-after-milliseconds mymaster 3000 在这个时间内如果主从切换没有完成就停止切换。单位毫秒 sentinel failover-timeout mymaster 100003.每台机器启动哨兵服务： # ./src/redis-sentinel sentinel.conf 注意:在生产环境下将哨兵模式启动放到后台执行: ./src/redis-sentinel sentinel.conf &amp; 在master上面执行 这是启动成功的！ 将master的哨兵模式退出，再将redis服务stop了，在两台slave上面查看其中一台是否切换为master:(没有优先级，为随机切换) master 192.168.19.129 slave 192.168.19.136 ​ 主从+哨兵模式测试部署完成！ ========================================================== 了解 主从+哨兵+lvs 制作redis主从的高科用 redis切片等","categories":[{"name":"SQL","slug":"SQL","permalink":"https://cyylog.github.io/categories/SQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://cyylog.github.io/tags/Redis/"}]},{"title":"找出Java中CPU使用率最高的线程，并打印这些线程的堆栈。","slug":"Linux/找出Java中CPU使用率最高的线程，并打印这些线程的堆栈。","date":"2018-09-28T15:54:25.000Z","updated":"2020-09-30T20:02:23.846Z","comments":true,"path":"2018/09/28/linux/zhao-chu-java-zhong-cpu-shi-yong-lu-zui-gao-de-xian-cheng-bing-da-yin-zhe-xie-xian-cheng-de-dui-zhan/","link":"","permalink":"https://cyylog.github.io/2018/09/28/linux/zhao-chu-java-zhong-cpu-shi-yong-lu-zui-gao-de-xian-cheng-bing-da-yin-zhe-xie-xian-cheng-de-dui-zhan/","excerpt":"","text":"1#!/bin/bash # @Function # Find out the highest cpu consumed threads of java, and print the stack of these threads. # # @Usage # $ ./show-busy-java-threads.sh # # @author Jerry Lee readonly PROG=`basename $0` readonly -a COMMAND_LINE=(\"$0\" \"$@\") usage() { cat /dev/null } trap \"cleanupWhenExit\" EXIT printStackOfThread() { local line local count=1 while IFS=\" \" read -a line ; do local pid=${line[0]} local threadId=${line[1]} local threadId0x=`printf %x ${threadId}` local user=${line[2]} local pcpu=${line[4]} local jstackFile=/tmp/${uuid}_${pid} [ ! -f \"${jstackFile}\" ] && { { if [ \"${user}\" == \"${USER}\" ]; then jstack ${pid} > ${jstackFile} else if [ $UID == 0 ]; then sudo -u ${user} jstack ${pid} > ${jstackFile} else redEcho \"[$((count++))] Fail to jstack Busy(${pcpu}%) thread(${threadId}/0x${threadId0x}) stack of java process(${pid}) under user(${user}).\" redEcho \"User of java process($user) is not current user($USER), need sudo to run again:\" yellowEcho \" sudo ${COMMAND_LINE[@]}\" echo continue fi fi } || { redEcho \"[$((count++))] Fail to jstack Busy(${pcpu}%) thread(${threadId}/0x${threadId0x}) stack of java process(${pid}) under user(${user}).\" echo rm ${jstackFile} continue } } blueEcho \"[$((count++))] Busy(${pcpu}%) thread(${threadId}/0x${threadId0x}) stack of java process(${pid}) under user(${user}):\" sed \"/nid=0x${threadId0x} /,/^$/p\" -n ${jstackFile} done } ps -Leo pid,lwp,user,comm,pcpu --no-headers | { [ -z \"${pid}\" ] && awk '$4==\"java\"{print $0}' || awk -v \"pid=${pid}\" '$1==pid,$4==\"java\"{print $0}' } | sort -k5 -r -n | head --lines \"${count}\" | printStackOfThread 原文地址：https://files-cdn.cnblogs.com/files/clsn/show-busy-java-threads.sh","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/tags/Linux/"}]},{"title":"监控体系","slug":"监控/监控体系","date":"2018-06-04T17:55:39.000Z","updated":"2020-10-25T19:30:53.492Z","comments":true,"path":"2018/06/05/jian-kong/jian-kong-ti-xi/","link":"","permalink":"https://cyylog.github.io/2018/06/05/jian-kong/jian-kong-ti-xi/","excerpt":"","text":"监控对象： 1. 监控对象的理解：CPU是怎么工作的，原理 2. 监控对象的指标：CPU使用率 CPU负载 CPU个数 上下文切换 3. 确定性能基准线：怎么样才算故障？CPU负载多上才算高监控范围： 1.硬件监控服务器的硬件故障 2.操作系统监控 CPU、内存、硬盘、IO、进程 3.应用服务监控 nginx、MySQL、等服务 4.业务监控 硬件监控： 1.使用IPMI 2.机房巡检远程控制卡： DELL服务器：iDRAC HP服务器：ILO ————-Linux就可以使用IPMI（依赖于BMC控制器） IBM服务器：IMM | Linux是管理IPMI工具 ‘ipmitool’（监控和控制） 1.硬件要支持2.操作系统 ‘Linux IPMI’ipmitool安装: [root@localhost ~]# yum install OpenIPMI ipmitool -y [root@localhost ~]# rpm -qa OpenIPMI ipmitool ipmitool-1.8.13-8.el7_1.x86_64 OpenIPMI-2.0.19-11.el7.x86_64使用IPMI有两种方式1、本地进行调用2、远程调用 （IP地址 用户名和密码） [root@localhost ~]# systemctl start ipmi #启动 本次以Centos7进行演示IPMI相关命令 [root@localhost ~]# ipmitool --help ipmitool: invalid option -- &#39;-&#39; ipmitool version 1.8.13 usage: ipmitool [options...] &lt;command&gt; -h This help -V Show version information -v Verbose (can use multiple times) -c Display output in comma separated format -d N Specify a /dev/ipmiN device to use (default=0) -I intf Interface to use -H hostname Remote host name for LAN interface -p port Remote RMCP port [default=623] -U username Remote session username -f file Read remote session password from file -z size Change Size of Communication Channel (OEM) -S sdr Use local file for remote SDR cache -D tty:b[:s] Specify the serial device, baud rate to use and, optionally, specify that interface is the system one -a Prompt for remote password -Y Prompt for the Kg key for IPMIv2 authentication -e char Set SOL escape character -C ciphersuite Cipher suite to be used by lanplus interface -k key Use Kg key for IPMIv2 authentication -y hex_key Use hexadecimal-encoded Kg key for IPMIv2 authentication -L level Remote session privilege level [default=ADMINISTRATOR] Append a &#39;+&#39; to use name/privilege lookup in RAKP1 -A authtype Force use of auth type NONE, PASSWORD, MD2, MD5 or OEM -P password Remote session password -E Read password from IPMI_PASSWORD environment variable -K Read kgkey from IPMI_KGKEY environment variable -m address Set local IPMB address -b channel Set destination channel for bridged request -t address Bridge request to remote target address -B channel Set transit channel for bridged request (dual bridge) -T address Set transit address for bridge request (dual bridge) -l lun Set destination lun for raw commands -o oemtype Setup for OEM (use &#39;list&#39; to see available OEM types) -O seloem Use file for OEM SEL event descriptions -N seconds Specify timeout for lan [default=2] / lanplus [default=1] interface -R retry Set the number of retries for lan/lanplus interface [default=4] Interfaces: open Linux OpenIPMI Interface [default] imb Intel IMB Interface lan IPMI v1.5 LAN Interface lanplus IPMI v2.0 RMCP+ LAN Interface serial-terminal Serial Interface, Terminal Mode serial-basic Serial Interface, Basic Mode Commands: raw Send a RAW IPMI request and print response i2c Send an I2C Master Write-Read command and print response spd Print SPD info from remote I2C device lan Configure LAN Channels chassis Get chassis status and set power state power Shortcut to chassis power commands event Send pre-defined events to MC mc Management Controller status and global enables sdr Print Sensor Data Repository entries and readings sensor Print detailed sensor information fru Print built-in FRU and scan SDR for FRU locators gendev Read/Write Device associated with Generic Device locators sdr sel Print System Event Log (SEL) pef Configure Platform Event Filtering (PEF) sol Configure and connect IPMIv2.0 Serial-over-LAN tsol Configure and connect with Tyan IPMIv1.5 Serial-over-LAN isol Configure IPMIv1.5 Serial-over-LAN user Configure Management Controller users channel Configure Management Controller channels session Print session information dcmi Data Center Management Interface sunoem OEM Commands for Sun servers kontronoem OEM Commands for Kontron devices picmg Run a PICMG/ATCA extended cmd fwum Update IPMC using Kontron OEM Firmware Update Manager firewall Configure Firmware Firewall delloem OEM Commands for Dell systems shell Launch interactive IPMI shell exec Run list of commands from file set Set runtime variable for shell and exec hpm Update HPM components using PICMG HPM.1 file ekanalyzer run FRU-Ekeying analyzer using FRU files ime Update Intel Manageability Engine FirmwareIPMI配置网络，有两种方式：ipmi over lan（大体意思是通过网卡来进行连接）独立 （给服务器单独插一个网线） DELL服务器可以在小面板中设置ipmi 云主机我们不需要考虑IPMI 对于路由器和交换机：SNMP对于这些设备，就不做具体描述了，毕竟没有接触过 系统监控做为系统运维来说系统监控是重点 - CPU - 内存 - IO Input/Ouput（网络、磁盘）CPU三个重要的概念： 1.上下文切换：CPU调度器实施的进程的切换过程，上下文切换 2.运行队列（负载）：运行队列，排队 可以参考我是一个进程文章 3.使用率监控CPU需要确定服务类型：（1） IO密集型 （数据库）（2） CPU密集型（Web/mail） 确定性能的基准线 运行队列：1-3个线程 1CPU 4核 负载不超过12 CPU使用：65%-70%用户态利用率 30%-35%内核态利用率 0%-5% 空闲 上下文切换： 越少越好所有的监控都要根据业务来考虑 常见的系统监控工具Top、sysstat、mpstat 工具的使用方法TOP参数解释 top的详细可以参考我在51cto的这篇文章 http://blog.51cto.com/12419955/2052642 其实对于Top，现在我更喜欢htop和gtop，gtop虽然色彩和功能更强大，但是因为gtop不在epel源里，导致gtop的使用没有htop用的广泛 当然gtop这么好用，当然要用一下，这是另一片关于gtop的文章 https://tigerfivegit.github.io/2018/12/14/Linux%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7-gtop/ 第一行 分别显示：系统当前时间 系统运行时间 当前用户登陆数 系统负载。 系统负载（load average），这里有三个数值，分别是系统最近1分钟，5分钟，15分钟的平均负载。一般对于单个处理器来说，负载在0 — 1.00 之间是正常的，超过1.00就要引起注意了。在多核处理器中，你的系统均值不应该高于处理器核心的总数。 第二行 分别显示：total进程总数、 running正在运行的进程数、 sleeping睡眠的进程数、stopped停止的进程数、 zombie僵尸进程数。 第三行分别显示：%us用户空间占用CPU百分比、%sy内核空间占用CPU百分比、%ni用户进程空间内改变过优先级的进程占用CPU百分比、%id空闲CPU百分比、%wa等待输入输出（I/O）的CPU时间百分比 、%hi指的是cpu处理硬件中断的时间、%si指的是cpu处理软中断的时间 、%st用于有虚拟cpu的情况，用来指示被虚拟机偷掉的cpu时间。通常id%值可以反映一个系统cpu的闲忙程度。 第四行 MEM ：total 物理内存总量、 used 使用的物理内存总量、free 空闲内存总量、 buffers 用作内核缓存的内存量。 第五行 SWAP：total 交换区总量、 used使用的交换区总量、free 空闲交换区总量、 cached缓冲的交换区总量。buffers和cached的区别需要说明一下，buffers指的是块设备的读写缓冲区，cached指的是文件系统本身的页面缓存。它们都是linux操作系统底层的机制，目的就是为了加速对磁盘的访问。 第六行 PID(进程号)、 USER（运行用户）、PR（优先级）、NI（任务nice值）、VIRT（虚拟内存用量）VIRT=SWAP+RES 、RES（物理内存用量）、SHR（共享内存用量）、S（进程状态）、%CPU（CPU占用比）、%MEM（物理内存占用比）、TIME+（累计CPU占 用时间)、 COMMAND 命令名/命令行。 下面简单介绍top命令的使用方法：top [-] [d] [q] [c] [C] [S] [n]运维必会！参数说明d指定每两次屏幕信息刷新之间的时间间隔。当然用户可以使用s交互命令来改变之。p通过指定监控进程ID来仅仅监控某个进程的状态。q该选项将使top没有任何延迟的进行刷新。如果调用程序有超级用户权限，那么top将以尽可能高的优先级运行。S指定累计模式。s使top命令在安全模式中运行。这将去除交互命令所带来的潜在危险。i使top不显示任何闲置或者僵死进程。c显示整个命令行而不只是显示命令名。下面介绍在top命令执行过程中可以使用的一些交互命令 从使用角度来看，熟练的掌握这些命令比掌握选项还重要一些。 这些命令都是单字母的，如果在命令行选项中使用了s选项，则可能其中一些命令会被屏蔽掉。Ctrl+L 擦除并且重写屏幕。h或者? 显示帮助画面，给出一些简短的命令总结说明。k 终止一个进程。系统将提示用户输入需要终止的进程PID，以及需要发送给该进程什么样的信号。一般的终止进程可以使用15信号；如果不能正常结束那就使用信号9强制结束该进程。默认值是信号15。在安全模式中此命令被屏蔽。i 忽略闲置和僵死进程。这是一个开关式命令。q 退出程序。r 重新安排一个进程的优先级别。系统提示用户输入需要改变的进程PID以及需要设置的进程优先级值。输入一个正值将使优先级降低，反之则可以使该进程拥有更高的优先权。默认值是10。s 改变两次刷新之间的延迟时间。系统将提示用户输入新的时间，单位为s。如果有小数，就换算成m s。输入0值则系统将不断刷新，默认值是5 s。需要注意的是如果设置太小的时间，很可能会引起不断刷新，从而根本来不及看清显示的情况，而且系统负载也会大大增加。f或者F 从当前显示中添加或者删除项目。o或者O 改变显示项目的顺序。l 切换显示平均负载和启动时间信息。m 切换显示内存信息。t 切换显示进程和CPU状态信息。c 切换显示命令名称和完整命令行。M 根据驻留内存大小进行排序。P 根据CPU使用百分比大小进行排序。T 根据时间/累计时间进行排序。W 将当前设置写入~/.toprc文件中。这是写top配置文件的推荐方法。Shift+M 可按内存占用情况进行排序。 sysstat 说明yum install sysstat -y vmstat --help usage: vmstat [-V] [-n] [delay [count]] -V prints version. -n causes the headers not to be reprinted regularly. -a print inactive/active page stats. -d prints disk statistics -D prints disk table -p prints disk partition statistics -s prints vm table -m prints slabinfo -t add timestamp to output -S unit size delay is the delay between updates in seconds. unit size k:1000 K:1024 m:1000000 M:1048576 (default is K) count is the number of updates.例子：每隔1秒获取1次，次数不限 # vmstat 1 procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 547332 177544 535336 0 0 1 6 5 41 1 0 98 0 0 0 0 0 547324 177544 535336 0 0 0 0 210 445 1 0 99 0 0 0 0 0 547324 177544 535336 0 0 0 0 195 435 0 0 100 0 0 0 0 0 547324 177544 535336 0 0 0 0 208 440 1 0 99 0 0 0 0 0 547332 177544 535336 0 0 0 0 209 446 0 0 100 0 0 0 0 0 547332 177544 535336 0 0 0 0 207 442 1 1 98 0 0 0 0 0 547332 177544 535336 0 0 0 0 201 438 0 0 100 0 0#r表示CPU排队的情况，b代表 进程堵塞，等待io每隔1秒获取1次，次数10次 # vmstat 1 10 procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 547340 177544 535344 0 0 1 6 5 41 1 0 98 0 0 0 0 0 547332 177544 535344 0 0 0 28 210 453 1 1 97 1 0 0 0 0 547332 177544 535344 0 0 0 0 200 433 0 0 100 0 0 0 0 0 547332 177544 535344 0 0 0 0 211 445 1 0 99 0 0 0 0 0 547332 177544 535344 0 0 0 0 201 439 0 1 99 0 0 0 0 0 547332 177544 535344 0 0 0 0 197 436 0 0 100 0 0 0 0 0 547332 177544 535344 0 0 0 0 201 442 1 0 99 0 0 0 0 0 547324 177544 535348 0 0 0 0 240 484 2 1 97 0 0 0 0 0 547324 177544 535348 0 0 0 0 203 438 0 0 100 0 0 0 0 0 547324 177544 535348 0 0 0 0 197 430 1 0 99 0 0mpstat查看所有CPU的平均值 mpstat 1 Linux 2.6.32-431.23.3.el6.x86_64 (www) 08/30/2016 _x86_64_ (1 CPU) 05:13:22 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %idle 05:13:23 PM all 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 99.00 105:13:24 PM all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 05:13:25 PM all 2.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 97.00 05:13:26 PM all 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 99.00mpstat 1 10 Linux 2.6.32-431.23.3.el6.x86_64 (www) 08/30/2016 _x86_64_ (1 CPU) 05:13:38 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %idle 05:13:39 PM all 2.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 98.00 05:13:40 PM all 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 99.00 05:13:41 PM all 1.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 98.99上述是CPU监控，CPU监控主要靠经验。因为业务不同指标不同，指标越低越好是不变的道理 sar命令也有类似的功能，但是sar命令更能看到历史的信息，对于问题排查有更好的作用当然对于我这种喜欢骚操作的人，sar命令不可能不搞啊，这里放个链接 https://tigerfivegit.github.io/2018/11/21/sar/ 内存硬盘监控：硬盘格式化后分成块（blog）内存默认是页（大小4kb）读取按照页来进行读取内存：free vmstat free -m total used free shared buffers cached Mem: 1875 1338 537 0 173 523 -/+ buffers/cache: 640 1234 Swap: 0 0 0total 总内存used 已使用内存free 空闲内存shared 共享内存（进程间相互通信使用共享内存）buffers 缓冲cached 缓存Centos7 会有一个available，活动内存 #云服务器一般不分配swap分区，物理机能不使用交换分区就不使用交换分区 vmstat 1 procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 550628 177684 536324 0 0 1 6 7 46 1 0 98 0 0 0 0 0 550620 177684 536324 0 0 0 40 187 429 0 0 100 0 0 0 0 0 550620 177684 536324 0 0 0 0 183 427 1 0 99 0 0 0 0 0 550620 177684 536324 0 0 0 0 197 436 0 1 99 0 0swpd交换分区的大小free可用的物理内存大小buff 缓冲区的大小cache 缓存区的大小si 数据从交换分区读取到内存的大小so 数据从内存到交换分区bi 从交换分区读到内存（block）bo 内存写到硬盘的 内存达到多少报警呢？ 80%硬盘：IOPS IO’s Per Second iotop df -h iostat 顺序IO（快） 随机IO（慢）查看磁盘剩余空间 df -h Filesystem Size Used Avail Use% Mounted on /dev/xvda1 40G 4.1G 34G 11% / tmpfs 938M 0 938M 0% /dev/shm监控磁盘IO iotop yum install iotop -y 可以使用dd命令生成一个文件夹进行测试 生成命令如下： # dd if=/dev/zero of=/tmp/1.txt bs=1M count=1000 1000+0 records in 1000+0 records out 1048576000 bytes (1.0 GB) copied, 20.509 s, 51.1 MB/s [root@www ~]# ls -lh /tmp/1.txt -rw-r--r-- 1 root root 1000M Aug 30 19:48 /tmp/1.txt此时IO写入如下图iostat命令，可以看到那块磁盘，比iotop更加细致 # iostat 1 2 Linux 2.6.32-431.23.3.el6.x86_64 (www) 08/30/2016 _x86_64_ (1 CPU) avg-cpu: %user %nice %system %iowait %steal %idle 1.10 0.00 0.27 0.16 0.00 98.46 Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtn xvda 1.51 2.26 17.09 986748 7467560 avg-cpu: %user %nice %system %iowait %steal %idle 1.02 0.00 0.00 0.00 0.00 98.98 Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtn xvda 0.00 0.00 0.00 0 0tps 设备每秒的传输次数（每秒多少的io请求）Blk_read/s 每秒从设备读取的数据量Blk_wrtn/s 每秒像设备写入的数据量Blk_read 写入数据的总数Blk_wrtn 读取数据的总数 网络监控：iftop# yum install iftop -y # iftop -n #-n不做域名解析 正常监控只需要监控网卡带宽即可其中网络监控是最复杂的，ping监控网络延迟网络丢包等。但是此类的网络监控只是监控自己到客户端是否丢包，并不能保证客户端到服务器这边不丢包 其中就产生了如：阿里测、奇云测、站长工具等一系列多节点的监控工具 性能测试常用工具：IBM nmon （nmon analyser—生成AIX性能报告的免费工具）http://nmon.sourceforge.net/pmwiki.php #下载地址（需要翻墙工具）所以我们提供了百度云下载链接：http://pan.baidu.com/s/1boXV6R9 密码：sblf只需要下载对应的版本，给执行权限。执行即可 # chmod +x nmon16e_x86_rhel72 # ./nmon16e_x86_rhel72 我们可以直接输入一个c 一个m一个d。这个是实时的一个状态 ./nmon16e_x86_rhel72 --help ./nmon16e_x86_rhel72: invalid option -- &#39;-&#39; Hint for nmon16e_x86_rhel72 version 16e Full Help Info : nmon16e_x86_rhel72 -h On-screen Stats: nmon16e_x86_rhel72 Data Collection: nmon16e_x86_rhel72 -f [-s &lt;seconds&gt;] [-c &lt;count&gt;] [-t|-T] Capacity Plan : nmon16e_x86_rhel72 -x Interactive-Mode: Read the Welcome screen &amp; at any time type: &quot;h&quot; for more help Type &quot;q&quot; to exit nmon For Data-Collect-Mode -f Must be the first option on the line (switches off interactive mode) Saves data to a CSV Spreadsheet format .nmon file in then local directory Note: -f sets a defaults -s300 -c288 which you can then modify Further Data Collection Options: -s &lt;seconds&gt; time between data snapshots -c &lt;count&gt; of snapshots before exiting -t Includes Top Processes stats (-T also collects command arguments) -x Capacity Planning=15 min snapshots for 1 day. (nmon -ft -s 900 -c 96) ---- End of Hints -c 采集的次数 -s 采集的间隔时间 -f 生成一个文件 -m 指定生成文件位置采集10次 间隔10秒 # ./nmon16e_x86_rhel72 -c 10 -s 10 -f -m /tmp/ # ls localhost_160831_0435.nmon nmon16e_x86_rhel72前面为主机名后面是日期（年月日时分）因为测试可能需要，我们要制作成表格，所以现在将文件上传到桌面上 sz localhost_160831_0435.nmon我们打开下载的工具 解压文件夹，打开nmon analyser v34a.xls 点击Analyse nmon data找到我们刚刚复制出来的文件，就可以看到了。 应用服务监控：举例：Nginx安装nginx # yum install -y gcc glibc gcc-c++ prce-devel openssl-devel pcre-devel提示：nginx可以使用稳定版的最新版，因为安全性会不断的提高。如果是特别老的版本会有一些漏洞和功能 要想监控nginx需要在编译时添加如下参数 --with-http_stub_status_module下载Nginx wget http://nginx.org/download/nginx-1.10.1.tar.gz解压，后面步骤太简单不说了安装 [root@localhost nginx-1.10.1]# useradd -s /sbin/nologin www [root@localhost nginx-1.10.1]# ./configure --prefix=/usr/local/nginx-1.10.1 --user=www --group=www --with-http_ssl_module --with-http_stub_status_module#configure 是一个shell脚本，执行它的作用是生成MAKEFILE（编译make需要） [root@localhost nginx-1.10.1]# make &amp;&amp; make install [root@localhost nginx-1.10.1]# ll total 676 drwxr-xr-x 6 1001 1001 4096 Aug 31 06:02 auto -rw-r--r-- 1 1001 1001 262898 May 31 09:47 CHANGES -rw-r--r-- 1 1001 1001 400701 May 31 09:47 CHANGES.ru drwxr-xr-x 2 1001 1001 4096 Aug 31 06:02 conf -rwxr-xr-x 1 1001 1001 2481 May 31 09:47 configure drwxr-xr-x 4 1001 1001 68 Aug 31 06:02 contrib drwxr-xr-x 2 1001 1001 38 Aug 31 06:02 html -rw-r--r-- 1 1001 1001 1397 May 31 09:47 LICENSE -rw-r--r-- 1 root root 404 Aug 31 07:46 Makefile drwxr-xr-x 2 1001 1001 20 Aug 31 06:02 man drwxr-xr-x 3 root root 119 Aug 31 07:46 objs -rw-r--r-- 1 1001 1001 49 May 31 09:47 README drwxr-xr-x 9 1001 1001 84 Aug 31 06:02 src#make是生成文件，make install是将生成的文件拷贝到不同的地方make install 完成之后可以直接将当前目录拷贝到其他服务器上，安装相同的依赖就可以进行使用。 [root@localhost nginx-1.10.1]# ln -s /usr/local/nginx-1.10.1/ /usr/local/nginx [root@localhost nginx-1.10.1]# netstat -lntp|grep nginx tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 7058/nginx: master修改nginx.conf配置文件 location /status { stub_status on; access_log off; allow 192.168.56.0/24; deny all; }设置只允许56网段访问，并开启日志和状态模块 #这个比较基础，如果不知道怎么添加。可以参考www.nginx.org 状态模块浏览器访问：http://192.168.56.11/status Active connections: 1 server accepts handled requests 3 3 163 Reading: 0 Writing: 1 Waiting: 0Active connections: 当前活跃的连接数3—-&gt; 一共处理了多少个链接（请求）3—-&gt; 成功创建多少次握手163–&gt; 总共创建了多少个请求Reading:当前读取客户端heardr的数量Writing:当前返回给客户端heardr的数量 #如果这个指标飙升，说明是后面的节点挂掉了，例如数据库等。Waiting:大体意思是已经处理完，等待下次请求的数量提示：我们只需要关注活动链接即可 监控最基础的功能采集 存储 展示 告警 几款监控软件说明：几款监控软件大家都知道应该是zabbix，这个入门和部署比较简单，对于中小企业都是友好的，但是难以细化和深入化。后来因业务需求从zabbix逐渐转用小米的开源监控open-falcon，这个对于新手不太友好，但是后期的添加和细化都是特别友好的，模块化、分支化","categories":[{"name":"监控","slug":"监控","permalink":"https://cyylog.github.io/categories/%E7%9B%91%E6%8E%A7/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/tags/Linux/"}]},{"title":"Linux日志切割工具Logrotate配置详解","slug":"Linux/[Linux日志切割工具Logrotate配置详解]","date":"2018-04-04T17:55:39.000Z","updated":"2020-05-25T13:51:10.623Z","comments":true,"path":"2018/04/05/linux/linux-ri-zhi-qie-ge-gong-ju-logrotate-pei-zhi-xiang-jie/","link":"","permalink":"https://cyylog.github.io/2018/04/05/linux/linux-ri-zhi-qie-ge-gong-ju-logrotate-pei-zhi-xiang-jie/","excerpt":"","text":"[Linux日志切割工具Logrotate配置详解]文章目录 [TOC] Logrotate 程序是一个日志文件管理工具。用于分割日志文件，压缩转存、删除旧的日志文件，并创建新的日志文件，下面就对logrotate日志轮转的记录： 1. Logrotate配置文件介绍Linux系统默认安装logrotate，默认的配置文件： /etc/logrotate.conf /etc/logrotate.d/ logrotate.conf：为主配置文件 logrotate.d：为配置相关子系统，用于隔离每个应用配置（Nginx、PHP、Tomcat…） Logrotate是基于CRON来运行的，其脚本是/etc/cron.daily/logrotate，日志轮转是系统自动完成的。实际运行时，Logrotate会调用配置文件/etc/logrotate.conf。 Logrotate可以由自动或者手动触发日志轮转： logrotate -f /etc/logrotate.d/nginx logrotate -f /etc/logrotate.d/php 不过正式执行前最好通过Debug选项来验证一下（-d参数）具体logrotate命令格式如下： logrotate [OPTION...] -d, --debug ：debug模式，测试配置文件是否有错误。 -f, --force ：强制转储文件。 -m, --mail=command ：压缩日志后，发送日志到指定邮箱。 -s, --state=statefile ：使用指定的状态文件。 -v, --verbose ：显示转储过程。 2. Logrotater日志文件切割策略查看logrotate.conf配置： cat /etc/logrotate.conf weekly //默认每一周执行一次rotate轮转工作 rotate 4 //保留多少个日志文件(轮转几次).默认保留四个.就是指定日志文件删除之前轮转的次数，0 指没有备份 create //自动创建新的日志文件，新的日志文件具有和原来的文件相同的权限；因为日志被改名,因此要创建一个新的来继续存储之前的日志 dateext //这个参数很重要！就是切割后的日志文件以当前日期为格式结尾，如xxx.log-20131216这样,如果注释掉,切割出来是按数字递增,即前面说的 xxx.log-1这种格式 compress //是否通过gzip压缩转储以后的日志文件，如xxx.log-20131216.gz ；如果不需要压缩，注释掉就行 include /etc/logrotate.d //导入/etc/logrotate.d/ 目录中的各个应用配置 /var/log/wtmp { //仅针对 /var/log/wtmp 所设定的参数 monthly //每月一次切割,取代默认的一周 minsize 1M //文件大小超过 1M 后才会切割 create 0664 root utmp //指定新建的日志文件权限以及所属用户和组 rotate 1 //只保留一个日志. } #这个 wtmp 可记录用户登录系统及系统重启的时间 #因为有 minsize 的参数，因此不见得每个月一定会执行一次喔.要看文件大小。 Logrotate中其他可配置参数，具体如下： compress //通过gzip 压缩转储以后的日志 nocompress //不做gzip压缩处理 copytruncate //用于还在打开中的日志文件，把当前日志备份并截断；是先拷贝再清空的方式，拷贝和清空之间有一个时间差，可能会丢失部分日志数据。 nocopytruncate //备份日志文件不过不截断 create mode owner group //轮转时指定创建新文件的属性，如create 0777 nobody nobody nocreate //不建立新的日志文件 delaycompress //和compress 一起使用时，转储的日志文件到下一次转储时才压缩 nodelaycompress //覆盖 delaycompress 选项，转储同时压缩。 missingok //如果日志丢失，不报错继续滚动下一个日志 errors address //专储时的错误信息发送到指定的Email 地址 ifempty //即使日志文件为空文件也做轮转，这个是logrotate的缺省选项。 notifempty //当日志文件为空时，不进行轮转 mail address //把转储的日志文件发送到指定的E-mail 地址 nomail //转储时不发送日志文件 olddir directory //转储后的日志文件放入指定的目录，必须和当前日志文件在同一个文件系统 noolddir //转储后的日志文件和当前日志文件放在同一个目录下 sharedscripts //运行postrotate脚本，作用是在所有日志都轮转后统一执行一次脚本。如果没有配置这个，那么每个日志轮转后都会执行一次脚本 prerotate //在logrotate转储之前需要执行的指令，例如修改文件的属性等动作；必须独立成行 postrotate //在logrotate转储之后需要执行的指令，例如重新启动 (kill -HUP) 某个服务！必须独立成行 daily //指定转储周期为每天 weekly //指定转储周期为每周 monthly //指定转储周期为每月 rotate count //指定日志文件删除之前转储的次数，0 指没有备份，5 指保留5 个备份 dateext //使用当期日期作为命名格式 dateformat .%s //配合dateext使用，紧跟在下一行出现，定义文件切割后的文件名，必须配合dateext使用，只支持 %Y %m %d %s 这四个参数 size(或minsize) log-size //当日志文件到达指定的大小时才转储，log-size能指定bytes(缺省)及KB (sizek)或MB(sizem). 当日志文件 >= log-size 的时候就转储。 以下为合法格式：（其他格式的单位大小写没有试过） size = 5 或 size 5 （>= 5 个字节就转储） size = 100k 或 size 100k size = 100M 或 size 100M 3. NGINX日志的配置实例参考:vim /etc/logrotate.d/nginx /var/log/weblog/*.log { daily //指定转储周期为每天 compress //通过gzip 压缩转储以后的日志 rotate 7 //保存7天的日志 missingok //如果日志文件丢失，不要显示错误 notifempty //当日志文件为空时，不进行轮转 dateext //使用当期日期作为命名格式，exp: nginx_access.log-20190120 sharedscripts //运行postrotate脚本 postrotate //执行的指令 if [ -f /run/nginx.pid ]; then kill -USR1 `cat /run/nginx.pid` fi endscript //结束指令 } 4. PHP-FPM日志的配置实例参考:vim /etc/logrotate.d/nginx /usr/local/php/var/log/*.log { daily compress rotate 7 missingok notifempty dateext sharedscripts postrotate if [ -f /usr/local/php/var/run/php-fpm.pid ]; then kill -USR2 `cat /usr/local/php/var/run/php-fpm.pid` fi endscript } 5. Logrotater日志切割轮询由于Logrotate是基于CRON运行的，所以这个日志轮转的时间是由CRON控制的，具体可以查询CRON的配置文件/etc/anacrontab，过往的老版本的文件为（/etc/crontab） 查看轮转文件：/etc/anacrontab cat /etc/anacrontab SHELL=/bin/sh PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root RANDOM_DELAY=45 START_HOURS_RANGE=3-22 1 5 cron.daily nice run-parts /etc/cron.daily 7 25 cron.weekly nice run-parts /etc/cron.weekly @monthly 45 cron.monthly nice run-parts /etc/cron.monthly 使用anacrontab轮转的配置文件，日志切割的生效时间是在凌晨3点到22点之间，而且随机延迟时间是45分钟，但是这样配置无法满足我们在现实中的应用 现在的需求是将切割时间调整到每天的晚上12点，即每天切割的日志是前一天的0-24点之间的内容，操作如下： mv /etc/anacrontab /etc/anacrontab.bak //取消日志自动轮转的设置 使用crontab来作为日志轮转的触发容器来修改Logrotate默认执行时间 vi /etc/crontab SHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root HOME=/ # run-parts 01 * * * * root run-parts /etc/cron.hourly 59 23 * * * root run-parts /etc/cron.daily 22 4 * * 0 root run-parts /etc/cron.weekly 42 4 1 * * root run-parts /etc/cron.monthly 6. 解决logrotate无法自动轮询日志的办法现象说明： 使用logrotate轮询nginx日志，配置好之后，发现nginx日志连续两天没被切割，检查后确定配置文件一切正常，这是为什么呢？？ 强行启动记录文件维护操作，纵使logrotate指令认为没有需要，应该有可能是logroate认为nginx日志太小，不进行轮询。故需要强制轮询，即在/etc/cron.daily/logrotate脚本中将 -t 参数替换成 -f 参数 vim /etc/cron.daily/logrotate #!/bin/sh /usr/sbin/logrotate /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -f logrotate \"ALERT exited abnormally with [$EXITVALUE]\" fi exit 0 最后最后重启下cron服务： /etc/init.d/crond restart Stopping crond: [ OK ] Starting crond: [ OK ]","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://cyylog.github.io/tags/Linux/"}]}]}